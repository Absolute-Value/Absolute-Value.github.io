<!DOCTYPE html>
<html>
  <head>
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
  </head>
  <body>
    <header>
      <div class="container">
        <div class="header-left">
          <img class="logo" src="/assets/images/kei.png">
          <a href="/">軸屋敬介 | Keisuke Jikuya</a>
        </div>
        <div class="header-right">
          <a href="/notes">Note</a>
          <a href="/posts">Post</a>
        </div>
      </div>
    </header>
    <!DOCTYPE html>
<html>
  <head>
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
  </head>
  <body>
    <img src="/assets/images/posts/PortraitNeRF/1.png">
    <h1>Portrait Neural Radiance Fields from a Single Image</h1>
    <p><span class="fa fa-link"></span><a href="https://portrait-nerf.github.io">https://portrait-nerf.github.io</a></p>
    <p><span class="fa fa-calendar"></span> May 13, 2022</p>
    <p><span class="fa fa-flag"></span> NeRF Meta Learning Sparse Views </p>
    <p><span class="fa fa-graduation-cap"></span> Arxiv (2020) </p>
    <h2 id="概要">概要</h2>

<ul>
  <li>メタ学習を活用した1枚の顔写真からのNeRF生成
    <ul>
      <li>顔だけでなく、頭頂部、髪、胴体、眼鏡などのアクセサリを含む</li>
    </ul>
  </li>
  <li>世界座標からの剛体変換を用いて、顔空間においてNeRFを事前学習させるアルゴリズムを提案
    <ul>
      <li>学習データ間の形状のばらつき補正により、未見の被験者に対するモデルの汎化が大幅に改善</li>
    </ul>
  </li>
  <li>照明ステージでの制御されたキャプチャからなる多視点ポートレートデータセットを提供</li>
</ul>

<h2 id="アイデア">アイデア</h2>
<p><img src="/assets/images/posts/PortraitNeRF/2.png" alt="" /></p>

<h3 id="事前学習">事前学習</h3>
<p><img src="/assets/images/posts/PortraitNeRF/3.png" alt="" /></p>
<ul>
  <li>学習データは正面のサポートセット$D_s$と他の角度$D_q$に分かれている</li>
  <li>被験者$m$ごとに
    <ul>
      <li>まず、$D_s$を用いて$\theta_{p,m}$を更新して$\theta^*_m$を取得 (1)</li>
      <li>続いて、$D_q$を用いて$\theta^*<em>m$を更新するときの勾配情報$\nabla</em>\theta L_{D_q}(f_{\theta_m})$を取得 (2)
        <ul>
          <li>正面以外の勾配更新情報</li>
        </ul>
      </li>
      <li>勾配情報$\nabla_\theta L_{D_q}(f_{\theta_m})$を$\theta_{p,m}$に適用することで新たなパラメータ$\theta_{p,m+1}$を得る (3)
        <ul>
          <li>正面情報はテスト時に入力されるため直接勾配更新はしない</li>
        </ul>
      </li>
      <li>すべての被験者$m$に繰り返して学習終了、パラメータ$\theta^*_p$を得る</li>
    </ul>
  </li>
</ul>

<h3 id="剛体変換">剛体変換</h3>
<ul>
  <li>顔のメッシュを用いて、World座標から標準顔座標への剛体変換を学習する
    <ul>
      <li>顔のメッシュ(b)を平均顔(d)の頂点の対応を用いている
<img src="/assets/images/posts/PortraitNeRF/4.png" alt="" /></li>
    </ul>
  </li>
  <li>World座標そのままの(b)に比べて標準顔座標を用いた(c)では顎と目がより自然になっている
<img src="/assets/images/posts/PortraitNeRF/5.png" alt="" /></li>
</ul>

<h3 id="テスト">テスト</h3>
<ul>
  <li>1つの画像で事前学習したパラメータ$\theta^*_p$をファインチューニングし、色と密度を求めてボリュームレンダリングすることで画像を生成</li>
</ul>

<h2 id="結果">結果</h2>

<p>-最新手法との比較
<img src="/assets/images/posts/PortraitNeRF/6.png" alt="" />
<img src="/assets/images/posts/PortraitNeRF/7.png" alt="" /></p>

  </body>
</html>
    <footer>
      <div class="btn-wrapper">
        <a href="mailto:jikuya[at]cv.info.gifu-u.ac.jp" class="btn email"><span class="fa fa-envelope"></span>Email</a>
        <a href="https://twitter.com/jky_kei" class="btn twitter"><span class="fa fa-twitter"></span>Twitter</a>
        <a href="https://github.com/Absolute-Value" class="btn github"><span class="fa fa-github"></span>Github</a>
      </div>
      <div class="container">
        <p>2022 Copyright</p>
      </div>
    </footer>
  </body>
</html>