<!DOCTYPE html>
<html>
  <head>
    <script src="https://kit.fontawesome.com/314069a903.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/markdown.css">
    <link rel="stylesheet" href="/assets/css/header.css">
    <link rel="icon" href="/assets/images/favicon.ico">
    
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>
  </head>
  <body>
    <header>
      <div class="container">
        <div class="header-left">
          <a href="/" class="header-logo"><img class="logo" src="/assets/images/kei.png"></a>
          <a href="/" class="header-name">軸屋敬介 | Keisuke Jikuya</a>
        </div>
        <div class="header-right">
          <a id="sun_moon" class="fa fa-moon-o" onclick="darkButton()"></a>
          <script src="/assets/js/darkmode.js"></script>
          <a href="/">Home</a>
          <a href="/notes">Note</a>
          <a href="/blogs">Blog</a>
          <a href="/posts">Post</a>
        </div>
      </div>
    </header>
    <!DOCTYPE html>
<html>
  <head>
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
  </head>
  <body>
    <div class="top-wrapper">
      <div class="container">
        <h1> Posts | 2</h1>
        <p>読んだ論文をまとめておく場所です</p>
      </div>
    </div>
    <!DOCTYPE html>
<html>
  <title> Posts | 2</title>
  <head>
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/posts.css">
    <script>
  MathJax = {
    tex: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true,
      tags: "ams",
      autoload: {
        color: [],
        colorV2: ['color']
      },
      packages: {'[+]': ['noerrors']}
    },
    chtml: {
      matchFontHeight: false,
      displayAlign: "left",
      displayIndent: "2em"
    },
    options: {
      renderActions: {
        /* add a new named action to render <script type="math/tex"> */
        find_script_mathtex: [10, function (doc) {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/);
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
            const text = document.createTextNode('');
            node.parentNode.replaceChild(text, node);
            math.start = {node: text, delim: '', n: 0};
            math.end = {node: text, delim: '', n: 0};
            doc.math.push(math);
          }
        }, '']
      }
    },
    loader: {
      load: ['[tex]/noerrors']
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>
  </head>
  <body>
    
    <div class="myposts">
      
        
        <div class="post">
          <a href="/caption/2023/07/03/RedCaps.html">
            <img class="post_img" src="/assets/images/posts/RedCaps/1.png">
            <div class="post_detail"> 
              <p><b>RedCaps: Web-curated image-text data created by the people, for the people</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>ビジョンと言語のタスクのための大規模データセットは、検索エンジンをクエリにしたりHTMLのaltテキストを収集することで構築されているが、ウェブデータはノイズが多いため、品質を維持するために複雑なフィルタリングパイプラインが必要</li>
  <li>最小限のフィルタリングで高品質なデータを収集するための代替データソースを探索</li>
  <li>Redditから収集された1200万の画像とキャプションのペアのRedCapsという大規模なデータセットを紹介</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> Jul 3, 2023
                <span class="fa fa-folder"></span> Caption
                <span class="fa fa-graduation-cap"></span> NeurIPS (2021)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/vision%20and%20language/2023/05/15/OFASys.html">
            <img class="post_img" src="https://github.com/OFA-Sys/OFASys/raw/main/images/task7.gif">
            <div class="post_detail"> 
              <p><b>OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>マルチモーダルの汎用モデル学習システムOFASysを提案
    <ul>
      <li>7つ(TEXT、IMAGE、AUDIO、VIDEO、STRUCT、MOTION)のモダリティの23のタスク</li>
    </ul>
  </li>
  <li>複数モダリティのタスクを1行のコードで宣言することで、学習・推論用のタスクプランを自動生成する</li>
  <li>テキスト、画像、音声、動画、モーションデータを扱うことができる世界初の単一モデルOFA+も開発し、15個のタスクに調整されたモデルのわずか16％のパラメータで平均95％の性能を達成</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> May 15, 2023
                <span class="fa fa-folder"></span> Vision and Language
                <span class="fa fa-graduation-cap"></span> arXiv (2022)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/vision%20and%20language/2023/05/10/SimVLM.html">
            <img class="post_img" src="/assets/images/posts/SimVLM/SimVLM.png">
            <div class="post_detail"> 
              <p><b>SimVLM: Simple Visual Language Model Pretraining with Weak Supervision</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>最小限のVision-Language PretrainingフレームワークであるSimple Visual Language Model (SimVLM)を提案</li>
  <li>Prefix Language Modelingによって余分なデータやタスク固有のカスタマイズが必要ない</li>
  <li>従来の事前学習方法を大幅に上回り、VQA、NLVR2、SNLI-VEなどの幅広いVLタスクでSOTA</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> May 10, 2023
                <span class="fa fa-folder"></span> Vision and Language
                <span class="fa fa-graduation-cap"></span> ICLR (2022)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/hoi/2023/05/09/ViPLO.html">
            <img class="post_img" src="/assets/images/posts/ViPLO/top.png">
            <div class="post_detail"> 
              <p><b>ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>MOAモジュールと姿勢条件付きグラフの2段階のHOI検出器ViPROを提案</li>
  <li>MOAモジュールにより量子化問題に対処し、ViTを特徴抽出器として利用</li>
  <li>人間のプロセスに触発された姿勢条件付きグラフにより、人間の姿勢から豊富な情報を利用</li>
  <li>1段階法と比べて、低複雑性と実世界シナリオへの適用性の利点がある</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> May 9, 2023
                <span class="fa fa-folder"></span> HOI
                <span class="fa fa-graduation-cap"></span> CVPR (2023)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/vision%20and%20language/2023/05/08/Unified-IO.html">
            <img class="post_img" src="/assets/images/posts/Unified-IO/Unified-IO-1.png">
            <div class="post_detail"> 
              <p><b>Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>統一された入力と出力を使用して、姿勢推定、物体検出、深度推定、画像生成などのCVタスク、領域キャプションや参照表現などのVLタスク、質問応答やテキスト要約などのNLタスクを実行する統合モデルUNIFIED-IOを提案</li>
  <li>UNIFIED-IOは、単一のtransformerベースのアーキテクチャを使用して、CVとNLの90を超える多様なデータセットを共同でトレーニングできる</li>
  <li>GRITベンチマークで7つのタスクすべてを実行できる最初のモデルであり、NYUv2-Depth、ImageNet、VQA2.0、OK-VQA、Swig、VizWizGround、BoolQ、およびSciTailなどの16の多様なベンチマークでタスク固有のFinetuningなしで優れた結果</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> May 8, 2023
                <span class="fa fa-folder"></span> Vision and Language
                <span class="fa fa-graduation-cap"></span> ICLR (2023)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/language/2023/05/04/BPE.html">
            <img class="post_img" src="/assets/images/posts/BPE/0.png">
            <div class="post_detail"> 
              <p><b>Neural Machine Translation of Rare Words with Subword Units</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>実際の翻訳はopen-vocabularyであるのに対し、ニューラル機械翻訳(NMT)は固定の語彙で動作し、語彙にない単語は辞書で対処してきた（翻訳は1対1とは限らないので不適切）</li>
  <li>そこでBPEを単語分割のタスクに対応させ、希少や未知の単語をサブワード単位で符号化することで、open-vocabularyに対応した</li>
  <li>これにより、WMT15の翻訳課題において英→独で最大1.1BLEU、英→露で1.3BLEU向上</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> May 4, 2023
                <span class="fa fa-folder"></span> Language
                <span class="fa fa-graduation-cap"></span> ACM (2016)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/vision%20and%20language/2023/05/01/VisProg.html">
            <img class="post_img" src="/assets/images/posts/VisProg/VisProg-1.png">
            <div class="post_detail"> 
              <p><b>Visual Programming: Compositional visual reasoning without training</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>1枚または複数枚の画像と自然言語の命令を与え、GPT-3を利用して命令プログラムを作成し、そのプログラムを実行することで目的の出力を得るシステムVISPROGを提案</li>
  <li>命令プログラムの各行では、CVモデル・言語モデル・OpenCVの画像処理・演算子のいずれかのモジュールを実行し、後続で使用できる中間出力を生成している</li>
  <li>事実知識オブジェクトタグ付け・言語ガイド付き画像編集などの4つのタスクで柔軟性を実証</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> May 1, 2023
                <span class="fa fa-folder"></span> Vision and Language
                <span class="fa fa-graduation-cap"></span> CVPR (2023)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/object%20detection/2023/03/31/ViLD.html">
            <img class="post_img" src="/assets/images/posts/ViLD/1.png">
            <div class="post_detail"> 
              <p><b>Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>任意のテキストで物体検出をするオープンボキャブラリ物体検出器ViLD(Vision and Language knowledge Distillation)を提案</li>
  <li>オープンボキャブラリの画像分類である教師モデルから2段階の検出器である生徒モデルに知識蒸留する</li>
  <li>ResNetやALIGNをバックボーンとして、PASCAL VOC、COCO、Objects365で高精度が出た</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> Mar 31, 2023
                <span class="fa fa-folder"></span> Object Detection
                <span class="fa fa-graduation-cap"></span> ICLR (2022)
              </p>
            </div>
          </a>
        </div>
        
      
    </div>
    
    <div class="btn-wrapper">
      
      <a href="/posts" class="btn back_btn previous"><<<</a>
      <a href="/posts/" class="btn back_btn previous"><</a>
      
      
      <a href="/posts/3/" class="btn back_btn next">></a>
      <a href="/posts/9/" class="btn back_btn previous">>>></a>
      
    
    </div>
  </body>
</html>
  </body>
</html>
    <footer>
      <div class="btn-wrapper">
        <a href="mailto:jikuya[at]cv.info.gifu-u.ac.jp" class="bottom-btn email"><i class="fa fa-envelope fa-2x"></i></a>
        <a href="https://twitter.com/jky_kei" class="bottom-btn twitter" target="_blank"><i class="fa-brands fa-x-twitter fa-2x"></i></a>
        <a href="https://github.com/Absolute-Value" class="bottom-btn github" target="_blank"><i class="fa fa-github fa-2x"></i></a>
      </div>
      <div class="container">
        <p>Copyright © 2022 - 2025 Keisuke JIKUYA</p>
      </div>
    </footer>
  </body>
</html>