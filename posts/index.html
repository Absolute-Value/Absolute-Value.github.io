<!DOCTYPE html>
<html>
  <head>
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/markdown.css">
    <link rel="stylesheet" href="/assets/css/header.css">
    <link rel="icon" href="/assets/images/favicon.ico">
    
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>
  </head>
  <body>
    <header>
      <div class="container">
        <div class="header-left">
          <a href="/index" class="header-logo"><img class="logo" src="/assets/images/kei.png"></a>
          <a href="/index" class="header-name">軸屋敬介 | Keisuke Jikuya</a>
        </div>
        <div class="header-right">
          <a id="sun_moon" class="fa fa-moon-o" onclick="dark_btn()"></a>
          <script src="/assets/js/darkmode.js"></script>
          <a href="/index">Home</a>
          <a href="/blogs">Blog</a>
          <a href="/notes">Note</a>
          <a href="/posts">Post</a>
        </div>
      </div>
    </header>
    <!DOCTYPE html>
<html>
  <head>
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
  </head>
  <body>
    <div class="top-wrapper">
      <div class="container">
        <h1>Posts</h1>
        <p>読んだ論文をまとめておく場所です</p>
      </div>
    </div>
    <!DOCTYPE html>
<html>
  <title>Posts</title>
  <head>
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/posts.css">
    <script>
  MathJax = {
    tex: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true,
      tags: "ams",
      autoload: {
        color: [],
        colorV2: ['color']
      },
      packages: {'[+]': ['noerrors']}
    },
    chtml: {
      matchFontHeight: false,
      displayAlign: "left",
      displayIndent: "2em"
    },
    options: {
      renderActions: {
        /* add a new named action to render <script type="math/tex"> */
        find_script_mathtex: [10, function (doc) {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/);
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
            const text = document.createTextNode('');
            node.parentNode.replaceChild(text, node);
            math.start = {node: text, delim: '', n: 0};
            math.end = {node: text, delim: '', n: 0};
            doc.math.push(math);
          }
        }, '']
      }
    },
    loader: {
      load: ['[tex]/noerrors']
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>
  </head>
  <body>
    
    <div class="myposts">
      
        
        <div class="post">
          <a href="/object%20detection/2023/03/14/GO-Finder.html">
            <img class="post_img" src="/assets/images/posts/Go-Finder/GoFinder-1.png">
            <div class="post_detail"> 
              <p><b>GO-Finder: A Registration-Free Wearable System for Assisting Users in Finding Lost Objects via Hand-Held Object Discovery</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>登録不要のウェアラブルカメラを用いた物体の発見支援システムGO-Finderを提案</li>
  <li>手持ちの物体を自動的に検出しグループ化しておくことで、アプリから対象物の最後の出現を取得できる
    <ul>
      <li>手で扱う物体に限定することで、対象となる物体を大幅に削減</li>
    </ul>
  </li>
  <li>物体画像をクエリとして利用し、候補の中から物体を選択することができる</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> Mar 14, 2023
                <span class="fa fa-folder"></span> Object Detection
                <span class="fa fa-graduation-cap"></span> IUI (2021)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/vision%20and%20language/2023/03/14/Flamingo.html">
            <img class="post_img" src="/assets/images/posts/Flamingo/3.png">
            <div class="post_detail"> 
              <p><b>Flamingo: a Visual Language Model for Few-Shot Learning</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>少数のアノテーションで重みの更新なしに新しいタスクに迅速に対応できるVision&amp;LanguageモデルであるFlamingoを提案</li>
  <li>数千倍のタスク専用データでFinetuningに対して、6/16のタスクでSotA</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> Mar 14, 2023
                <span class="fa fa-folder"></span> Vision and Language
                <span class="fa fa-graduation-cap"></span> NeurIPS (2022)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/hoi/2023/03/07/QAHOI.html">
            <img class="post_img" src="/assets/images/posts/QAHOI/1.png">
            <div class="post_detail"> 
              <p><b>QAHOI: Query-Based Anchors for Human-Object Interaction Detection</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>transformerベースの手法QAHOI（Query-Based Anchors for Human-Object Interac
tion detection）を提案</li>
  <li>マルチスケールで特徴を抽出し，クエリベースのアンカーを用いてHOIを予測する</li>
  <li>強力なバックボーンによって、精度が大幅に向上した</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> Mar 7, 2023
                <span class="fa fa-folder"></span> HOI
                <span class="fa fa-graduation-cap"></span> arXiv (2021)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/hoi/2023/02/21/FGAHOI.html">
            <img class="post_img" src="/assets/images/posts/FGAHOI/FGAHOI-1.png">
            <div class="post_detail"> 
              <p><b>FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>MSS, HSAM, TAMという3つから成るEnd-to-endのtransformerベースの手法(FGAHOI)を提案
    <ul>
      <li>MSSは人間、物体、インタラクション領域の特徴を抽出</li>
      <li>HSAMとTAMは抽出された特徴量とクエリ埋め込みを
  階層的な空間視点とタスク視点で順番に意味的に整列・結合</li>
      <li>複雑な学習を軽減するために、新しい学習戦略Stage-wise Training Strategyを設計</li>
    </ul>
  </li>
  <li>新規のデータセットHOI-SDCを提案</li>
  <li>既存手法から大幅に精度向上</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> Feb 21, 2023
                <span class="fa fa-folder"></span> HOI
                <span class="fa fa-graduation-cap"></span> arXiv (2023)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/hoi/2023/02/21/STIP.html">
            <img class="post_img" src="/assets/images/posts/STIP/2.png">
            <div class="post_detail"> 
              <p><b>Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>新しいtransformerベースのHOI手法のStructure-aware Transformer over Interaction Proposals (STIP)を提案</li>
  <li>「インタラクションのある人間と物体のペア提案」と「構造考慮型transformerで提案をHOIに変換」の2つのフェーズでHOIを予測</li>
  <li>構造考慮型transformerはバニラtransformerに対し、全体的意味構造および各相互作用提案内のヒト／モノの局所的空間構造を追加的に符号化することでHOI予測を強化している</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> Feb 21, 2023
                <span class="fa fa-folder"></span> HOI
                <span class="fa fa-graduation-cap"></span> CVPR (2022)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/lane%20detection/2022/12/15/CLRNet.html">
            <img class="post_img" src="/assets/images/posts/CLRNet/img1.png">
            <div class="post_detail"> 
              <p><b>CLRNet: Cross Layer Refinement Network for Lane Detection</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>特徴抽出したFPN構造の特徴マップを、上位から下位まで複合的に活用する車線検出手法であるCross Layer Refinement Network (CLRNet)を提案</li>
  <li>CULaneとTuSimpleとLLAMASのデータセットで従来手法を上回る</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> Dec 15, 2022
                <span class="fa fa-folder"></span> Lane Detection
                <span class="fa fa-graduation-cap"></span> CVPR (2022)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/lane%20detection/2022/12/14/CondLaneNet.html">
            <img class="post_img" src="/assets/images/posts/CondLaneNet/img2.png">
            <div class="post_detail"> 
              <p><b>CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>既存の車線検出は密集線や分岐線のような複雑な場合に苦労している(下図)</li>
  <li>車線を検出し、次に各車線の形状を予測する車線検出フレームワークであるCondLaneNetを提案</li>
  <li>3つのベンチマークデータセットで最先端手法を凌駕</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> Dec 14, 2022
                <span class="fa fa-folder"></span> Lane Detection
                <span class="fa fa-graduation-cap"></span> ICCV (2021)
              </p>
            </div>
          </a>
        </div>
        
      
        
        <div class="post">
          <a href="/lane%20detection/2022/12/13/LaneATT.html">
            <img class="post_img" src="/assets/images/posts/LaneATT/img1.png">
            <div class="post_detail"> 
              <p><b>Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection</b></p>
              <p class="post_p"><h1 id=""></h1>

<ul>
  <li>YOLOv3やSSDのようなアンカーベースのモデルであるLaneATTを提案</li>
  <li>大域的情報の取得のためにAttentionも使用</li>
  <li>CULaneとTuSimpleとLLAMASのデータセットで最先端手法を凌駕</li>
</ul>
</p>
              <p>
                <span class="fa fa-calendar"></span> Dec 13, 2022
                <span class="fa fa-folder"></span> Lane Detection
                <span class="fa fa-graduation-cap"></span> CVPR (2021)
              </p>
            </div>
          </a>
        </div>
        
      
    </div>
    
    <div class="btn-wrapper">
      
      
      <a href="/posts/2/" class="btn back_btn next">></a>
      <a href="/posts/7/" class="btn back_btn previous">>>></a>
      
    
    </div>
  </body>
</html>
  </body>
</html>
    <footer>
      <div class="btn-wrapper">
        <a href="mailto:jikuya[at]cv.info.gifu-u.ac.jp" class="bottom-btn email"><i class="fa fa-envelope fa-2x"></i></a>
        <a href="https://twitter.com/jky_kei" class="bottom-btn twitter" target="_blank"><i class="fa fa-twitter fa-2x"></i></a>
        <a href="https://github.com/Absolute-Value" class="bottom-btn github" target="_blank"><i class="fa fa-github fa-2x"></i></a>
      </div>
      <div class="container">
        <p>2022 Copyright</p>
      </div>
    </footer>
  </body>
</html>