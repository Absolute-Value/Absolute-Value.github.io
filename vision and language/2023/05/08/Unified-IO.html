<!DOCTYPE html>
<html>
  <head>
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
    <link rel="stylesheet" href="/assets/css/markdown.css">
    <link rel="stylesheet" href="/assets/css/header.css">
    <link rel="icon" href="/assets/images/favicon.ico">
    
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>
  </head>
  <body>
    <header>
      <div class="container">
        <div class="header-left">
          <a href="/index" class="header-logo"><img class="logo" src="/assets/images/kei.png"></a>
          <a href="/index" class="header-name">軸屋敬介 | Keisuke Jikuya</a>
        </div>
        <div class="header-right">
          <a id="sun_moon" class="fa fa-moon-o" onclick="dark_btn()"></a>
          <script src="/assets/js/darkmode.js"></script>
          <a href="/index">Home</a>
          <a href="/blogs">Blog</a>
          <a href="/notes">Note</a>
          <a href="/posts">Post</a>
        </div>
      </div>
    </header>
    <!DOCTYPE html>
<html>
  <title>Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks</title>
  <head>
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/style.css">
    <script>
  MathJax = {
    tex: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true,
      tags: "ams",
      autoload: {
        color: [],
        colorV2: ['color']
      },
      packages: {'[+]': ['noerrors']}
    },
    chtml: {
      matchFontHeight: false,
      displayAlign: "left",
      displayIndent: "2em"
    },
    options: {
      renderActions: {
        /* add a new named action to render <script type="math/tex"> */
        find_script_mathtex: [10, function (doc) {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/);
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
            const text = document.createTextNode('');
            node.parentNode.replaceChild(text, node);
            math.start = {node: text, delim: '', n: 0};
            math.end = {node: text, delim: '', n: 0};
            doc.math.push(math);
          }
        }, '']
      }
    },
    loader: {
      load: ['[tex]/noerrors']
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>
  </head>
  <body>
    <img src="/assets/images/posts/Unified-IO/Unified-IO-1.png" class="hero">
    <h1>Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks</h1>
    <p><span class="fa fa-link"></span><a href="https://unified-io.allenai.org/" target="_blank"> https://unified-io.allenai.org/</a></p>
    <p><span class="fa fa-calendar"></span> May 8, 2023</p>
    <p><span class="fa fa-flag"></span> Multimodal Pretraining, Multitask Learning, Unified Frameworks, Zero-shot Learning, Vision and Language, </p>
    <p><span class="fa fa-graduation-cap"></span> ICLR (2023) </p>
    <div class="share-btn-wrapper">
      <a href="https://twitter.com/share?text=Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks&url=https://absolute-value.github.io/vision%20and%20language/2023/05/08/Unified-IO.html" class="share-btn share-twitter"><span class="fa fa-twitter fa-2x"></span></a>
      <a href="https://www.facebook.com/sharer.php?u=https://absolute-value.github.io/vision%20and%20language/2023/05/08/Unified-IO.html" class="share-btn share-facebook"><span class="fa fa-facebook fa-2x"></span></a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://absolute-value.github.io/vision%20and%20language/2023/05/08/Unified-IO.html&title=Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks" class="share-btn"><span class="fa fa-linkedin fa-2x"></span></a>
      <a href="mailto:?subject=Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks&body=Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks https://absolute-value.github.io/vision%20and%20language/2023/05/08/Unified-IO.html" class="share-btn share-email"><span class="fa fa-envelope fa-2x"></span></a>
    </div>
    <h1 id="概要">概要</h1>

<ul>
  <li>統一された入力と出力を使用して、姿勢推定、物体検出、深度推定、画像生成などのCVタスク、領域キャプションや参照表現などのVLタスク、質問応答やテキスト要約などのNLタスクを実行する統合モデルUNIFIED-IOを提案</li>
  <li>UNIFIED-IOは、単一のtransformerベースのアーキテクチャを使用して、CVとNLの90を超える多様なデータセットを共同でトレーニングできる</li>
  <li>GRITベンチマークで7つのタスクすべてを実行できる最初のモデルであり、NYUv2-Depth、ImageNet、VQA2.0、OK-VQA、Swig、VizWizGround、BoolQ、およびSciTailなどの16の多様なベンチマークでタスク固有のFinetuningなしで優れた結果
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>モダリティ固有のブランチを必要とせずに、GRITベンチマークで7つのタスクをサポートする最初のモデル</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/Unified-IO/Unified-IO-2.png" alt="" /></p>
<ul>
  <li>アーキテクチャ
    <ul>
      <li>Text-to-Text Transfer Transformer (T5)に従って、基本は純粋なTransformer Encoder-Decoder構造</li>
      <li>画像はViTに倣ってパッチトークンにし、2次元絶対位置埋め込みを追加</li>
      <li>入力は言語256画像576トークン(384x384画像から24x24パッチ)で、出力は言語128画像256トークン(256x256画像から16x16パッチ)</li>
      <li>パラメータ
        <ul>
          <li><img src="/assets/images/posts/Unified-IO/Unified-IO-3.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>学習
    <ul>
      <li>2つの事前学習
        <ul>
          <li>言語のノイズ除去(BERTと同様)
            <ul>
              <li>半分がテキストデータ(C4とWikipedia)</li>
              <li>残りがImagenet21kなどの画像とクラスデータや、YFCC15Mなどの画像とキャプションデータ</li>
            </ul>
          </li>
          <li>画像のマスクノイズ除去(MAEと同様)
            <ul>
              <li>CVデータの一部の画像を使用</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>学習とデータセット
        <ul>
          <li>62箇所から95の公開データセットを使用
            <ul>
              <li><img src="/assets/images/posts/Unified-IO/Unified-IO-4.png" alt="" /></li>
              <li><img src="/assets/images/posts/Unified-IO/Unified-IO-5.png" alt="" /></li>
            </ul>
          </li>
          <li>バッチにデータセットを混ぜて訓練
            <ul>
              <li>データの多い画像合成は3/16、少ない密度ラベリングは1/16、それ以外はグループで均等</li>
              <li>グループ内ではデータセットサイズの平方根に比例してサンプリング</li>
              <li>一部のタスクはめったにサンプリングされない（深度推定は0.43％）</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>語彙
    <ul>
      <li>言語が32152，場所が1000，画像が16384の計49536</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>GRIT ベンチマーク
    <ul>
      <li>画像分類、物体検出、VQA、参照表現、セグメンテーション、キーポイント、および表面法線推定など7つのタスクで構成</li>
      <li>既存で最も多いGPV-2が4タスクしかできないのに対し、UNIFIED-IOは7つのタスクすべてをサポート</li>
      <li>UNIFIED-IO XLは画像分類、VQA、参照表現、セグメンテーションにおいてSOTA</li>
      <li>キーポイントはMask R-CNNより劣っている（推論が二段階のため）
        <ul>
          <li><img src="/assets/images/posts/Unified-IO/Unified-IO-6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Detection
    <ul>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-7.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-8.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-9.png" alt="" /></li>
    </ul>
  </li>
  <li>Reration
    <ul>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-10.png" alt="" /></li>
    </ul>
  </li>
  <li>Other
    <ul>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-11.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-12.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-13.png" alt="" /></li>
    </ul>
  </li>
</ul>


    <div class="btn-wrapper">
      <a href="/posts" class="btn back_btn">一覧へ戻る</a>
    </div>
  </body>
</html>
    <footer>
      <div class="btn-wrapper">
        <a href="mailto:jikuya[at]cv.info.gifu-u.ac.jp" class="bottom-btn email"><i class="fa fa-envelope fa-2x"></i></a>
        <a href="https://twitter.com/jky_kei" class="bottom-btn twitter" target="_blank"><i class="fa fa-twitter fa-2x"></i></a>
        <a href="https://github.com/Absolute-Value" class="bottom-btn github" target="_blank"><i class="fa fa-github fa-2x"></i></a>
      </div>
      <div class="container">
        <p>Copyright © 2022 - 2023 Keisuke JIKUYA</p>
      </div>
    </footer>
  </body>
</html>