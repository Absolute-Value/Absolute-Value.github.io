<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-07-18T19:07:37+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">軸屋敬介 | Keisuke Jikuya</title><entry><title type="html">Mora: Enabling Generalist Video Generation via A Multi-Agent Framework</title><link href="http://localhost:4000/transformer/2024/03/28/Mora.html" rel="alternate" type="text/html" title="Mora: Enabling Generalist Video Generation via A Multi-Agent Framework" /><published>2024-03-28T17:18:00+09:00</published><updated>2024-03-28T17:18:00+09:00</updated><id>http://localhost:4000/transformer/2024/03/28/Mora</id><content type="html" xml:base="http://localhost:4000/transformer/2024/03/28/Mora.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>OpenAIによるSoraの影響を受け、新しいマルチエージェントフレームワークであるMoraを提案</li>
  <li>テキストからビデオへの生成、画像からビデオへの編集、ビデオの接続など、複数のビデオ関連タスクに対応</li>
  <li>広範な実験を通じて、MoraがSoraに近い性能を示すものの、全体的な性能ギャップが存在することを確認
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>さまざまなビデオ生成タスクに対応する最初のオープンソースマルチエージェントフレームワーク</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/Mora/1.png" alt="" /></p>
<ol>
  <li>GPT-4などの大規模生成モデルを使用して、高品質なプロンプトを生成</li>
  <li>大規模テキストから事前学習された画像モデル（例：Stable Diffusion XL）を使用して、最初の画像を生成</li>
  <li>InstructPix2Pixのようなモデルを使用して、特定のテキスト指示に基づいてソース画像を編集または改善</li>
  <li>Stable Video Diffusionなどのビデオ生成モデルを使用して、初期画像から動画シーケンスを生成</li>
  <li>SEINEなどのビデオトランジションエージェントを使用して、2つの入力ビデオを滑らかにつなぎ合わせ
    <ul>
      <li>これらの組み合わせによってタスクを実行（例：Text-to-Video 1→2→4または1→2→3→4）</li>
    </ul>
  </li>
</ol>

<h1 id="結果">結果</h1>

<ul>
  <li>Text-to-Video
    <ul>
      <li>Performance
        <ul>
          <li><img src="/assets/images/posts/Mora/2.png" alt="" /></li>
        </ul>
      </li>
      <li>Sample
        <ul>
          <li><img src="/assets/images/posts/Mora/3.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Text-condtional Image-to-Video Generation
    <ul>
      <li>Performance
        <ul>
          <li><img src="/assets/images/posts/Mora/4.png" alt="" /></li>
        </ul>
      </li>
      <li>Sample
        <ul>
          <li><img src="/assets/images/posts/Mora/5.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Extend Generated Videos
    <ul>
      <li>Performance
        <ul>
          <li><img src="/assets/images/posts/Mora/6.png" alt="" /></li>
        </ul>
      </li>
      <li>Sample
        <ul>
          <li><img src="/assets/images/posts/Mora/7.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Video-to-Video Editing
    <ul>
      <li>Performance
        <ul>
          <li><img src="/assets/images/posts/Mora/8.png" alt="" /></li>
        </ul>
      </li>
      <li>Sample
        <ul>
          <li><img src="/assets/images/posts/Mora/9.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Connect Videos
    <ul>
      <li>Performance
        <ul>
          <li><img src="/assets/images/posts/Mora/10.png" alt="" /></li>
        </ul>
      </li>
      <li>Sample
        <ul>
          <li><img src="/assets/images/posts/Mora/11.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Transformer" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 OpenAIによるSoraの影響を受け、新しいマルチエージェントフレームワークであるMoraを提案 テキストからビデオへの生成、画像からビデオへの編集、ビデオの接続など、複数のビデオ関連タスクに対応 広範な実験を通じて、MoraがSoraに近い性能を示すものの、全体的な性能ギャップが存在することを確認]]></summary></entry><entry><title type="html">ReConPatch : Contrastive Patch Representation Learning for Industrial Anomaly Detection</title><link href="http://localhost:4000/transformer/2024/03/28/ReConPatch.html" rel="alternate" type="text/html" title="ReConPatch : Contrastive Patch Representation Learning for Industrial Anomaly Detection" /><published>2024-03-28T15:37:00+09:00</published><updated>2024-03-28T15:37:00+09:00</updated><id>http://localhost:4000/transformer/2024/03/28/ReConPatch</id><content type="html" xml:base="http://localhost:4000/transformer/2024/03/28/ReConPatch.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>既存手法では、自然画像データセットから事前に学習した視覚表現を活用しているため、産業データセットへの適用にはギャップが存在</li>
  <li>事前学習モデルからのパッチ特徴の線形変調を訓練し、コントラスト表現学習を用いて異常検出のための識別的特徴を構築</li>
  <li>MVTec ADデータセットおよびBTADデータセットにおける最先端の異常検出性能
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>ペアワイズおよびコンテクスト類似性を疑似ラベルとして活用し、ラベルのないデータに対するコントラスト学習を可能にした</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/ReConPatch/1.png" alt="" /></li>
  <li>学習
    <ul>
      <li>訓練データを事前学習済みCNNに通して特徴マップを収集
        <ul>
          <li>異なる空間解像度は同じ解像度に補間して連結</li>
        </ul>
      </li>
      <li>特定のパッチサイズ内の近傍の特徴ベクトルを集約し、パッチレベルの特徴を生成</li>
      <li>特徴表現学習のための2つのネットワーク
        <ol>
          <li>パッチレベルの特徴表現学習用</li>
          <li>パッチレベルの特徴ペア間のペアワイズおよびコンテクスト類似性を計算</li>
        </ol>
      </li>
      <li>relaxed contrastive lossに基づいて、特徴表現層と射影層を訓練
        <ul>
          <li>似た特徴が近づき、異なる特徴が遠ざかるように学習</li>
          <li><img src="/assets/images/posts/ReConPatch/2.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>テスト
    <ul>
      <li>事前学習済みCNNに通して特徴マップを収集</li>
      <li>メモリバンク内の代表的な特徴との比較によりピクセルごとの異常スコアを計<strong>算</strong></li>
      <li>画像レベルの異常スコアは画像内のすべてのパッチ特徴に対して計算された異常スコアの最大値</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>MVTec AD
    <ul>
      <li><img src="/assets/images/posts/ReConPatch/3.png" alt="" /></li>
      <li><img src="/assets/images/posts/ReConPatch/4.png" alt="" /></li>
    </ul>
  </li>
  <li>BTAD
    <ul>
      <li><img src="/assets/images/posts/ReConPatch/5.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Transformer" /><category term="Anomaly Detection" /><summary type="html"><![CDATA[概要 既存手法では、自然画像データセットから事前に学習した視覚表現を活用しているため、産業データセットへの適用にはギャップが存在 事前学習モデルからのパッチ特徴の線形変調を訓練し、コントラスト表現学習を用いて異常検出のための識別的特徴を構築 MVTec ADデータセットおよびBTADデータセットにおける最先端の異常検出性能]]></summary></entry><entry><title type="html">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</title><link href="http://localhost:4000/transformer/2024/02/15/Unified-IO2.html" rel="alternate" type="text/html" title="Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action" /><published>2024-02-15T13:00:00+09:00</published><updated>2024-02-15T13:00:00+09:00</updated><id>http://localhost:4000/transformer/2024/02/15/Unified-IO2</id><content type="html" xml:base="http://localhost:4000/transformer/2024/02/15/Unified-IO2.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>マルチモーダルモデルUNIFIED-IO 2を提案</li>
  <li>異なるモダリティを統合するために、入出力をトークン化し、単一のEncoder Decoder Transformerモデルで意味空間を共有</li>
  <li>モデルトレーニングの安定化のためにさまざまなアーキテクチャ改善</li>
  <li>GRITベンチマークで最先端の性能を達成し、画像生成と理解、自然言語理解、ビデオとオーディオの理解、ロボット操作を含む35以上のベンチマークで強力な結果を達成
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>画像、テキスト、音声、アクションを理解し、生成することができる初のマルチモーダルモデル</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>Architecture
    <ul>
      <li><img src="/assets/images/posts/Unified-IO2/1.png" alt="" /></li>
      <li>モダリティを追加統合するにつれて学習が不安定になるため構造変更
        <ul>
          <li>relative positional embeddingではなく2D Rotary Embedding</li>
          <li>dot-product attentionの前にクエリとキーにLayerNormを適用するQK Normalization</li>
          <li>画像や音声の圧縮にperceiver resamplerを使用
            <ul>
              <li>perceiver resamplerにはScaled Cosine Attentionを用いる</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Multimodal Mixture of Denoisers
    <ul class="task-list">
      <li>[R] – 入力画像や音声パッチの特徴のx%をランダムにマスクし、それを再構築</li>
      <li>[S] – 他の入力モダリティのみを条件として、ターゲットモダリティを生成</li>
      <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />– 極端な破損</li>
    </ul>
  </li>
  <li>Autoregressive with Dynamic Masking
    <ul>
      <li>(a) autoregressive, (b) mask auto-encoder, (c) autoregressive</li>
      <li><img src="/assets/images/posts/Unified-IO2/2.png" alt="" /></li>
    </ul>
  </li>
  <li>効率化するために、トークンの短い文章を結合して学習</li>
  <li>Data
    <ul>
      <li>pre-training
        <ul>
          <li><img src="/assets/images/posts/Unified-IO2/3.png" alt="" /></li>
          <li>10億の画像とテキストのペア、1兆のテキストトークン、1億8000万のビデオクリップ、1億3000万のインターリーブされた画像とテキスト、300万の3Dアセット、100万のエージェントの軌跡など</li>
        </ul>
      </li>
      <li>instruction tuning
        <ul>
          <li><img src="/assets/images/posts/Unified-IO2/4.png" alt="" /></li>
          <li>視覚、言語、音声、行動など220のタスクをカバーする120以上のデータセット</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>Zero-shot performance
    <ul>
      <li><img src="/assets/images/posts/Unified-IO2/5.png" alt="" /></li>
    </ul>
  </li>
  <li>GRIT ablation and test set
    <ul>
      <li><img src="/assets/images/posts/Unified-IO2/6.png" alt="" /></li>
    </ul>
  </li>
  <li>text-to-image generation
    <ul>
      <li><img src="/assets/images/posts/Unified-IO2/7.png" alt="" /></li>
    </ul>
  </li>
  <li>Vision-language results
    <ul>
      <li><img src="/assets/images/posts/Unified-IO2/8.png" alt="" /></li>
    </ul>
  </li>
  <li>action classification, video captioning, VQA, visual comprehension, audio classification, and audio captioning
    <ul>
      <li><img src="/assets/images/posts/Unified-IO2/9.png" alt="" /></li>
    </ul>
  </li>
  <li>Single-object 3D detection on Objectron
    <ul>
      <li><img src="/assets/images/posts/Unified-IO2/10.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Transformer" /><category term="Unified Frameworks" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 マルチモーダルモデルUNIFIED-IO 2を提案 異なるモダリティを統合するために、入出力をトークン化し、単一のEncoder Decoder Transformerモデルで意味空間を共有 モデルトレーニングの安定化のためにさまざまなアーキテクチャ改善 GRITベンチマークで最先端の性能を達成し、画像生成と理解、自然言語理解、ビデオとオーディオの理解、ロボット操作を含む35以上のベンチマークで強力な結果を達成]]></summary></entry><entry><title type="html">Video Swin Transformer</title><link href="http://localhost:4000/dataset/2024/02/15/VideoSwin.html" rel="alternate" type="text/html" title="Video Swin Transformer" /><published>2024-02-15T12:00:00+09:00</published><updated>2024-02-15T12:00:00+09:00</updated><id>http://localhost:4000/dataset/2024/02/15/VideoSwin</id><content type="html" xml:base="http://localhost:4000/dataset/2024/02/15/VideoSwin.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>既存のビデオモデルは、空間的・時間的次元にわたってパッチをグローバルに接続するTransformer Layerを使用</li>
  <li>提案ビデオアーキテクチャは局所性の帰納バイアスを持ち、従来のアプローチに比べて速度と精度のトレードオフを改善</li>
  <li>小さな事前学習データセットとモデルサイズを使用しながらも、Kinetics-400とKinetics-600とSomething-Something v2でSoTA
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>ビデオ認識においてCNNからTransformerへのモデリングシフト</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/VideoSwin/1.png" alt="" /></li>
  <li>各フレームの3Dパッチをトークンとする</li>
  <li>線形埋め込み層で任意の次元に東映</li>
  <li>Video Swin Transformer blocks
    <ul>
      <li>SwinTransformerのMSAモジュールを3D shifted windowsに基づくMSAモジュールに置き換え
        <ul>
          <li><img src="/assets/images/posts/VideoSwin/2.png" alt="" /></li>
        </ul>
      </li>
      <li>3D shifted windows
        <ul>
          <li><img src="/assets/images/posts/VideoSwin/3.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>Kinetics-400
    <ul>
      <li>84.9%(SoTA)</li>
      <li><img src="/assets/images/posts/VideoSwin/4.png" alt="" /></li>
    </ul>
  </li>
  <li>Kinetics-600
    <ul>
      <li>86.1%(SoTA)</li>
      <li><img src="/assets/images/posts/VideoSwin/5.png" alt="" /></li>
    </ul>
  </li>
  <li>Something-Something v2
    <ul>
      <li>69.6%(SoTA)</li>
      <li><img src="/assets/images/posts/VideoSwin/6.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Dataset" /><category term="Action Recognition" /><category term="Transformer" /><summary type="html"><![CDATA[概要 既存のビデオモデルは、空間的・時間的次元にわたってパッチをグローバルに接続するTransformer Layerを使用 提案ビデオアーキテクチャは局所性の帰納バイアスを持ち、従来のアプローチに比べて速度と精度のトレードオフを改善 小さな事前学習データセットとモデルサイズを使用しながらも、Kinetics-400とKinetics-600とSomething-Something v2でSoTA]]></summary></entry><entry><title type="html">Music Transformer: Generating music with long-term structure</title><link href="http://localhost:4000/transformer/2023/10/23/MusicTransformer.html" rel="alternate" type="text/html" title="Music Transformer: Generating music with long-term structure" /><published>2023-10-23T20:14:00+09:00</published><updated>2023-10-23T20:14:00+09:00</updated><id>http://localhost:4000/transformer/2023/10/23/MusicTransformer</id><content type="html" xml:base="http://localhost:4000/transformer/2023/10/23/MusicTransformer.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>音楽の長期構造を生成するためのMusic Transformerを提案</li>
  <li>Music Transformerは既存のTransformerモデルの相対位置情報の表現を改善し、音楽の相対的なタイミングとピッチを捉えることができる</li>
  <li>データセット「JSB Chorales」と「Piano-e-Competition」で評価され、後者で最先端の結果を達成
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>Transformerを用いて長期構造を持つ音楽を生成する初の成功例で、LSTMを超えた</li>
  <li>提案アルゴリズムは、相対的自己注意メカニズムの空間複雑さを大幅に削減し、より長い音楽構造の生成を可能にしている</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>Relative positional self-attention
    <ul>
      <li>Shawらによって提案されたもの
        <ul>
          <li><img src="/assets/images/posts/MusicTransformer/1.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Skewing
    <ul>
      <li>LxDの相対埋め込みE（学習パラメータ）がある</li>
      <li>ShawらはEからLxLxDの埋め込みに拡張しQをかけることでSを作成 O($L^2D$)</li>
      <li>SkewingではEに直接Qをかけて、図下のような変形をすることで効率化 O($LD$)
        <ul>
          <li><img src="/assets/images/posts/MusicTransformer/2.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Relative local attention
    <ul>
      <li>非常に長い文の場合にすべてのAttentionをとるのは非現実的</li>
      <li>Mブロックにわけた長さNに対してAttentionをとる</li>
      <li>その場合のSkewingは、図のような変形
        <ul>
          <li><img src="/assets/images/posts/MusicTransformer/3.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>ShawらのRelative Attentionとのメモリ使用量の比較
    <ul>
      <li><img src="/assets/images/posts/MusicTransformer/4.png" alt="" /></li>
    </ul>
  </li>
  <li>J.S.Bach Chorales
    <ul>
      <li><img src="/assets/images/posts/MusicTransformer/5.png" alt="" /></li>
    </ul>
  </li>
  <li>Piano-e-Competition dataset
    <ul>
      <li><img src="/assets/images/posts/MusicTransformer/6.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Transformer" /><category term="Attention" /><category term="Transformer" /><summary type="html"><![CDATA[概要 音楽の長期構造を生成するためのMusic Transformerを提案 Music Transformerは既存のTransformerモデルの相対位置情報の表現を改善し、音楽の相対的なタイミングとピッチを捉えることができる データセット「JSB Chorales」と「Piano-e-Competition」で評価され、後者で最先端の結果を達成]]></summary></entry><entry><title type="html">Places: An Image Database for Deep Scene Understanding</title><link href="http://localhost:4000/dataset/2023/10/23/Places.html" rel="alternate" type="text/html" title="Places: An Image Database for Deep Scene Understanding" /><published>2023-10-23T18:45:00+09:00</published><updated>2023-10-23T18:45:00+09:00</updated><id>http://localhost:4000/dataset/2023/10/23/Places</id><content type="html" xml:base="http://localhost:4000/dataset/2023/10/23/Places.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>シーンの意味論的なカテゴリと属性がラベル付けされた 1,000 万枚のシーン写真のリポジトリである Places Databaseについて説明</li>
  <li>最先端のCNNを使用して分類時に優れたベースラインパフォーマンス
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>環境のカテゴリの広範なカバレッジを提供することにより、シーン理解の能力を向上させることを目指している</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>オンライン画像検索エンジン(Google画像、Bing画像、およびFlickr) から、SUN データセットのクラスのリストからのクエリワードを使用して候補画像をダウンロード
    <ul>
      <li>外観の多様性のために696の一般的な形容詞 (例: 乱雑、晴れ、荒涼など)と組み合わせた</li>
      <li>SUNデータセットと同じ画像が含まれないようにしている</li>
    </ul>
  </li>
  <li>Amazon Mechanical Turk (AMT) にクラウドソーシングしてラベルが正しいかを検証
    <ul>
      <li><img src="/assets/images/posts/Places/1.png" alt="" /></li>
      <li>最初の反復では50％以上、2回目の反復では25.4％がNo</li>
      <li>最終的に少なくとも1,000個のサンプルが含まれるカテゴリが413、20,000個を超えるサンプルが含まれるカテゴリが98</li>
    </ul>
  </li>
  <li>AlexNetで残りの 5,300 万枚の画像を分類
    <ul>
      <li>データセットは1,000個のサンプルが含まれる413カテゴリ</li>
      <li>サンプルが少ないカテゴリは信頼度0.8以上のものをAMTへ</li>
      <li>5,000個のサンプルが含まれるカテゴリが401、20,000個を超えるサンプルが含まれるカテゴリが240</li>
    </ul>
  </li>
  <li>「スキー ロッジ」と「スキー リゾート」のような類似カテゴリを統合</li>
  <li>分類が曖昧な画像をどちらに属すかをAMTで収集することで分類
    <ul>
      <li><img src="/assets/images/posts/Places/2.png" alt="" /></li>
      <li><img src="/assets/images/posts/Places/3.png" alt="" /></li>
      <li>434の場所カテゴリ,10,624,928 枚の画像のデータベースが完成</li>
    </ul>
  </li>
  <li>4,000枚を超える画像を含む365のカテゴリからPlaces365-StandardとPlaces365-Challengeを作成
    <ul>
      <li>Places365-Standardは1,803,460個の学習画像</li>
      <li>Places365-Challengeは800万個の学習画像</li>
      <li>検証はクラスごとに50個、テストはクラスごとに900個</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>CNN分類精度(Places205 &amp; SUN205)
    <ul>
      <li><img src="/assets/images/posts/Places/4.png" alt="" /></li>
    </ul>
  </li>
  <li>CNN分類精度(Places365)
    <ul>
      <li><img src="/assets/images/posts/Places/5.png" alt="" /></li>
    </ul>
  </li>
  <li>VGG(Places365)
    <ul>
      <li><img src="/assets/images/posts/Places/6.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Dataset" /><category term="Dataset" /><summary type="html"><![CDATA[概要 シーンの意味論的なカテゴリと属性がラベル付けされた 1,000 万枚のシーン写真のリポジトリである Places Databaseについて説明 最先端のCNNを使用して分類時に優れたベースラインパフォーマンス]]></summary></entry><entry><title type="html">Sun database: Large-scale scene recognition from abbey to zoo</title><link href="http://localhost:4000/dataset/2023/10/22/sun397.html" rel="alternate" type="text/html" title="Sun database: Large-scale scene recognition from abbey to zoo" /><published>2023-10-22T18:45:00+09:00</published><updated>2023-10-22T18:45:00+09:00</updated><id>http://localhost:4000/dataset/2023/10/22/sun397</id><content type="html" xml:base="http://localhost:4000/dataset/2023/10/22/sun397.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>899のカテゴリと130,519の画像を含む広範なScene UNderstanding (SUN) データベースを提案</li>
  <li>さまざまな最先端のアルゴリズムを使用してシーン認識の新しいパフォーマンスの境界を設定
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>SUNデータベースは現存する最大のシーンカテゴリデータセットで、これまでの研究よりも多様な環境をカバー</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>小さな画像データセットで利用可能なWordNetの70,000の用語から、シーン、場所、および環境を記述する2500の空間とシーンの初期用語を選択
    <ul>
      <li>具体的な地名（ニューヨークなど）や特定のアイデンティティ（職場など）を想起させる用語は含めない</li>
      <li>ナビゲート不可能なシーン（デスクトップなど）や乗り物を含めない</li>
      <li>WordNetに欠けているいくつかのカテゴリを追加</li>
    </ul>
  </li>
  <li>同義語を重ねて、屋内と屋内などを分離して、899のカテゴリーと130,519の画像でSUNデータベースを構築</li>
  <li>各シーン・カテゴリーについて、ウェブ上の様々な検索エンジンでは200×200ピクセル以上のカラー画像を取得
    <ul>
      <li>類似のシーン・カテゴリー（例：修道院と教会）については、定義の重複を避けるために明示的なルールを設定</li>
      <li>退化した画像や異常な画像（白黒、歪んだ色、非常にぼやけている、ノイズが多い、回転が正しくない、航空写真、境界線が目立つ）は削除</li>
    </ul>
  </li>
  <li>899カテゴリのうち、100枚のユニークな写真がある397カテゴリで実験
    <ul>
      <li>人間のシーン分類
        <ul>
          <li>Amazon’s Mechanical Turk (AMT)を使って、各SUNカテゴリについて20の異なるテストシーンで合計397×20=7940の実験を行った</li>
          <li>語彙の混乱を避けるために米国の参加者限定</li>
        </ul>
      </li>
      <li>複数の特徴量（GIST、Dense SIFT、HOG2x2、自己類似性記述子、Tiny Images、色ヒストグラム、直線ヒストグラムなど）でSVMを学習</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>人間のシーン分類の精度
    <ul>
      <li>397のカテゴリで58.6%</li>
      <li>一部の作業者は0%の精度</li>
      <li>最初の階層で95%以上の精度を持つ「良い作業者」に焦点を当てると、精度は68.5%に上昇</li>
      <li>精度が高いカテゴリ
        <ul>
          <li><img src="/assets/images/posts/sun397/1.png" alt="" /></li>
        </ul>
      </li>
      <li>精度が低いカテゴリ
        <ul>
          <li><img src="/assets/images/posts/sun397/2.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>SVMの精度
    <ul>
      <li><img src="/assets/images/posts/sun397/3.png" alt="" /></li>
      <li>全特徴を用いて38%で人間を大きく下回る</li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Dataset" /><category term="Dataset" /><category term="SVM" /><summary type="html"><![CDATA[概要 899のカテゴリと130,519の画像を含む広範なScene UNderstanding (SUN) データベースを提案 さまざまな最先端のアルゴリズムを使用してシーン認識の新しいパフォーマンスの境界を設定]]></summary></entry><entry><title type="html">BEIT: BERT Pre-Training of Image Transformers</title><link href="http://localhost:4000/transformer/2023/10/16/BEIT.html" rel="alternate" type="text/html" title="BEIT: BERT Pre-Training of Image Transformers" /><published>2023-10-16T20:06:00+09:00</published><updated>2023-10-16T20:06:00+09:00</updated><id>http://localhost:4000/transformer/2023/10/16/BEIT</id><content type="html" xml:base="http://localhost:4000/transformer/2023/10/16/BEIT.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>Vision Transformer事前学習する自己教師ありタスクを提案</li>
  <li>BERTのようなマスク画像モデリングを行う</li>
  <li>画像分類とセマンティックセグメンテーションで競争力のある結果を達成し、事前トレーニング方法を改善
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>BERTスタイルの事前トレーニングを画像データに直接適用するのが難しいという課題を解決</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/BEIT/0.png" alt="" /></li>
  <li>事前学習
    <ul>
      <li>入力画像をdiscrete VAEでvisual tokensにする</li>
      <li>同時に入力画像をパッチ分割し、ランダムにマスクしてtransformerへ</li>
      <li>transformerはマスクされたパッチに対応するvisual tokensを予測するように学習</li>
    </ul>
  </li>
  <li>この事前学習をしたモデルをダウンストリームタスクに応用</li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>画像分類（ILSVRC-2012 ImageNet）
    <ul>
      <li><img src="/assets/images/posts/BEIT/1.png" alt="" /></li>
    </ul>
  </li>
  <li>セマンティックセグメンテーション
    <ul>
      <li><img src="/assets/images/posts/BEIT/2.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Transformer" /><category term="Vision Pretraining" /><category term="Transformer" /><summary type="html"><![CDATA[概要 Vision Transformer事前学習する自己教師ありタスクを提案 BERTのようなマスク画像モデリングを行う 画像分類とセマンティックセグメンテーションで競争力のある結果を達成し、事前トレーニング方法を改善]]></summary></entry><entry><title type="html">RedCaps: Web-curated image-text data created by the people, for the people</title><link href="http://localhost:4000/caption/2023/07/03/RedCaps.html" rel="alternate" type="text/html" title="RedCaps: Web-curated image-text data created by the people, for the people" /><published>2023-07-03T10:50:00+09:00</published><updated>2023-07-03T10:50:00+09:00</updated><id>http://localhost:4000/caption/2023/07/03/RedCaps</id><content type="html" xml:base="http://localhost:4000/caption/2023/07/03/RedCaps.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>ビジョンと言語のタスクのための大規模データセットは、検索エンジンをクエリにしたりHTMLのaltテキストを収集することで構築されているが、ウェブデータはノイズが多いため、品質を維持するために複雑なフィルタリングパイプラインが必要</li>
  <li>最小限のフィルタリングで高品質なデータを収集するための代替データソースを探索</li>
  <li>Redditから収集された1200万の画像とキャプションのペアのRedCapsという大規模なデータセットを紹介
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>Redditから収集された画像とキャプションのペアで構成</li>
  <li>独自のデータ収集パイプラインやフィルタリング手法を使用して高品質なデータを提供</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>パイプライン
    <ul>
      <li>ステップ1：Subredditの選択
        <ul>
          <li>手動で選ばれた一連のSubredditからデータを収集</li>
          <li>Subredditの選択により、個々のインスタンスに注釈を付けることなく、データセットの構成を調整することができる</li>
          <li>人々の画像を共有したり、コメントしたりすることを目的とするサブレディットは除外</li>
        </ul>
      </li>
      <li>ステップ2：画像投稿のフィルタリング
        <ul>
          <li>PushshiftとRedditのAPIを使用して、選択したSubredditに投稿されたすべての画像投稿をダウンロード</li>
          <li>画像はReddit、Imgur、Flickrの3つのドメインにホストされているもののみ収集</li>
          <li>人気のないコンテンツや不適切なコンテンツを避けるために、2つ未満の評価やNSFWマークのある投稿は除外</li>
        </ul>
      </li>
      <li>ステップ3：キャプションのクリーニング
        <ul>
          <li>Redditの投稿タイトルは他の大規模なソース（例：altテキスト）に比べてノイズが少ないため、テキストのクリーニングは最小限</li>
          <li>キャプションを小文字に変換し、文字のアクセント、絵文字、非ラテン文字を削除</li>
          <li>括弧で囲まれた部分を簡単なパターンマッチングで削除</li>
          <li>ソーシャルメディアのハンドル（’@’で始まる単語）を[USR]トークンに置き換え、ユーザーのプライバシーを保護し重複を減らす</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>プライバシー保護
    <ul>
      <li>RetinaFaceで信頼度0.9以上の顔が検出されたものを削除</li>
    </ul>
  </li>
  <li>NSFW画像
    <ul>
      <li>InceptionV3でポルノまたはヘンタイとして検出されたものを削除</li>
    </ul>
  </li>
  <li>Potentially derogatory language
    <ul>
      <li>軽蔑的な言葉を含むものを削除</li>
    </ul>
  </li>
  <li>Consent
    <ul>
      <li>ユーザーはデータセットになることを同意していないため、Redditの投稿が消えたらデータセットから削除され、削除申請できるフォームも用意</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>image-textデータセットとの比較
    <ul>
      <li><img src="/assets/images/posts/RedCaps/2.png" alt="" /></li>
    </ul>
  </li>
  <li>TOP20 Subreddit
    <ul>
      <li><img src="/assets/images/posts/RedCaps/3.png" alt="" /></li>
    </ul>
  </li>
  <li>Captionの長さ
    <ul>
      <li><img src="/assets/images/posts/RedCaps/4.png" alt="" /></li>
    </ul>
  </li>
  <li>言語的多様性の比較
    <ul>
      <li><img src="/assets/images/posts/RedCaps/5.png" alt="" /></li>
    </ul>
  </li>
  <li>言語統計
    <ul>
      <li>
        <p><img src="/assets/images/posts/RedCaps/6.png" alt="" /></p>
      </li>
      <li>VirTex-v2を大規模データセットでPretrainしたときの、7つのデータセットのゼロショット画像分類の比較
        <ul>
          <li>6つのデータセットで他の大規模データセットを上回る</li>
        </ul>
      </li>
      <li><img src="/assets/images/posts/RedCaps/7.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Caption" /><category term="Dataset" /><category term="Caption" /><summary type="html"><![CDATA[概要 ビジョンと言語のタスクのための大規模データセットは、検索エンジンをクエリにしたりHTMLのaltテキストを収集することで構築されているが、ウェブデータはノイズが多いため、品質を維持するために複雑なフィルタリングパイプラインが必要 最小限のフィルタリングで高品質なデータを収集するための代替データソースを探索 Redditから収集された1200万の画像とキャプションのペアのRedCapsという大規模なデータセットを紹介]]></summary></entry><entry><title type="html">OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models</title><link href="http://localhost:4000/vision%20and%20language/2023/05/15/OFASys.html" rel="alternate" type="text/html" title="OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models" /><published>2023-05-15T12:00:00+09:00</published><updated>2023-05-15T12:00:00+09:00</updated><id>http://localhost:4000/vision%20and%20language/2023/05/15/OFASys</id><content type="html" xml:base="http://localhost:4000/vision%20and%20language/2023/05/15/OFASys.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>マルチモーダルの汎用モデル学習システムOFASysを提案
    <ul>
      <li>7つ(TEXT、IMAGE、AUDIO、VIDEO、STRUCT、MOTION)のモダリティの23のタスク</li>
    </ul>
  </li>
  <li>複数モダリティのタスクを1行のコードで宣言することで、学習・推論用のタスクプランを自動生成する</li>
  <li>テキスト、画像、音声、動画、モーションデータを扱うことができる世界初の単一モデルOFA+も開発し、15個のタスクに調整されたモデルのわずか16％のパラメータで平均95％の性能を達成
<!--more-->
<img src="/assets/images/posts/OFASys/1.png" alt="" /></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>マルチモーダル用の汎用モデル学習システムOFASysを提案</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>マルチモーダル命令
    <ul>
      <li>タスクが何をするのか、データのモダリティの種類を指定する記述行
        <ul>
          <li><img src="/assets/images/posts/OFASys/2.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ユーザーインターフェイス
    <ul>
      <li>多様なデータやタスクに対応する命令形式（正規表現）
        <ul>
          <li><img src="/assets/images/posts/OFASys/3.png" alt="" /></li>
          <li><img src="/assets/images/posts/OFASys/4.png" alt="" /></li>
        </ul>
      </li>
      <li>例：入力-&gt;出力（Image Caption）
        <ul>
          <li><img src="/assets/images/posts/OFASys/5.png" alt="" /></li>
        </ul>
      </li>
      <li>例：可変長スロット(Object Detection)
        <ul>
          <li><img src="/assets/images/posts/OFASys/6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>システムデザイン
    <ul>
      <li>fairseqやtransformersなどのフレームワークは開発コストを削減したが、マルチモーダルやマルチタスクではデータ処理の実装や特徴抽出器などを手動で設定しなければならない
        <ul>
          <li>マルチモーダルやマルチタスクを単一のフレームワークで行なうOFASysとマルチタスク実行を管理するタスクスケジューラを開発</li>
        </ul>
      </li>
      <li>マルチデータプロセッシング
        <ul>
          <li>データの種類ごとに機械学習データに変換
            <ul>
              <li>テキストならトークン、オーディオならfbank特徴</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>計算機
        <ul>
          <li>T5やDiffusionに向けたU-Net、GPTなど様々なモデルに対応</li>
          <li>現在はtransformer Enc-DecとMixture-of-Experts（MoE）</li>
        </ul>
      </li>
      <li><img src="/assets/images/posts/OFASys/7.png" alt="" /></li>
      <li><img src="/assets/images/posts/OFASys/8.png" alt="" /></li>
    </ul>
  </li>
  <li>応用例：OFA+
    <ul>
      <li>OFA-Sysを用いて、テキスト、画像、音声、動画、モーションデータをオールインワンで扱えるGeneralistモデルを学習した</li>
      <li>OFA+ (Generalist)
        <ul>
          <li>OFA-baseの事前学習済み重みから学習</li>
          <li>90/270Mがモダリティ固有のパラメータ</li>
        </ul>
      </li>
      <li>OFA+ (Generalist MoE)
        <ul>
          <li>OFA-baseに基づくがVLMOの実装に近い</li>
          <li>275/455Mがモダリティ固有のパラメータ</li>
        </ul>
      </li>
      <li>どちらも7つのモダリティの17のタスクで学習し、タスク固有のFinetuningは行わない</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li><img src="/assets/images/posts/OFASys/9.png" alt="" /></li>
  <li><img src="/assets/images/posts/OFASys/10.png" alt="" /></li>
  <li>
    <p><img src="/assets/images/posts/OFASys/11.png" alt="" /></p>
  </li>
  <li>OFA+ (Specialist), OFA+ (Generalist), and OFA+ (Generalist MoE)
    <ul>
      <li>Generalist MoE &gt; Generalist</li>
      <li><img src="/assets/images/posts/OFASys/12.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Vision and Language" /><category term="Multimodal Pretraining" /><category term="Unified Frameworks" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 マルチモーダルの汎用モデル学習システムOFASysを提案 7つ(TEXT、IMAGE、AUDIO、VIDEO、STRUCT、MOTION)のモダリティの23のタスク 複数モダリティのタスクを1行のコードで宣言することで、学習・推論用のタスクプランを自動生成する テキスト、画像、音声、動画、モーションデータを扱うことができる世界初の単一モデルOFA+も開発し、15個のタスクに調整されたモデルのわずか16％のパラメータで平均95％の性能を達成]]></summary></entry></feed>