<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-05-04T18:42:51+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">軸屋敬介 | Keisuke Jikuya</title><entry><title type="html">Visual Programming: Compositional visual reasoning without training</title><link href="http://localhost:4000/object%20detection/2023/05/01/VisProg.html" rel="alternate" type="text/html" title="Visual Programming: Compositional visual reasoning without training" /><published>2023-05-01T12:00:00+09:00</published><updated>2023-05-01T12:00:00+09:00</updated><id>http://localhost:4000/object%20detection/2023/05/01/VisProg</id><content type="html" xml:base="http://localhost:4000/object%20detection/2023/05/01/VisProg.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>1枚または複数枚の画像と自然言語の命令を与え、GPT-3を利用して命令プログラムを作成し、そのプログラムを実行することで目的の出力を得るシステムVISPROGを提案</li>
  <li>命令プログラムの各行では、CVモデル・言語モデル・OpenCVの画像処理・演算子のいずれかのモジュールを実行し、後続で使用できる中間出力を生成している</li>
  <li>事実知識オブジェクトタグ付け・言語ガイド付き画像編集などの4つのタスクで柔軟性を実証
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>Neural Module Networksに比べて、GPT-3によって訓練を必要とせずに少数の例からプログラムを作成できる</li>
  <li>中間出力を確認することで、間違いの理由や視覚的根拠を得ることができる</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>GPT-3は入力と出力のデモを与えることで、入力から欲しい出力を得ることができる
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-2.png" alt="" /></li>
      <li>これを利用し、命令とプログラムのデモを与えることで、目的のプログラムを得る（画像編集タスクの例）
        <ul>
          <li><img src="/assets/images/posts/VisProg/VisProg-3.png" alt="" /></li>
        </ul>
      </li>
      <li>GPT-3が各モジュールの入出力や機能を理解できるように、説明的なモジュール名(Select, Replaceなど)、引数名(image, objectなど)、変数名(IMAGE, OBJ)を用いている
        <ul>
          <li>各モジュールはPythonのクラスとして実装されている
            <ul>
              <li><img src="/assets/images/posts/VisProg/VisProg-4.png" alt="" />
                <ol>
                  <li>行を解析して入力引数名と値、出力変数名を抽出</li>
                  <li>学習済みNNを含む計算を実行し、出力変数名と値でプログラムの状態を更新</li>
                  <li>htmlを用いて計算を視覚的に要約</li>
                </ol>
              </li>
            </ul>
          </li>
          <li>現在サポートされているモジュール（赤はNNモデル, 青は画像処理などのPythonルーチン）
            <ul>
              <li><img src="/assets/images/posts/VisProg/VisProg-5.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>視覚的要約
        <ul>
          <li>例（画像編集タスク）
            <ul>
              <li><img src="/assets/images/posts/VisProg/VisProg-6.png" alt="" /></li>
            </ul>
          </li>
          <li>例（NLVRタスク）
            <ul>
              <li><img src="/assets/images/posts/VisProg/VisProg-7.png" alt="" /></li>
            </ul>
          </li>
          <li>この視覚的要約によって、プログラムの論理的な正しさや失敗の原因が理解できる</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>タスクとプロンプト
    <ul>
      <li>合成VQAタスク
        <ul>
          <li><img src="/assets/images/posts/VisProg/VisProg-8.png" alt="" /></li>
          <li>例：「ヘルメットをかぶっている人の左側に小さなトラックがあるか、右側にあるのか？」</li>
        </ul>
      </li>
      <li>NLVR
        <ul>
          <li><img src="/assets/images/posts/VisProg/VisProg-9.png" alt="" /></li>
          <li>画像ペアに対するVQA</li>
        </ul>
      </li>
      <li>知識タグ付けタスク
        <ul>
          <li><img src="/assets/images/posts/VisProg/VisProg-10.png" alt="" /></li>
          <li>画像に写っている人物や物体の名前を識別</li>
        </ul>
      </li>
      <li>画像編集タスク
        <ul>
          <li><img src="/assets/images/posts/VisProg/VisProg-11.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>合成VQAタスク
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-12.png" alt="" /></li>
      <li>2.7ポイントUP</li>
    </ul>
  </li>
  <li>NLVR
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-13.png" alt="" /></li>
      <li>62.4%の高いゼロショット精度</li>
    </ul>
  </li>
  <li>知識タグ付けタスク
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-14.png" alt="" /></li>
      <li><img src="/assets/images/posts/VisProg/VisProg-15.png" alt="" /></li>
    </ul>
  </li>
  <li>画像編集タスク
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-16.png" alt="" /></li>
      <li><img src="/assets/images/posts/VisProg/VisProg-17.png" alt="" /></li>
    </ul>
  </li>
  <li>視覚的要約による失敗原因の解明とプロンプトの修正
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-18.png" alt="" /></li>
    </ul>
  </li>
</ul>

<h1 id="関連論文">関連論文</h1>

<ul>
  <li>PROGPROMPT: Generating Situated Robot Task Plans using Large Language Models</li>
</ul>]]></content><author><name></name></author><category term="Object Detection" /><category term="Transformer" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 1枚または複数枚の画像と自然言語の命令を与え、GPT-3を利用して命令プログラムを作成し、そのプログラムを実行することで目的の出力を得るシステムVISPROGを提案 命令プログラムの各行では、CVモデル・言語モデル・OpenCVの画像処理・演算子のいずれかのモジュールを実行し、後続で使用できる中間出力を生成している 事実知識オブジェクトタグ付け・言語ガイド付き画像編集などの4つのタスクで柔軟性を実証]]></summary></entry><entry><title type="html">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</title><link href="http://localhost:4000/object%20detection/2023/03/31/ViLD.html" rel="alternate" type="text/html" title="Open-vocabulary Object Detection via Vision and Language Knowledge Distillation" /><published>2023-03-31T23:30:00+09:00</published><updated>2023-03-31T23:30:00+09:00</updated><id>http://localhost:4000/object%20detection/2023/03/31/ViLD</id><content type="html" xml:base="http://localhost:4000/object%20detection/2023/03/31/ViLD.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>任意のテキストで物体検出をするオープンボキャブラリ物体検出器ViLD(Vision and Language knowledge Distillation)を提案</li>
  <li>オープンボキャブラリの画像分類である教師モデルから2段階の検出器である生徒モデルに知識蒸留する</li>
  <li>ResNetやALIGNをバックボーンとして、PASCAL VOC、COCO、Objects365で高精度が出た
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>任意のテキストで物体検出</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/ViLD/2.png" alt="" /></p>

<ul>
  <li>変数
    <ul>
      <li>教師データセットを基本サブセット$C_B$と新規サブセット$C_N$に分ける
        <ul>
          <li>$C_B$のみ学習に使う</li>
        </ul>
      </li>
      <li>テキストEncoder  $T()$, 画像Encoder $V()$</li>
    </ul>
  </li>
  <li>位置の検出
    <ul>
      <li>Mask R-CNNのような二段階物体検出器をベースとする</li>
    </ul>
  </li>
  <li>Open Vocab検出
    <ul>
      <li>検出した位置を切り抜き</li>
      <li>画像埋め込み
        <ul>
          <li>基本サブセット$C_B$を用いて位置提案ネットワークを学習し、提案領域を得る</li>
          <li>提案領域の切り抜きとリサイズをして，事前学習済み画像Encoder$V()$で画像埋め込みを得る</li>
        </ul>
      </li>
      <li>テキスト埋め込み
        <ul>
          <li>カテゴリー名をプロンプトテンプレート（例：「a photo of {category} in the scene」）にして，テキストEncoder  $T()$でテキスト埋め込みを得る</li>
        </ul>
      </li>
      <li>画像とテキストの埋め込み間のコサイン類似度を計算
        <ul>
          <li>すべての提案領域に対し，画像Encoder$V()$に通すので推論は低速</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Vision&amp;Languageの知識蒸留
    <ul>
      <li>推論の遅さを解決するため</li>
      <li>ViLD-text
        <ul>
          <li><img src="/assets/images/posts/ViLD/3.png" alt="" /></li>
          <li>検出器の分類を前述のテキスト埋め込みに置き換え</li>
          <li>背景というテキストは表現できないので独自の埋め込みを学習</li>
          <li>すべてのカテゴリとのCos類似度を出しクロスエントロピーを計算
            <ul>
              <li><img src="/assets/images/posts/ViLD/4.png" alt="" /></li>
            </ul>
          </li>
          <li>推論時には新規カテゴリに対して，テキスト埋め込みを追加することで新規カテゴリに対応</li>
        </ul>
      </li>
      <li>ViLD-image
        <ul>
          <li><img src="/assets/images/posts/ViLD/5.png" alt="" /></li>
          <li>Mask R-CNNで切り取った特徴が、前述の画像埋め込みと一致するように学習することで知識蒸留（L1距離の最小化）
            <ul>
              <li><img src="/assets/images/posts/ViLD/6.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>組み合わせた最終形態
        <ul>
          <li><img src="/assets/images/posts/ViLD/7.png" alt="" /></li>
          <li><img src="/assets/images/posts/ViLD/8.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li><img src="/assets/images/posts/ViLD/9.png" alt="" /></li>
  <li><img src="/assets/images/posts/ViLD/10.png" alt="" /></li>
  <li>PASCAL VOCで72.2 AP</li>
  <li>COCOで36.6 AP</li>
  <li>Objects365で11.8 AP</li>
</ul>]]></content><author><name></name></author><category term="Object Detection" /><category term="Object Detection" /><category term="Zero-shot Learning" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 任意のテキストで物体検出をするオープンボキャブラリ物体検出器ViLD(Vision and Language knowledge Distillation)を提案 オープンボキャブラリの画像分類である教師モデルから2段階の検出器である生徒モデルに知識蒸留する ResNetやALIGNをバックボーンとして、PASCAL VOC、COCO、Objects365で高精度が出た]]></summary></entry><entry><title type="html">Unified Visual Relationship Detection with Vision and Language Models</title><link href="http://localhost:4000/hoi/2023/03/28/UniVRD.html" rel="alternate" type="text/html" title="Unified Visual Relationship Detection with Vision and Language Models" /><published>2023-03-28T23:30:00+09:00</published><updated>2023-03-28T23:30:00+09:00</updated><id>http://localhost:4000/hoi/2023/03/28/UniVRD</id><content type="html" xml:base="http://localhost:4000/hoi/2023/03/28/UniVRD.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>Visual Relationship Detection(VRD)では、１つのデータセットから学習するため、画像ドメインと語彙に制約があり、汎用性と拡張性に限界がある</li>
  <li>Vision&amp;Languageモデルを活用し、複数のデータセットを統一するフレームワークUniVRDを提案</li>
  <li>HICO-DETにおいて60%アップの38.07mAP
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>Vision&amp;Languageモデルによる複数データセットの統一</li>
  <li>モデルや損失、訓練方法の改善</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/UniVRD/UniVRD-1.png" alt="" /></p>

<ul>
  <li>物体検出器
    <ul>
      <li>CLIPなどと同様にテキストEncoderとしてtransformer，画像EncoderとしてViTを使用</li>
      <li>画像Encoderは、プーリング層と最終層を削除して、トークン埋め込み層を追加する
        <ul>
          <li>トークン埋め込みを線形層に通すことで分類埋め込みを，FFNに通すことでBBを得る</li>
        </ul>
      </li>
      <li>Encoderのみで動作するため、知識抽出や事前学習を必要としない</li>
    </ul>
  </li>
  <li>Relationship Decoder
    <ul>
      <li>入力をEncoderのトークン埋め込みと関係クエリ（学習）として，関係埋め込みを得る
        <ul>
          <li>関係埋め込みを線形層に通すことで関係分類埋め込みを，FFNに通すことで主語と目的後のインデックスを得る</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>分類用テキスト埋め込み
    <ul>
      <li>物体／関係の分類では整数値ではなく，テキスト埋め込みを使用
        <ul>
          <li>物体の分類：「person」を「a photo of person」にしてテキストEncoderへ</li>
          <li>関係の分類：「person(主語), ride(述語), horse(目的)」を「a person riding a orse」にしてテキストEncoderへ</li>
        </ul>
      </li>
      <li>検出器はテキスト埋め込みに対してクラス確率を予測する</li>
    </ul>
  </li>
  <li>データ拡張
    <ul>
      <li>モザイク
        <ul>
          <li>画像をグリッドにすることで、モデルが見るスケールの幅を広げる</li>
        </ul>
      </li>
      <li>テキストプロンプティング
        <ul>
          <li>CLIPのように、プロンプトテンプレートを使用
            <ul>
              <li>物体：「a photo of &lt;object&gt;」</li>
              <li>関係：「a &lt;subject&gt; &lt;predicate&gt;ing a &lt;object&gt;」</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>学習
    <ul>
      <li>物体検出とDecoderを順番に学習したほうが安定的かつ効果的</li>
      <li>Decoderの学習時に学習データが限られている場合は物体検出器を凍結したほうがよい</li>
      <li>Loss関数
        <ul>
          <li><img src="/assets/images/posts/UniVRD/UniVRD-2.png" alt="" /></li>
          <li><img src="/assets/images/posts/UniVRD/UniVRD-3.png" alt="" /></li>
          <li><img src="/assets/images/posts/UniVRD/UniVRD-4.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>HOI(Himan-Object Interaction)
    <ul>
      <li>HICO-DETでSOTA
        <ul>
          <li><img src="/assets/images/posts/UniVRD/UniVRD-5.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>SGG(Scene Graph Generation)
    <ul>
      <li>Visual Genome
        <ul>
          <li><img src="/assets/images/posts/UniVRD/UniVRD-6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="HOI" /><category term="Transformer" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 Visual Relationship Detection(VRD)では、１つのデータセットから学習するため、画像ドメインと語彙に制約があり、汎用性と拡張性に限界がある Vision&amp;Languageモデルを活用し、複数のデータセットを統一するフレームワークUniVRDを提案 HICO-DETにおいて60%アップの38.07mAP]]></summary></entry><entry><title type="html">GO-Finder: A Registration-Free Wearable System for Assisting Users in Finding Lost Objects via Hand-Held Object Discovery</title><link href="http://localhost:4000/object%20detection/2023/03/14/GO-Finder.html" rel="alternate" type="text/html" title="GO-Finder: A Registration-Free Wearable System for Assisting Users in Finding Lost Objects via Hand-Held Object Discovery" /><published>2023-03-14T23:30:00+09:00</published><updated>2023-03-14T23:30:00+09:00</updated><id>http://localhost:4000/object%20detection/2023/03/14/GO-Finder</id><content type="html" xml:base="http://localhost:4000/object%20detection/2023/03/14/GO-Finder.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>登録不要のウェアラブルカメラを用いた物体の発見支援システムGO-Finderを提案</li>
  <li>手持ちの物体を自動的に検出しグループ化しておくことで、アプリから対象物の最後の出現を取得できる
    <ul>
      <li>手で扱う物体に限定することで、対象となる物体を大幅に削減</li>
    </ul>
  </li>
  <li>物体画像をクエリとして利用し、候補の中から物体を選択することができる
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>従来のシステムではユーザーが事前に対象物を登録する必要があった</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>概要
    <ul>
      <li>必要なもの
        <ul>
          <li>ウェアラブルカメラ、処理サーバー、参照用スマートフォン</li>
          <li><img src="/assets/images/posts/GO-Finder/GoFinder-2.png" alt="" /></li>
        </ul>
      </li>
      <li>観察フェーズ
        <ul>
          <li>ウェアラブルカメラで連続的に画像を撮って、サーバーに送信</li>
          <li>サーバーは手持ちの物体を検出・追跡</li>
          <li>物体の外観によってクラスタリング</li>
        </ul>
      </li>
      <li>検索フェーズ
        <ul>
          <li><img src="/assets/images/posts/GO-Finder/GoFinder-3.png" alt="" /></li>
          <li>スマートフォンのインターフェースを使って、処理結果を受信</li>
          <li>検索したい物体を選択</li>
          <li>物体が最後に出現したシーンがポップアップ画面で出てくる</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>アルゴリズム
    <ul>
      <li><img src="/assets/images/posts/GO-Finder/GoFinder-4.png" alt="" /></li>
      <li>手持ち物体の検出
        <ul>
          <li>既存手法「Understanding Human Hands in Contact at Internet Scale」を用いて、バウンディングボックスと携帯型物体判定と接触状態を取得</li>
          <li>携帯型物体判定かつ接触状態の物体のバウンディングボックスを取得</li>
          <li>フレームの半分の辺長はノイズとして除外</li>
        </ul>
      </li>
      <li>検出されたバウンディングボックスを、外観特徴に基づいてクラスタリング
        <ul>
          <li>ステージ１：トラッキングしたものを同じクラスタへ(c)</li>
          <li>ステージ２：トラッキングが失敗した場合、ImageNet学習済みResNetに通した特徴に対してコサイン類似度を計算し、閾値を超えたらクラスタへ(d上)
            <ul>
              <li>条件を満たすクラスタが1つもない場合は新しいクラスタを作成</li>
            </ul>
          </li>
          <li>ステージ３：クラスタ単位でコサイン類似度行列を作成、最大値と中央値が閾値を超えたらクラスタへ(d下)</li>
        </ul>
      </li>
      <li>手の外観や類似の質感による誤った関連付けを防ぐために、ヒューリスティックを導入
        <ul>
          <li>バウンディングボックスのアスペクト比が1.5より大きい場合</li>
          <li>肌色領域(カラーヒストグラム)の比率が0.3より大きい場合</li>
          <li>物体と手の面積比が1.5より大きい場合</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>12名のボランティアに指定した場所に物体を隠してもらい、15分のインターバルで位置を忘れさせ、システムを利用しながら隠した物体の一部を持ち帰るように指示
    <ul>
      <li><img src="/assets/images/posts/GO-Finder/GoFinder-5.png" alt="" /></li>
      <li>パフォーマンス
        <ul>
          <li><img src="/assets/images/posts/GO-Finder/GoFinder-6.png" alt="" /></li>
        </ul>
      </li>
      <li>タスク完了時間
        <ul>
          <li><img src="/assets/images/posts/GO-Finder/GoFinder-7.png" alt="" /></li>
        </ul>
      </li>
      <li>クラスタの様子
        <ul>
          <li><img src="/assets/images/posts/GO-Finder/GoFinder-8.png" alt="" /></li>
          <li>いくつかのクラスタは分割され過ぎている</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Object Detection" /><category term="Object Detection" /><summary type="html"><![CDATA[概要 登録不要のウェアラブルカメラを用いた物体の発見支援システムGO-Finderを提案 手持ちの物体を自動的に検出しグループ化しておくことで、アプリから対象物の最後の出現を取得できる 手で扱う物体に限定することで、対象となる物体を大幅に削減 物体画像をクエリとして利用し、候補の中から物体を選択することができる]]></summary></entry><entry><title type="html">Flamingo: a Visual Language Model for Few-Shot Learning</title><link href="http://localhost:4000/vision%20and%20language/2023/03/14/Flamingo.html" rel="alternate" type="text/html" title="Flamingo: a Visual Language Model for Few-Shot Learning" /><published>2023-03-14T22:00:00+09:00</published><updated>2023-03-14T22:00:00+09:00</updated><id>http://localhost:4000/vision%20and%20language/2023/03/14/Flamingo</id><content type="html" xml:base="http://localhost:4000/vision%20and%20language/2023/03/14/Flamingo.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>少数のアノテーションで重みの更新なしに新しいタスクに迅速に対応できるVision&amp;LanguageモデルであるFlamingoを提案</li>
  <li>数千倍のタスク専用データでFinetuningに対して、6/16のタスクでSotA
<!--more-->
<img src="/assets/images/posts/Flamingo/1.png" alt="" />
<img src="/assets/images/posts/Flamingo/2.png" alt="" /></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>言語モデルのFew-shot学習をPerceiver-basedアーキテクチャなどを用いて画像やビデオを入力できるように拡張</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/Flamingo/3.png" alt="" /></p>

<ul>
  <li>概要
    <ul>
      <li>Vision Encoderから画像または動画の特徴を取得し、Perceiver Resamplerで固定数の画像トークンにする</li>
      <li>画像トークンをCross AttentionでLM layerの間に挟むことで、LM Blockを視覚情報で条件付けする</li>
    </ul>
  </li>
  <li>Vision Encoder
    <ul>
      <li>事前学習済みNormalizerFree ResNet (NFNet)を使用</li>
      <li>ビデオの場合は1FPSごとにEncoderに通し、時間埋め込みを追加</li>
    </ul>
  </li>
  <li>Perceiver Resampler
    <ul>
      <li>サイズの異なる特徴マップを固定数の画像トークンを生成
        <ul>
          <li>計算の複雑さを軽減</li>
        </ul>
      </li>
      <li>V&amp;L Sampler Moduleによって通常のTransformerやMLPを凌駕</li>
    </ul>
  </li>
  <li>GATED XATTN-DENSE layers within a frozen pretrained LM
    <ul>
      <li><img src="/assets/images/posts/Flamingo/4.png" alt="" /></li>
      <li>事前学習済みのLM layer(Chinchilla)を用意し、重みは固定する</li>
      <li>その中にGATED XATTN-DENSE layersを挿入</li>
      <li>残差ブロックの接続前にtanhを掛けることで、学習の安定化と性能の向上</li>
    </ul>
  </li>
  <li>Vision&amp;Languageデータセットで学習
    <ul>
      <li>M3W(画像とテキストが交互に並ぶデータセット)</li>
      <li>ALIGN, LTIP(画像とテキストのペア)</li>
      <li>VTP(動画とテキストのペア)</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>Few-shot learning
    <ul>
      <li><img src="/assets/images/posts/Flamingo/5.png" alt="" /></li>
      <li>FlamingoはFinetuningを行わずに16個中6個のタスクでSotA
        <ul>
          <li>比較対象(100%)はFinetuning（重みの更新）済みの最新手法</li>
        </ul>
      </li>
      <li>また、白色のZero/Few Shotを上回る
        <ul>
          <li><img src="/assets/images/posts/Flamingo/6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Finetuning
    <ul>
      <li><img src="/assets/images/posts/Flamingo/7.png" alt="" /></li>
      <li>FewshotでSotAを上回らなかった9個中5個のタスクでSotA</li>
    </ul>
  </li>
  <li>Ablation studies
    <ul>
      <li><img src="/assets/images/posts/Flamingo/8.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Vision and Language" /><category term="Vision and Language" /><category term="Few-shot Learning" /><category term="Transformer" /><summary type="html"><![CDATA[概要 少数のアノテーションで重みの更新なしに新しいタスクに迅速に対応できるVision&amp;LanguageモデルであるFlamingoを提案 数千倍のタスク専用データでFinetuningに対して、6/16のタスクでSotA]]></summary></entry><entry><title type="html">QAHOI: Query-Based Anchors for Human-Object Interaction Detection</title><link href="http://localhost:4000/hoi/2023/03/07/QAHOI.html" rel="alternate" type="text/html" title="QAHOI: Query-Based Anchors for Human-Object Interaction Detection" /><published>2023-03-07T13:00:00+09:00</published><updated>2023-03-07T13:00:00+09:00</updated><id>http://localhost:4000/hoi/2023/03/07/QAHOI</id><content type="html" xml:base="http://localhost:4000/hoi/2023/03/07/QAHOI.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>transformerベースの手法QAHOI（Query-Based Anchors for Human-Object Interac
tion detection）を提案</li>
  <li>マルチスケールで特徴を抽出し，クエリベースのアンカーを用いてHOIを予測する</li>
  <li>強力なバックボーンによって、精度が大幅に向上した
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>1ステージアプローチでは，物体の位置や大きさのばらつきを無視している</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/QAHOI/1.png" alt="" /></p>

<ul>
  <li>階層型バックボーンで4段階の特徴マップを抽出し，1×1畳み込みで次元を整えて平坦化</li>
  <li>Deformable transformer Encoderでさらに特徴抽出</li>
  <li>Deformable transformer Decoder
    <ul>
      <li>クエリ埋め込みを，HOI問い合わせ埋め込みと位置埋め込みに等分割</li>
      <li>位置埋め込みからアンカーを作成</li>
      <li>それらをTransformer Decoderで処理し，HOI埋め込みを取得</li>
      <li><img src="/assets/images/posts/QAHOI/2.png" alt="" /></li>
    </ul>
  </li>
  <li>Interaction Head
    <ul>
      <li><img src="/assets/images/posts/QAHOI/3.png" alt="" /></li>
      <li>Decoderの出力とアンカーを利用して， 人間と物体のBB・物体のクラス・インタラクションを予測</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>HICO-DET
    <ul>
      <li><img src="/assets/images/posts/QAHOI/4.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="Human-Object Interaction" /><category term="Transformer" /><summary type="html"><![CDATA[概要 transformerベースの手法QAHOI（Query-Based Anchors for Human-Object Interac tion detection）を提案 マルチスケールで特徴を抽出し，クエリベースのアンカーを用いてHOIを予測する 強力なバックボーンによって、精度が大幅に向上した]]></summary></entry><entry><title type="html">FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection</title><link href="http://localhost:4000/hoi/2023/02/21/FGAHOI.html" rel="alternate" type="text/html" title="FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection" /><published>2023-02-21T13:00:00+09:00</published><updated>2023-02-21T13:00:00+09:00</updated><id>http://localhost:4000/hoi/2023/02/21/FGAHOI</id><content type="html" xml:base="http://localhost:4000/hoi/2023/02/21/FGAHOI.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>MSS, HSAM, TAMという3つから成るEnd-to-endのtransformerベースの手法(FGAHOI)を提案
    <ul>
      <li>MSSは人間、物体、インタラクション領域の特徴を抽出</li>
      <li>HSAMとTAMは抽出された特徴量とクエリ埋め込みを
  階層的な空間視点とタスク視点で順番に意味的に整列・結合</li>
      <li>複雑な学習を軽減するために、新しい学習戦略Stage-wise Training Strategyを設計</li>
    </ul>
  </li>
  <li>新規のデータセットHOI-SDCを提案</li>
  <li>既存手法から大幅に精度向上
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>以下の問題を低減
    <ul>
      <li>複雑な背景情報を持つ画像からいかにして重要な特徴を抽出するか</li>
      <li>抽出した特徴量とクエリ埋め込みをどのように意味的に整合させるか</li>
    </ul>
  </li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>全体構造
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-1.png" alt="" /></li>
    </ul>
  </li>
  <li>Multi-Scale Features Extractor
    <ul>
      <li>
\[M = F_{encoder}(F_{flatten}(\phi(x)),p,s,r,l) \in \mathbb{R}^{N_s \times C_d}\]
      </li>
      <li>事前学習済みBackbone(Swin Transformer)でマルチスケール特徴を抽出</li>
      <li>transformer encoderで符号化して意味特徴量を得る</li>
    </ul>
  </li>
  <li>Decoder
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-2.png" alt="" /></li>
      <li>Multi-Scale Sampling (MSS)
        <ul>
          <li>
\[x^i_s = F_{sample} (reshape(M)^i, A, size^i, bilinear)\]
          </li>
          <li>transformer encoderの特徴を元の形状に整形</li>
          <li>小さなインスタンスを検出する浅い特徴量では小さくサンプリング</li>
          <li>大きなインスタンスを検出する深い特徴量では大きくサンプリング
            <ul>
              <li>サイズに関係なくインスタンスを検出するため</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Hierarchical Spatial-Aware Merging (HSAM)
        <ul>
          <li>サンプリングした各層の特徴をKeyとValue，位置PとコンテキストCをQueryとしてMHA
            <ul>
              <li>
\[C_u = C + F_{MHA}\big((C+P)W^q, (C+P)W^k, CW^v \big)\]
              </li>
              <li>
\[x^u_m = F_{concat} (\mathrm{head}_1, ... , \mathrm{head}_h) W^O\]
                <ul>
                  <li>
\[\mathrm{where} \space \mathrm{head}_n = \mathrm{Softmax\big(\frac{(C_u W^q_n)\cdot(x^i_s W^k_n)^T}{\sqrt{d_k}}\big)} (x^i_s W^v_n)\]
                  </li>
                </ul>
              </li>
              <li>特徴内Attention</li>
            </ul>
          </li>
          <li>MHAに通した各層の特徴をConcat
            <ul>
              <li>
\[X_m = F_{concat} ({x^i_m}_{i=0,1,2}) \in \mathbb{R}^{B \times N_q \times N_L \times N_{hd}}\]
              </li>
            </ul>
          </li>
          <li>Concatした特徴をKeyとValue，位置PとコンテキストCをQueryとしてMHA
            <ul>
              <li>
\[X_u = F_{concat} (\mathrm{head}_1, ... , \mathrm{head}_h) W^O\]
                <ul>
                  <li>
\[\mathrm{where} \space \mathrm{head}_n = \mathrm{Softmax\big(\frac{(C_u W^q_n)\cdot(X_m W^k_n)^T}{\sqrt{d_k}}\big)} (X W^v_n)\]
                  </li>
                </ul>
              </li>
              <li>特徴間Attention</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Task-Aware Mergin (TAM)
        <ul>
          <li>HSAMの出力特徴とコンテンツ埋め込みを融合しCross Attention
            <ul>
              <li>
\[X = F_{stack} (C_u, X_u) \in \mathbb{R}^{B \times N_q \times (2 \times N_{hd})}\]
              </li>
              <li>
\[X_{switch} = F_{stack} (\mathrm{head}_1, ... , \mathrm{head}_h) W^O\]
                <ul>
                  <li>
\[\mathrm{where} \space \mathrm{head}_n = \mathrm{Softmax\big(\frac{(C_u W^q_n)\cdot(X W^k_n)^T}{\sqrt{d_k}}\big)} (X W^v_n)\]
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>適切なチャンネルを選択する動的なスイッチを生成
            <ul>
              <li>
\[Switch^{\gamma} = F_{normalize}(D_{mlp}(X_{switch}))^{\gamma} \in \mathbb{R}^{B \times N_q \times 2 \times 2}\]
              </li>
            </ul>
          </li>
          <li>HSAMの出力特徴に対し，スイッチで一部を選択
            <ul>
              <li>
\[U^{\gamma} = F_{Max} \{Switch^{\gamma}_{i,0} \odot X^{\gamma}_u + Switch^{\gamma}_{i,1}\}_{i=0,1} + C^{\gamma}_u\]
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Decoding with Fine-Grained Anchor
        <ul>
          <li>内容埋め込みを線形層、リシェイプ、SoftMaxに通すことで
  Cross AttentionのQueryとなるAnchorとKeyとなるAttention weightを生成</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>HOI Detection Head
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-13.png" alt="" /></li>
      <li>HOI埋め込みと初期アンカーを利用して，人間と物体のBB・物体のクラス・インタラクションを予測
        <ul>
          <li><img src="/assets/images/posts/FGAHOI/FGAHOI-14.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>HICO-DET
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-15.png" alt="" /></li>
    </ul>
  </li>
  <li>V-COCO
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-16.png" alt="" /></li>
    </ul>
  </li>
  <li>HOI-SDC
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-17.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="Human-Object Interaction" /><category term="FGAHOI" /><category term="Fine-Grained Anchors" /><category term="Noisy Background" /><category term="Semantically Aligning" /><summary type="html"><![CDATA[概要 MSS, HSAM, TAMという3つから成るEnd-to-endのtransformerベースの手法(FGAHOI)を提案 MSSは人間、物体、インタラクション領域の特徴を抽出 HSAMとTAMは抽出された特徴量とクエリ埋め込みを 階層的な空間視点とタスク視点で順番に意味的に整列・結合 複雑な学習を軽減するために、新しい学習戦略Stage-wise Training Strategyを設計 新規のデータセットHOI-SDCを提案 既存手法から大幅に精度向上]]></summary></entry><entry><title type="html">Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection</title><link href="http://localhost:4000/hoi/2023/02/21/STIP.html" rel="alternate" type="text/html" title="Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection" /><published>2023-02-21T11:00:00+09:00</published><updated>2023-02-21T11:00:00+09:00</updated><id>http://localhost:4000/hoi/2023/02/21/STIP</id><content type="html" xml:base="http://localhost:4000/hoi/2023/02/21/STIP.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>新しいtransformerベースのHOI手法のStructure-aware Transformer over Interaction Proposals (STIP)を提案</li>
  <li>「インタラクションのある人間と物体のペア提案」と「構造考慮型transformerで提案をHOIに変換」の2つのフェーズでHOIを予測</li>
  <li>構造考慮型transformerはバニラtransformerに対し、全体的意味構造および各相互作用提案内のヒト／モノの局所的空間構造を追加的に符号化することでHOI予測を強化している
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li><img src="/assets/images/posts/STIP/1.png" alt="" /></li>
  <li>他のHOIに依存していることを考慮させた（例：「人間が（野球の）グローブをつけている」ゆえに、「（別の）人間がバットを持っている」）</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/STIP/2.png" alt="" /></li>
  <li>DETR
    <ul>
      <li>人間と物体のインスタンスを検出</li>
    </ul>
  </li>
  <li>Interaction Proposal Network (IPN)
    <ul>
      <li>すべての人間と物体のペアを構築</li>
      <li>すべてのペアに対してインタラクションの確率を外観特徴と空間特徴と言語特徴をMLPに入れて予測
        <ul>
          <li>外観特徴：DETRで得られる特徴</li>
          <li>空間特徴：$[dx, dy, dis, A_h, A_o, I, U]$
            <ul>
              <li>$dx$ : 人間と物体のx距離</li>
              <li>$dy$ : 人間と物体のy距離</li>
              <li>$dis$ : 人間と物体のユークリッド</li>
              <li>$A_h$ : 人間の面積</li>
              <li>$A_o$ : 物体の面積</li>
              <li>$I$ : 人間と物体の面積の積集合</li>
              <li>$U$ : 人間と物体の面積の和集合</li>
            </ul>
          </li>
          <li>言語特徴：物体のラベル(OneHot)を200次元のベクトルに</li>
        </ul>
      </li>
      <li>インタラクションの確率が高い上位K組を提案として出力
        <ul>
          <li>
\[L_{proposal} = \frac{1}{\sum^N_{i=1}z_i} \sum^N_{i=1} FL(\hat{z}_i, z_i)\]
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>インタラクション中心グラフ構築
    <ul>
      <li>インタラクション意味構造
        <ul>
          <li><img src="/assets/images/posts/STIP/4.png" alt="" /></li>
          <li>インタラクション提案をグラフノードとしてグラフを構築</li>
          <li>6つのクラス
            <ul>
              <li>disjunctive
                <ul>
                  <li>人間も物体もインスタンスを共有していない</li>
                </ul>
              </li>
              <li>same-human</li>
              <li>same-object
                <ul>
                  <li>人間／物体のインスタンスのみ同じ</li>
                </ul>
              </li>
              <li>series-opposing</li>
              <li>series
                <ul>
                  <li>人間／物体のインスタンスが物体／人間のインスタンスと同じ</li>
                </ul>
              </li>
              <li>same-pair
                <ul>
                  <li>人間と物体の両方のインスタンスが同じ</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>インタラクション空間構造
        <ul>
          <li><img src="/assets/images/posts/STIP/5.png" alt="" /></li>
          <li>局所的な空間特徴を考慮させる</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Structure-aware transformer
    <ul>
      <li>Structure-aware Self-Attention
        <ul>
          <li>
\[e^{self}_{ij} = \frac{(W_q q_i)^T (W_k q_j + \psi (q_j, E_{dep}(d_{ij})))}{\sqrt{d_{key}}}\]
          </li>
          <li>IPNのK組の提案に対してSelf-Attention
            <ul>
              <li>Keyに対してインタラクション意味構造の6つのクラスで意味依存性を付与する</li>
            </ul>
          </li>
          <li>$E_{dep}$は意味依存を符号化する2層MLP</li>
        </ul>
      </li>
      <li>Structure-aware Cross-attention
        <ul>
          <li><img src="/assets/images/posts/STIP/7.png" alt="" /></li>
          <li>
\[e^{cross}_{ij} = \frac{(W_{\hat{q}} \hat{q}_i)^T (W_{\hat{k}} x_j + pos_j+ \phi (x_j, E_{lay}(l_{ij})))}{\sqrt{d_{key}}}\]
          </li>
          <li>K組の中間HOI特徴をQuery，画像特徴マップをKeyとValueとしCross-attention
            <ul>
              <li>Keyに対してインタラクション空間構造の5つのクラスを付与</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>最終出力
    <ul>
      <li>2層MLPでインタラクションクラスを予測
        <ul>
          <li>Focal Loss likeな損失関数
            <ul>
              <li>
\[L_{cls} = \frac{1}{\sum^N_{i=1} \sum^C_{c=1}} \sum^N_{i=1} \sum^C_{c=1} FL (\hat{y}_{ic}, y_{ic})\]
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>全体の損失関数
        <ul>
          <li>
\[L_{STIP} = L_{proposal} + L_{cls}\]
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>V-COCO
    <ul>
      <li><img src="/assets/images/posts/STIP/11.png" alt="" /></li>
    </ul>
  </li>
  <li>HICO-DET
    <ul>
      <li><img src="/assets/images/posts/STIP/12.png" alt="" /></li>
    </ul>
  </li>
  <li>提案のK組による精度比較
    <ul>
      <li><img src="/assets/images/posts/STIP/13.png" alt="" /></li>
    </ul>
  </li>
  <li>構造考慮型transformerのレイヤー数によおる精度比較
    <ul>
      <li><img src="/assets/images/posts/STIP/14.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="Human-Object Interaction Detection" /><category term="Transformer" /><summary type="html"><![CDATA[概要 新しいtransformerベースのHOI手法のStructure-aware Transformer over Interaction Proposals (STIP)を提案 「インタラクションのある人間と物体のペア提案」と「構造考慮型transformerで提案をHOIに変換」の2つのフェーズでHOIを予測 構造考慮型transformerはバニラtransformerに対し、全体的意味構造および各相互作用提案内のヒト／モノの局所的空間構造を追加的に符号化することでHOI予測を強化している]]></summary></entry><entry><title type="html">CLRNet: Cross Layer Refinement Network for Lane Detection</title><link href="http://localhost:4000/lane%20detection/2022/12/15/CLRNet.html" rel="alternate" type="text/html" title="CLRNet: Cross Layer Refinement Network for Lane Detection" /><published>2022-12-15T12:00:00+09:00</published><updated>2022-12-15T12:00:00+09:00</updated><id>http://localhost:4000/lane%20detection/2022/12/15/CLRNet</id><content type="html" xml:base="http://localhost:4000/lane%20detection/2022/12/15/CLRNet.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>特徴抽出したFPN構造の特徴マップを、上位から下位まで複合的に活用する車線検出手法であるCross Layer Refinement Network (CLRNet)を提案</li>
  <li>CULaneとTuSimpleとLLAMASのデータセットで従来手法を上回る
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>車線の部分領域と全体を組み合わせて大域的な特徴表現を獲得するROIGatherを使用</li>
  <li>車線の検出結果に対するIoUとしてLIoU(Line IoU)を定義し、LIoUの最大化を損失関数に含んだ</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/CLRNet/img1.png" alt="" /></li>
  <li>二次元の点列$𝑃$（レーン事前分布）をネットワークで出力
    <ul>
      <li>前景と背景の確率、線の長さ、角度、予測と正解の水平距離</li>
    </ul>
  </li>
  <li>ResNetをBackboneとし、FPN構造から特徴マップ$𝐿_0, 𝐿_1, 𝐿_2$を生成</li>
  <li>点列$P$と特徴マップ$L$を組み合わせる
    <ul>
      <li>
\[P_t = P_{t-1} \circ R_t (L_{t-1},P_{t-1})\]
      </li>
      <li>上位から下位の特徴を活用するため</li>
    </ul>
  </li>
  <li>点列Pをもとに特徴マップ$L$をROIAlignで抽出</li>
  <li>Attentionをとる
    <ul>
      <li>
\[\mathcal{G} = \mathcal{W}\mathcal{X}^T_f\]
      </li>
      <li>
\[\mathcal{W} = f(\frac{\mathcal{X}^T_p \mathcal{X}_f}{\sqrt{C}})\]
      </li>
    </ul>
  </li>
  <li>Line IoU Loss
    <ul>
      <li>点を別々の変数として扱うため、既存の距離損失を用いると精度が低くなってしまう</li>
      <li>そこで、車線専用のLossを提案
        <ul>
          <li>
\[IoU = \frac{d^\omicron_i}{d^u_i} = \frac{\min(x^p_i+e, x^q_i+e) - \max(x^p_i+e, x^q_i+e)}{\max(x^p_i-e, x^q_i-e) - \min(x^p_i-e, x^q_i-e)}\]
          </li>
          <li>
\[LIoU = \frac{\sum^N_{i=1} d^\omicron_i}{\sum^N_{i=1} d^u_i}\]
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>CULane
    <ul>
      <li><img src="/assets/images/posts/CLRNet/img7.png" alt="" /></li>
    </ul>
  </li>
  <li>LLAMAS
    <ul>
      <li><img src="/assets/images/posts/CLRNet/img8.png" alt="" /></li>
    </ul>
  </li>
  <li>TuSimple
    <ul>
      <li><img src="/assets/images/posts/CLRNet/img9.png" alt="" /></li>
      <li><img src="/assets/images/posts/CLRNet/img10.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Lane Detection" /><category term="Lane Detection" /><summary type="html"><![CDATA[概要 特徴抽出したFPN構造の特徴マップを、上位から下位まで複合的に活用する車線検出手法であるCross Layer Refinement Network (CLRNet)を提案 CULaneとTuSimpleとLLAMASのデータセットで従来手法を上回る]]></summary></entry><entry><title type="html">CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution</title><link href="http://localhost:4000/lane%20detection/2022/12/14/CondLaneNet.html" rel="alternate" type="text/html" title="CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution" /><published>2022-12-14T12:00:00+09:00</published><updated>2022-12-14T12:00:00+09:00</updated><id>http://localhost:4000/lane%20detection/2022/12/14/CondLaneNet</id><content type="html" xml:base="http://localhost:4000/lane%20detection/2022/12/14/CondLaneNet.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>既存の車線検出は密集線や分岐線のような複雑な場合に苦労している(下図)</li>
  <li>車線を検出し、次に各車線の形状を予測する車線検出フレームワークであるCondLaneNetを提案</li>
  <li>3つのベンチマークデータセットで最先端手法を凌駕
<!--more--></li>
  <li><img src="/assets/images/posts/CondLaneNet/img1.png" alt="" /></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>密集線や分岐線などの複雑な車線を検出する問題を克服</li>
  <li>CULaneでは78.14 F1スコアと220 FPSを達成するなど，精度と効率の両立が可能</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/CondLaneNet/img2.png" alt="" /></li>
  <li>事前学習済みResNetをBackboneとしてFPNを用いて、マルチスケール特徴を得る
    <ul>
      <li>車線は細長いため、文脈特徴の抽出のためにBackboneの最終層にTransformer Encoderを追加
        <ul>
          <li><img src="/assets/images/posts/CondLaneNet/img3.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Proposal head
    <ul>
      <li>線の始点を予測する
        <ul>
          <li>CenterNetに従うが、細長い線は中心を見つけることが難しいため始点</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Recurrent Instance Module
    <ul>
      <li><img src="/assets/images/posts/CondLaneNet/img4.png" alt="" /></li>
      <li>予測した始点の特徴量から動的カーネルパラメータを再帰的に予測</li>
      <li>密な線や複数の線が同一の始点から始まる場合(ex.分岐)に対応</li>
    </ul>
  </li>
  <li>Conditional shape head
    <ul>
      <li>RIMの動的カーネルパラメータを使って畳み込み、マルチスケール特徴から各線の形状を予測
        <ul>
          <li><img src="/assets/images/posts/CondLaneNet/img5.png" alt="" /></li>
        </ul>
      </li>
      <li>Location maps：行ごと列ごとに予測</li>
      <li>Offset maps：行ごとの水平方向の正確な位置を予測</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>可視化
    <ul>
      <li>上からCurveLanes、CULane、TuSimple
        <ul>
          <li><img src="/assets/images/posts/CondLaneNet/img6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>CurveLanes
    <ul>
      <li><img src="/assets/images/posts/CondLaneNet/img7.png" alt="" /></li>
    </ul>
  </li>
  <li>CULane
    <ul>
      <li><img src="/assets/images/posts/CondLaneNet/img8.png" alt="" /></li>
    </ul>
  </li>
  <li>TuSimple
    <ul>
      <li><img src="/assets/images/posts/CondLaneNet/img9.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Lane Detection" /><category term="Lane Detection" /><summary type="html"><![CDATA[概要 既存の車線検出は密集線や分岐線のような複雑な場合に苦労している(下図) 車線を検出し、次に各車線の形状を予測する車線検出フレームワークであるCondLaneNetを提案 3つのベンチマークデータセットで最先端手法を凌駕]]></summary></entry></feed>