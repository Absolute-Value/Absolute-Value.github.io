<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-11-22T17:38:23+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">軸屋敬介 | Keisuke Jikuya</title><entry><title type="html">RedCaps: Web-curated image-text data created by the people, for the people</title><link href="http://localhost:4000/caption/2023/07/03/RedCaps.html" rel="alternate" type="text/html" title="RedCaps: Web-curated image-text data created by the people, for the people" /><published>2023-07-03T10:50:00+09:00</published><updated>2023-07-03T10:50:00+09:00</updated><id>http://localhost:4000/caption/2023/07/03/RedCaps</id><content type="html" xml:base="http://localhost:4000/caption/2023/07/03/RedCaps.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>ビジョンと言語のタスクのための大規模データセットは、検索エンジンをクエリにしたりHTMLのaltテキストを収集することで構築されているが、ウェブデータはノイズが多いため、品質を維持するために複雑なフィルタリングパイプラインが必要</li>
  <li>最小限のフィルタリングで高品質なデータを収集するための代替データソースを探索</li>
  <li>Redditから収集された1200万の画像とキャプションのペアのRedCapsという大規模なデータセットを紹介
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>Redditから収集された画像とキャプションのペアで構成</li>
  <li>独自のデータ収集パイプラインやフィルタリング手法を使用して高品質なデータを提供</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>パイプライン
    <ul>
      <li>ステップ1：Subredditの選択
        <ul>
          <li>手動で選ばれた一連のSubredditからデータを収集</li>
          <li>Subredditの選択により、個々のインスタンスに注釈を付けることなく、データセットの構成を調整することができる</li>
          <li>人々の画像を共有したり、コメントしたりすることを目的とするサブレディットは除外</li>
        </ul>
      </li>
      <li>ステップ2：画像投稿のフィルタリング
        <ul>
          <li>PushshiftとRedditのAPIを使用して、選択したSubredditに投稿されたすべての画像投稿をダウンロード</li>
          <li>画像はReddit、Imgur、Flickrの3つのドメインにホストされているもののみ収集</li>
          <li>人気のないコンテンツや不適切なコンテンツを避けるために、2つ未満の評価やNSFWマークのある投稿は除外</li>
        </ul>
      </li>
      <li>ステップ3：キャプションのクリーニング
        <ul>
          <li>Redditの投稿タイトルは他の大規模なソース（例：altテキスト）に比べてノイズが少ないため、テキストのクリーニングは最小限</li>
          <li>キャプションを小文字に変換し、文字のアクセント、絵文字、非ラテン文字を削除</li>
          <li>括弧で囲まれた部分を簡単なパターンマッチングで削除</li>
          <li>ソーシャルメディアのハンドル（’@’で始まる単語）を[USR]トークンに置き換え、ユーザーのプライバシーを保護し重複を減らす</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>プライバシー保護
    <ul>
      <li>RetinaFaceで信頼度0.9以上の顔が検出されたものを削除</li>
    </ul>
  </li>
  <li>NSFW画像
    <ul>
      <li>InceptionV3でポルノまたはヘンタイとして検出されたものを削除</li>
    </ul>
  </li>
  <li>Potentially derogatory language
    <ul>
      <li>軽蔑的な言葉を含むものを削除</li>
    </ul>
  </li>
  <li>Consent
    <ul>
      <li>ユーザーはデータセットになることを同意していないため、Redditの投稿が消えたらデータセットから削除され、削除申請できるフォームも用意</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>image-textデータセットとの比較
    <ul>
      <li><img src="/assets/images/posts/RedCaps/2.png" alt="" /></li>
    </ul>
  </li>
  <li>TOP20 Subreddit
    <ul>
      <li><img src="/assets/images/posts/RedCaps/3.png" alt="" /></li>
    </ul>
  </li>
  <li>Captionの長さ
    <ul>
      <li><img src="/assets/images/posts/RedCaps/4.png" alt="" /></li>
    </ul>
  </li>
  <li>言語的多様性の比較
    <ul>
      <li><img src="/assets/images/posts/RedCaps/5.png" alt="" /></li>
    </ul>
  </li>
  <li>言語統計
    <ul>
      <li>
        <p><img src="/assets/images/posts/RedCaps/6.png" alt="" /></p>
      </li>
      <li>VirTex-v2を大規模データセットでPretrainしたときの、7つのデータセットのゼロショット画像分類の比較
        <ul>
          <li>6つのデータセットで他の大規模データセットを上回る</li>
        </ul>
      </li>
      <li><img src="/assets/images/posts/RedCaps/7.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Caption" /><category term="Dataset" /><category term="Caption" /><summary type="html"><![CDATA[概要 ビジョンと言語のタスクのための大規模データセットは、検索エンジンをクエリにしたりHTMLのaltテキストを収集することで構築されているが、ウェブデータはノイズが多いため、品質を維持するために複雑なフィルタリングパイプラインが必要 最小限のフィルタリングで高品質なデータを収集するための代替データソースを探索 Redditから収集された1200万の画像とキャプションのペアのRedCapsという大規模なデータセットを紹介]]></summary></entry><entry><title type="html">OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models</title><link href="http://localhost:4000/vision%20and%20language/2023/05/15/OFASys.html" rel="alternate" type="text/html" title="OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models" /><published>2023-05-15T12:00:00+09:00</published><updated>2023-05-15T12:00:00+09:00</updated><id>http://localhost:4000/vision%20and%20language/2023/05/15/OFASys</id><content type="html" xml:base="http://localhost:4000/vision%20and%20language/2023/05/15/OFASys.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>マルチモーダルの汎用モデル学習システムOFASysを提案
    <ul>
      <li>7つ(TEXT、IMAGE、AUDIO、VIDEO、STRUCT、MOTION)のモダリティの23のタスク</li>
    </ul>
  </li>
  <li>複数モダリティのタスクを1行のコードで宣言することで、学習・推論用のタスクプランを自動生成する</li>
  <li>テキスト、画像、音声、動画、モーションデータを扱うことができる世界初の単一モデルOFA+も開発し、15個のタスクに調整されたモデルのわずか16％のパラメータで平均95％の性能を達成
<!--more-->
<img src="/assets/images/posts/OFASys/1.png" alt="" /></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>マルチモーダル用の汎用モデル学習システムOFASysを提案</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>マルチモーダル命令
    <ul>
      <li>タスクが何をするのか、データのモダリティの種類を指定する記述行
        <ul>
          <li><img src="/assets/images/posts/OFASys/2.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ユーザーインターフェイス
    <ul>
      <li>多様なデータやタスクに対応する命令形式（正規表現）
        <ul>
          <li><img src="/assets/images/posts/OFASys/3.png" alt="" /></li>
          <li><img src="/assets/images/posts/OFASys/4.png" alt="" /></li>
        </ul>
      </li>
      <li>例：入力-&gt;出力（Image Caption）
        <ul>
          <li><img src="/assets/images/posts/OFASys/5.png" alt="" /></li>
        </ul>
      </li>
      <li>例：可変長スロット(Object Detection)
        <ul>
          <li><img src="/assets/images/posts/OFASys/6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>システムデザイン
    <ul>
      <li>fairseqやtransformersなどのフレームワークは開発コストを削減したが、マルチモーダルやマルチタスクではデータ処理の実装や特徴抽出器などを手動で設定しなければならない
        <ul>
          <li>マルチモーダルやマルチタスクを単一のフレームワークで行なうOFASysとマルチタスク実行を管理するタスクスケジューラを開発</li>
        </ul>
      </li>
      <li>マルチデータプロセッシング
        <ul>
          <li>データの種類ごとに機械学習データに変換
            <ul>
              <li>テキストならトークン、オーディオならfbank特徴</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>計算機
        <ul>
          <li>T5やDiffusionに向けたU-Net、GPTなど様々なモデルに対応</li>
          <li>現在はtransformer Enc-DecとMixture-of-Experts（MoE）</li>
        </ul>
      </li>
      <li><img src="/assets/images/posts/OFASys/7.png" alt="" /></li>
      <li><img src="/assets/images/posts/OFASys/8.png" alt="" /></li>
    </ul>
  </li>
  <li>応用例：OFA+
    <ul>
      <li>OFA-Sysを用いて、テキスト、画像、音声、動画、モーションデータをオールインワンで扱えるGeneralistモデルを学習した</li>
      <li>OFA+ (Generalist)
        <ul>
          <li>OFA-baseの事前学習済み重みから学習</li>
          <li>90/270Mがモダリティ固有のパラメータ</li>
        </ul>
      </li>
      <li>OFA+ (Generalist MoE)
        <ul>
          <li>OFA-baseに基づくがVLMOの実装に近い</li>
          <li>275/455Mがモダリティ固有のパラメータ</li>
        </ul>
      </li>
      <li>どちらも7つのモダリティの17のタスクで学習し、タスク固有のFinetuningは行わない</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li><img src="/assets/images/posts/OFASys/9.png" alt="" /></li>
  <li><img src="/assets/images/posts/OFASys/10.png" alt="" /></li>
  <li>
    <p><img src="/assets/images/posts/OFASys/11.png" alt="" /></p>
  </li>
  <li>OFA+ (Specialist), OFA+ (Generalist), and OFA+ (Generalist MoE)
    <ul>
      <li>Generalist MoE &gt; Generalist</li>
      <li><img src="/assets/images/posts/OFASys/12.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Vision and Language" /><category term="Multimodal Pretraining" /><category term="Unified Frameworks" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 マルチモーダルの汎用モデル学習システムOFASysを提案 7つ(TEXT、IMAGE、AUDIO、VIDEO、STRUCT、MOTION)のモダリティの23のタスク 複数モダリティのタスクを1行のコードで宣言することで、学習・推論用のタスクプランを自動生成する テキスト、画像、音声、動画、モーションデータを扱うことができる世界初の単一モデルOFA+も開発し、15個のタスクに調整されたモデルのわずか16％のパラメータで平均95％の性能を達成]]></summary></entry><entry><title type="html">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision</title><link href="http://localhost:4000/vision%20and%20language/2023/05/10/SimVLM.html" rel="alternate" type="text/html" title="SimVLM: Simple Visual Language Model Pretraining with Weak Supervision" /><published>2023-05-10T12:00:00+09:00</published><updated>2023-05-10T12:00:00+09:00</updated><id>http://localhost:4000/vision%20and%20language/2023/05/10/SimVLM</id><content type="html" xml:base="http://localhost:4000/vision%20and%20language/2023/05/10/SimVLM.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>最小限のVision-Language PretrainingフレームワークであるSimple Visual Language Model (SimVLM)を提案</li>
  <li>Prefix Language Modelingによって余分なデータやタスク固有のカスタマイズが必要ない</li>
  <li>従来の事前学習方法を大幅に上回り、VQA、NLVR2、SNLI-VEなどの幅広いVLタスクでSOTA
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>効率的なVLの事前学習方法を提案</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>背景
    <ul>
      <li>Masked Language Modeling (MLM)
        <ul>
          <li>BERTのように文章の一部にマスクをし、そのトークンを復元する訓練をする事前学習
            <ul>
              <li><img src="/assets/images/posts/SimVLM/MLM.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Language Modeling (LM)
        <ul>
          <li>テキストの確率を最大化するように事前学習
            <ul>
              <li><img src="/assets/images/posts/SimVLM/LM.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Prefix Language Modeling (PrefixLM)
    <ul>
      <li>LM Lossに倣って、下図のように、画像と文章前半を受け取り続きを予測することで、ゼロショットで画像と言語の関係を学習する
        <ul>
          <li><img src="/assets/images/posts/SimVLM/PrefixLM.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>アーキテクチャ
    <ul>
      <li><img src="/assets/images/posts/SimVLM/SimVLM.png" alt="" /></li>
      <li>言語と画像で成功しているtransformerをバックボーンとする
        <ul>
          <li>PrefixLMはDecoderのみでもEnc-Decにも適用できるが、Enc-Decの機能バイアスが下流タスクに寄与することを確認</li>
        </ul>
      </li>
      <li>画像：ResNetの最初の3つのブロックに通して、パッチ化してFlattenしてtransformerへ
        <ul>
          <li>ViTで使用される単純な線形射影より有利</li>
        </ul>
      </li>
      <li>言語：サブワードトークンにトークン化</li>
      <li>画像と言語に学習可能な1D位置埋め込み</li>
      <li>transformerレイヤー内の画像パッチに2D相対位置埋め込み</li>
    </ul>
  </li>
  <li>学習
    <ul>
      <li>ALIGINとC4データセットで1Mステップの事前学習
        <ul>
          <li>512個のTPU v3
            <ul>
              <li>4096 Img-Text(ALIGIN), 512 Text(C4)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Finetuning後、6つのVLベンチマークで評価</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>popular VL banchmarks
    <ul>
      <li><img src="/assets/images/posts/SimVLM/VL.png" alt="" /></li>
    </ul>
  </li>
  <li>SNLI-VE and Multi30k (Zero-shot)
    <ul>
      <li>+1.37% accuracy</li>
      <li><img src="/assets/images/posts/SimVLM/6.png" alt="" /></li>
    </ul>
  </li>
  <li>VQA
    <ul>
      <li>+3.74%</li>
      <li><img src="/assets/images/posts/SimVLM/VQA.png" alt="" /></li>
    </ul>
  </li>
  <li>ImageNet
    <ul>
      <li><img src="/assets/images/posts/SimVLM/ImageNet.png" alt="" /></li>
    </ul>
  </li>
  <li>VQA(Ablation study)
    <ul>
      <li><img src="/assets/images/posts/SimVLM/9.png" alt="" /></li>
    </ul>
  </li>
  <li>生成例
    <ul>
      <li>(a) zero-shot image captioning (b) zero-shot cross-modality transfer on German image captioning (c) generative VQA (d) zero-shot visual text completion (e) zero-shot open-ended VQA.
        <ul>
          <li><img src="/assets/images/posts/SimVLM/10.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Vision and Language" /><category term="Multimodal Pretraining" /><category term="Vision-Language Pretraining" /><category term="Zero-shot Learning" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 最小限のVision-Language PretrainingフレームワークであるSimple Visual Language Model (SimVLM)を提案 Prefix Language Modelingによって余分なデータやタスク固有のカスタマイズが必要ない 従来の事前学習方法を大幅に上回り、VQA、NLVR2、SNLI-VEなどの幅広いVLタスクでSOTA]]></summary></entry><entry><title type="html">ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection</title><link href="http://localhost:4000/hoi/2023/05/09/ViPLO.html" rel="alternate" type="text/html" title="ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection" /><published>2023-05-09T13:00:00+09:00</published><updated>2023-05-09T13:00:00+09:00</updated><id>http://localhost:4000/hoi/2023/05/09/ViPLO</id><content type="html" xml:base="http://localhost:4000/hoi/2023/05/09/ViPLO.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>MOAモジュールと姿勢条件付きグラフの2段階のHOI検出器ViPROを提案</li>
  <li>MOAモジュールにより量子化問題に対処し、ViTを特徴抽出器として利用</li>
  <li>人間のプロセスに触発された姿勢条件付きグラフにより、人間の姿勢から豊富な情報を利用</li>
  <li>1段階法と比べて、低複雑性と実世界シナリオへの適用性の利点がある
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>HICO-DETとV-COCOでSOTA</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>ViT Backboneを使用した特徴抽出
    <ul>
      <li><img src="/assets/images/posts/ViPLO/backbone.png" alt="" /></li>
      <li>Faster R-CNNを使用して、画像から人間とオブジェクトを検出</li>
      <li>特徴抽出にResNetではなくViTを使用</li>
      <li>ViTに対応するためにMOAモジュールを導入
        <ul>
          <li>ViT Backboneの出力特徴から、人間とオブジェクトの特徴を抽出する</li>
          <li>パッチと領域の重なりを利用して、Attention MAPを作成</li>
          <li>MOAモジュールによりHOI検出で大きなパフォーマンスの向上
            <ul>
              <li><img src="/assets/images/posts/ViPLO/moa.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>姿勢条件付きグラフニューラルネットワーク
    <ul>
      <li><img src="/assets/images/posts/ViPLO/gnn.png" alt="" /></li>
      <li>人間とオブジェクト間のインタラクションを検出するために使用</li>
      <li>空間条件付きグラフ（SCG）ベース
        <ul>
          <li>ResNetとROIAlignを使用して抽出した特徴でノードを初期化</li>
          <li>エッジエンコーディングを人間とオブジェクトの2つの境界ボックスの空間情報に基づく特徴で初期化</li>
          <li>エッジエンコーディングに条件付けられたノード間で双方向メッセージパッシングを実行</li>
          <li>更新されたノードエンコーディングとエッジエンコーディングはHOIの分類に使用</li>
        </ul>
      </li>
      <li>姿勢認識エッジエンコーディング
        <ul>
          <li>空間情報だけでなく人間の姿勢情報に基づいて初期化する</li>
          <li>CGと同様にペアワイズ空間特徴（つまり、クエリ）を計算</li>
          <li>各関節の座標と関節から、ペアワイズ関節特徴（つまり、キー）を計算</li>
          <li>クエリとキーの内積によって各人間の関節の注意スコアを計算</li>
        </ul>
      </li>
      <li>メッセージパッシングと姿勢
        <ul>
          <li>各人間の関節のローカル領域ボックスのViT出力をUnFlattenしてROIAlignを適用してローカル特徴を抽出</li>
          <li>抽出特徴を人間のノードエンコーディングを更新するために使用</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>HICO-DET &amp; V-COCO
    <ul>
      <li><img src="/assets/images/posts/ViPLO/HICO.png" alt="" /></li>
      <li>HICOで2.07 mAP向上</li>
    </ul>
  </li>
  <li>SCGとの比較
    <ul>
      <li><img src="/assets/images/posts/ViPLO/SCG.png" alt="" /></li>
      <li>left: ViPLO, right: SCG</li>
    </ul>
  </li>
  <li>CLIP重みの有効性
    <ul>
      <li><img src="/assets/images/posts/ViPLO/CLIP.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="Human-Object Interaction" /><category term="CLIP" /><category term="ViT" /><summary type="html"><![CDATA[概要 MOAモジュールと姿勢条件付きグラフの2段階のHOI検出器ViPROを提案 MOAモジュールにより量子化問題に対処し、ViTを特徴抽出器として利用 人間のプロセスに触発された姿勢条件付きグラフにより、人間の姿勢から豊富な情報を利用 1段階法と比べて、低複雑性と実世界シナリオへの適用性の利点がある]]></summary></entry><entry><title type="html">Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks</title><link href="http://localhost:4000/vision%20and%20language/2023/05/08/Unified-IO.html" rel="alternate" type="text/html" title="Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks" /><published>2023-05-08T12:00:00+09:00</published><updated>2023-05-08T12:00:00+09:00</updated><id>http://localhost:4000/vision%20and%20language/2023/05/08/Unified-IO</id><content type="html" xml:base="http://localhost:4000/vision%20and%20language/2023/05/08/Unified-IO.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>統一された入力と出力を使用して、姿勢推定、物体検出、深度推定、画像生成などのCVタスク、領域キャプションや参照表現などのVLタスク、質問応答やテキスト要約などのNLタスクを実行する統合モデルUNIFIED-IOを提案</li>
  <li>UNIFIED-IOは、単一のtransformerベースのアーキテクチャを使用して、CVとNLの90を超える多様なデータセットを共同でトレーニングできる</li>
  <li>GRITベンチマークで7つのタスクすべてを実行できる最初のモデルであり、NYUv2-Depth、ImageNet、VQA2.0、OK-VQA、Swig、VizWizGround、BoolQ、およびSciTailなどの16の多様なベンチマークでタスク固有のFinetuningなしで優れた結果
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>モダリティ固有のブランチを必要とせずに、GRITベンチマークで7つのタスクをサポートする最初のモデル</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/Unified-IO/Unified-IO-2.png" alt="" /></p>
<ul>
  <li>アーキテクチャ
    <ul>
      <li>Text-to-Text Transfer Transformer (T5)に従って、基本は純粋なTransformer Encoder-Decoder構造</li>
      <li>画像はViTに倣ってパッチトークンにし、2次元絶対位置埋め込みを追加</li>
      <li>入力は言語256画像576トークン(384x384画像から24x24パッチ)で、出力は言語128画像256トークン(256x256画像から16x16パッチ)</li>
      <li>パラメータ
        <ul>
          <li><img src="/assets/images/posts/Unified-IO/Unified-IO-3.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>学習
    <ul>
      <li>2つの事前学習
        <ul>
          <li>言語のノイズ除去(BERTと同様)
            <ul>
              <li>半分がテキストデータ(C4とWikipedia)</li>
              <li>残りがImagenet21kなどの画像とクラスデータや、YFCC15Mなどの画像とキャプションデータ</li>
            </ul>
          </li>
          <li>画像のマスクノイズ除去(MAEと同様)
            <ul>
              <li>CVデータの一部の画像を使用</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>学習とデータセット
        <ul>
          <li>62箇所から95の公開データセットを使用
            <ul>
              <li><img src="/assets/images/posts/Unified-IO/Unified-IO-4.png" alt="" /></li>
              <li><img src="/assets/images/posts/Unified-IO/Unified-IO-5.png" alt="" /></li>
            </ul>
          </li>
          <li>バッチにデータセットを混ぜて訓練
            <ul>
              <li>データの多い画像合成は3/16、少ない密度ラベリングは1/16、それ以外はグループで均等</li>
              <li>グループ内ではデータセットサイズの平方根に比例してサンプリング</li>
              <li>一部のタスクはめったにサンプリングされない（深度推定は0.43％）</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>語彙
    <ul>
      <li>言語が32152，場所が1000，画像が16384の計49536</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>GRIT ベンチマーク
    <ul>
      <li>画像分類、物体検出、VQA、参照表現、セグメンテーション、キーポイント、および表面法線推定など7つのタスクで構成</li>
      <li>既存で最も多いGPV-2が4タスクしかできないのに対し、UNIFIED-IOは7つのタスクすべてをサポート</li>
      <li>UNIFIED-IO XLは画像分類、VQA、参照表現、セグメンテーションにおいてSOTA</li>
      <li>キーポイントはMask R-CNNより劣っている（推論が二段階のため）
        <ul>
          <li><img src="/assets/images/posts/Unified-IO/Unified-IO-6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Detection
    <ul>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-7.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-8.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-9.png" alt="" /></li>
    </ul>
  </li>
  <li>Reration
    <ul>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-10.png" alt="" /></li>
    </ul>
  </li>
  <li>Other
    <ul>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-11.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-12.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-13.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Vision and Language" /><category term="Multimodal Pretraining" /><category term="Multitask Learning" /><category term="Unified Frameworks" /><category term="Zero-shot Learning" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 統一された入力と出力を使用して、姿勢推定、物体検出、深度推定、画像生成などのCVタスク、領域キャプションや参照表現などのVLタスク、質問応答やテキスト要約などのNLタスクを実行する統合モデルUNIFIED-IOを提案 UNIFIED-IOは、単一のtransformerベースのアーキテクチャを使用して、CVとNLの90を超える多様なデータセットを共同でトレーニングできる GRITベンチマークで7つのタスクすべてを実行できる最初のモデルであり、NYUv2-Depth、ImageNet、VQA2.0、OK-VQA、Swig、VizWizGround、BoolQ、およびSciTailなどの16の多様なベンチマークでタスク固有のFinetuningなしで優れた結果]]></summary></entry><entry><title type="html">Neural Machine Translation of Rare Words with Subword Units</title><link href="http://localhost:4000/language/2023/05/04/BPE.html" rel="alternate" type="text/html" title="Neural Machine Translation of Rare Words with Subword Units" /><published>2023-05-04T12:00:00+09:00</published><updated>2023-05-04T12:00:00+09:00</updated><id>http://localhost:4000/language/2023/05/04/BPE</id><content type="html" xml:base="http://localhost:4000/language/2023/05/04/BPE.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>実際の翻訳はopen-vocabularyであるのに対し、ニューラル機械翻訳(NMT)は固定の語彙で動作し、語彙にない単語は辞書で対処してきた（翻訳は1対1とは限らないので不適切）</li>
  <li>そこでBPEを単語分割のタスクに対応させ、希少や未知の単語をサブワード単位で符号化することで、open-vocabularyに対応した</li>
  <li>これにより、WMT15の翻訳課題において英→独で最大1.1BLEU、英→露で1.3BLEU向上
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>open-vocabularyでニューラル機械翻訳(NMT)</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>アルゴリズム
    <ul>
      <li><img src="/assets/images/posts/BPE/1.png" alt="" /></li>
      <li>BPE
        <ul>
          <li>シーケンス内で最も頻繁に使用されるバイトのペアを、単一の未使用バイトに繰り返し置き換えていく</li>
          <li>繰り返す回数はハイパラ</li>
        </ul>
      </li>
      <li>例
        <ul>
          <li>「l o w &lt;/w&gt;(5個), l o w e s t &lt;/w&gt;(2個), n e w e r &lt;/w&gt;(6個), w i d e r &lt;/w&gt;(3個)」という辞書がある場合（4回繰り返す）
            <ul>
              <li>最も頻繁に出てくるeとrの組み合わせを結合(er)</li>
              <li>最も頻繁に出てくるerと&lt;/w&gt;の組み合わせを結合(er&lt;/w&gt;)</li>
              <li>最も頻繁に出てくるlとoの組み合わせを結合(lo)</li>
              <li>最も頻繁に出てくるloとwの組み合わせを結合(low)</li>
            </ul>
          </li>
          <li>「low &lt;/w&gt;, low e s t &lt;/w&gt;, n e w er&lt;/w&gt;, w i d er&lt;/w&gt;」という辞書になる</li>
          <li>12(l o w e s t n r w i d &lt;/w&gt;)から10(low e s t n w er&lt;/w&gt; i d &lt;/w&gt;)に減った</li>
        </ul>
      </li>
      <li>ソースとターゲットで独立してEncodingを学習する方法と
  ソースとターゲットの結合でEncodingを学習する方法(joint BPE)を用意
        <ul>
          <li>独立する場合はニューラルモデルがサブワード単位間の翻訳を学習するのが難しくなる</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>newest2015(英→独)
    <ul>
      <li><img src="/assets/images/posts/BPE/2.png" alt="" /></li>
      <li>rareは上位5万語に含まれない単語</li>
    </ul>
  </li>
  <li>newest2015(英→露)
    <ul>
      <li><img src="/assets/images/posts/BPE/3.png" alt="" /></li>
    </ul>
  </li>
  <li>単語の分割例(英→独)
    <ul>
      <li><img src="/assets/images/posts/BPE/4.png" alt="" /></li>
    </ul>
  </li>
  <li>単語の分割例(英→露)
    <ul>
      <li><img src="/assets/images/posts/BPE/5.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Language" /><category term="Language" /><summary type="html"><![CDATA[概要 実際の翻訳はopen-vocabularyであるのに対し、ニューラル機械翻訳(NMT)は固定の語彙で動作し、語彙にない単語は辞書で対処してきた（翻訳は1対1とは限らないので不適切） そこでBPEを単語分割のタスクに対応させ、希少や未知の単語をサブワード単位で符号化することで、open-vocabularyに対応した これにより、WMT15の翻訳課題において英→独で最大1.1BLEU、英→露で1.3BLEU向上]]></summary></entry><entry><title type="html">Visual Programming: Compositional visual reasoning without training</title><link href="http://localhost:4000/vision%20and%20language/2023/05/01/VisProg.html" rel="alternate" type="text/html" title="Visual Programming: Compositional visual reasoning without training" /><published>2023-05-01T12:00:00+09:00</published><updated>2023-05-01T12:00:00+09:00</updated><id>http://localhost:4000/vision%20and%20language/2023/05/01/VisProg</id><content type="html" xml:base="http://localhost:4000/vision%20and%20language/2023/05/01/VisProg.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>1枚または複数枚の画像と自然言語の命令を与え、GPT-3を利用して命令プログラムを作成し、そのプログラムを実行することで目的の出力を得るシステムVISPROGを提案</li>
  <li>命令プログラムの各行では、CVモデル・言語モデル・OpenCVの画像処理・演算子のいずれかのモジュールを実行し、後続で使用できる中間出力を生成している</li>
  <li>事実知識オブジェクトタグ付け・言語ガイド付き画像編集などの4つのタスクで柔軟性を実証
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>Neural Module Networksに比べて、GPT-3によって訓練を必要とせずに少数の例からプログラムを作成できる</li>
  <li>中間出力を確認することで、間違いの理由や視覚的根拠を得ることができる</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>GPT-3は入力と出力のデモを与えることで、入力から欲しい出力を得ることができる
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-2.png" alt="" /></li>
      <li>これを利用し、命令とプログラムのデモを与えることで、目的のプログラムを得る（画像編集タスクの例）
        <ul>
          <li><img src="/assets/images/posts/VisProg/VisProg-3.png" alt="" /></li>
        </ul>
      </li>
      <li>GPT-3が各モジュールの入出力や機能を理解できるように、説明的なモジュール名(Select, Replaceなど)、引数名(image, objectなど)、変数名(IMAGE, OBJ)を用いている
        <ul>
          <li>各モジュールはPythonのクラスとして実装されている
            <ul>
              <li><img src="/assets/images/posts/VisProg/VisProg-4.png" alt="" />
                <ol>
                  <li>行を解析して入力引数名と値、出力変数名を抽出</li>
                  <li>学習済みNNを含む計算を実行し、出力変数名と値でプログラムの状態を更新</li>
                  <li>htmlを用いて計算を視覚的に要約</li>
                </ol>
              </li>
            </ul>
          </li>
          <li>現在サポートされているモジュール（赤はNNモデル, 青は画像処理などのPythonルーチン）
            <ul>
              <li><img src="/assets/images/posts/VisProg/VisProg-5.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>視覚的要約
        <ul>
          <li>例（画像編集タスク）
            <ul>
              <li><img src="/assets/images/posts/VisProg/VisProg-6.png" alt="" /></li>
            </ul>
          </li>
          <li>例（NLVRタスク）
            <ul>
              <li><img src="/assets/images/posts/VisProg/VisProg-7.png" alt="" /></li>
            </ul>
          </li>
          <li>この視覚的要約によって、プログラムの論理的な正しさや失敗の原因が理解できる</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>タスクとプロンプト
    <ul>
      <li>合成VQAタスク
        <ul>
          <li><img src="/assets/images/posts/VisProg/VisProg-8.png" alt="" /></li>
          <li>例：「ヘルメットをかぶっている人の左側に小さなトラックがあるか、右側にあるのか？」</li>
        </ul>
      </li>
      <li>NLVR
        <ul>
          <li><img src="/assets/images/posts/VisProg/VisProg-9.png" alt="" /></li>
          <li>画像ペアに対するVQA</li>
        </ul>
      </li>
      <li>知識タグ付けタスク
        <ul>
          <li><img src="/assets/images/posts/VisProg/VisProg-10.png" alt="" /></li>
          <li>画像に写っている人物や物体の名前を識別</li>
        </ul>
      </li>
      <li>画像編集タスク
        <ul>
          <li><img src="/assets/images/posts/VisProg/VisProg-11.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>合成VQAタスク
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-12.png" alt="" /></li>
      <li>2.7ポイントUP</li>
    </ul>
  </li>
  <li>NLVR
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-13.png" alt="" /></li>
      <li>62.4%の高いゼロショット精度</li>
    </ul>
  </li>
  <li>知識タグ付けタスク
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-14.png" alt="" /></li>
      <li><img src="/assets/images/posts/VisProg/VisProg-15.png" alt="" /></li>
    </ul>
  </li>
  <li>画像編集タスク
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-16.png" alt="" /></li>
      <li><img src="/assets/images/posts/VisProg/VisProg-17.png" alt="" /></li>
    </ul>
  </li>
  <li>視覚的要約による失敗原因の解明とプロンプトの修正
    <ul>
      <li><img src="/assets/images/posts/VisProg/VisProg-18.png" alt="" /></li>
    </ul>
  </li>
</ul>

<h1 id="関連論文">関連論文</h1>

<ul>
  <li>PROGPROMPT: Generating Situated Robot Task Plans using Large Language Models</li>
</ul>]]></content><author><name></name></author><category term="Vision and Language" /><category term="Transformer" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 1枚または複数枚の画像と自然言語の命令を与え、GPT-3を利用して命令プログラムを作成し、そのプログラムを実行することで目的の出力を得るシステムVISPROGを提案 命令プログラムの各行では、CVモデル・言語モデル・OpenCVの画像処理・演算子のいずれかのモジュールを実行し、後続で使用できる中間出力を生成している 事実知識オブジェクトタグ付け・言語ガイド付き画像編集などの4つのタスクで柔軟性を実証]]></summary></entry><entry><title type="html">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</title><link href="http://localhost:4000/object%20detection/2023/03/31/ViLD.html" rel="alternate" type="text/html" title="Open-vocabulary Object Detection via Vision and Language Knowledge Distillation" /><published>2023-03-31T23:30:00+09:00</published><updated>2023-03-31T23:30:00+09:00</updated><id>http://localhost:4000/object%20detection/2023/03/31/ViLD</id><content type="html" xml:base="http://localhost:4000/object%20detection/2023/03/31/ViLD.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>任意のテキストで物体検出をするオープンボキャブラリ物体検出器ViLD(Vision and Language knowledge Distillation)を提案</li>
  <li>オープンボキャブラリの画像分類である教師モデルから2段階の検出器である生徒モデルに知識蒸留する</li>
  <li>ResNetやALIGNをバックボーンとして、PASCAL VOC、COCO、Objects365で高精度が出た
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>任意のテキストで物体検出</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/ViLD/2.png" alt="" /></p>

<ul>
  <li>変数
    <ul>
      <li>教師データセットを基本サブセット$C_B$と新規サブセット$C_N$に分ける
        <ul>
          <li>$C_B$のみ学習に使う</li>
        </ul>
      </li>
      <li>テキストEncoder  $T()$, 画像Encoder $V()$</li>
    </ul>
  </li>
  <li>位置の検出
    <ul>
      <li>Mask R-CNNのような二段階物体検出器をベースとする</li>
    </ul>
  </li>
  <li>Open Vocab検出
    <ul>
      <li>検出した位置を切り抜き</li>
      <li>画像埋め込み
        <ul>
          <li>基本サブセット$C_B$を用いて位置提案ネットワークを学習し、提案領域を得る</li>
          <li>提案領域の切り抜きとリサイズをして，事前学習済み画像Encoder$V()$で画像埋め込みを得る</li>
        </ul>
      </li>
      <li>テキスト埋め込み
        <ul>
          <li>カテゴリー名をプロンプトテンプレート（例：「a photo of {category} in the scene」）にして，テキストEncoder  $T()$でテキスト埋め込みを得る</li>
        </ul>
      </li>
      <li>画像とテキストの埋め込み間のコサイン類似度を計算
        <ul>
          <li>すべての提案領域に対し，画像Encoder$V()$に通すので推論は低速</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Vision&amp;Languageの知識蒸留
    <ul>
      <li>推論の遅さを解決するため</li>
      <li>ViLD-text
        <ul>
          <li><img src="/assets/images/posts/ViLD/3.png" alt="" /></li>
          <li>検出器の分類を前述のテキスト埋め込みに置き換え</li>
          <li>背景というテキストは表現できないので独自の埋め込みを学習</li>
          <li>すべてのカテゴリとのCos類似度を出しクロスエントロピーを計算
            <ul>
              <li><img src="/assets/images/posts/ViLD/4.png" alt="" /></li>
            </ul>
          </li>
          <li>推論時には新規カテゴリに対して，テキスト埋め込みを追加することで新規カテゴリに対応</li>
        </ul>
      </li>
      <li>ViLD-image
        <ul>
          <li><img src="/assets/images/posts/ViLD/5.png" alt="" /></li>
          <li>Mask R-CNNで切り取った特徴が、前述の画像埋め込みと一致するように学習することで知識蒸留（L1距離の最小化）
            <ul>
              <li><img src="/assets/images/posts/ViLD/6.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>組み合わせた最終形態
        <ul>
          <li><img src="/assets/images/posts/ViLD/7.png" alt="" /></li>
          <li><img src="/assets/images/posts/ViLD/8.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li><img src="/assets/images/posts/ViLD/9.png" alt="" /></li>
  <li><img src="/assets/images/posts/ViLD/10.png" alt="" /></li>
  <li>PASCAL VOCで72.2 AP</li>
  <li>COCOで36.6 AP</li>
  <li>Objects365で11.8 AP</li>
</ul>]]></content><author><name></name></author><category term="Object Detection" /><category term="Object Detection" /><category term="Zero-shot Learning" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 任意のテキストで物体検出をするオープンボキャブラリ物体検出器ViLD(Vision and Language knowledge Distillation)を提案 オープンボキャブラリの画像分類である教師モデルから2段階の検出器である生徒モデルに知識蒸留する ResNetやALIGNをバックボーンとして、PASCAL VOC、COCO、Objects365で高精度が出た]]></summary></entry><entry><title type="html">Unified Visual Relationship Detection with Vision and Language Models</title><link href="http://localhost:4000/hoi/2023/03/28/UniVRD.html" rel="alternate" type="text/html" title="Unified Visual Relationship Detection with Vision and Language Models" /><published>2023-03-28T23:30:00+09:00</published><updated>2023-03-28T23:30:00+09:00</updated><id>http://localhost:4000/hoi/2023/03/28/UniVRD</id><content type="html" xml:base="http://localhost:4000/hoi/2023/03/28/UniVRD.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>Visual Relationship Detection(VRD)では、１つのデータセットから学習するため、画像ドメインと語彙に制約があり、汎用性と拡張性に限界がある</li>
  <li>Vision&amp;Languageモデルを活用し、複数のデータセットを統一するフレームワークUniVRDを提案</li>
  <li>HICO-DETにおいて60%アップの38.07mAP
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>Vision&amp;Languageモデルによる複数データセットの統一</li>
  <li>モデルや損失、訓練方法の改善</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/UniVRD/UniVRD-1.png" alt="" /></p>

<ul>
  <li>物体検出器
    <ul>
      <li>CLIPなどと同様にテキストEncoderとしてtransformer，画像EncoderとしてViTを使用</li>
      <li>画像Encoderは、プーリング層と最終層を削除して、トークン埋め込み層を追加する
        <ul>
          <li>トークン埋め込みを線形層に通すことで分類埋め込みを，FFNに通すことでBBを得る</li>
        </ul>
      </li>
      <li>Encoderのみで動作するため、知識抽出や事前学習を必要としない</li>
    </ul>
  </li>
  <li>Relationship Decoder
    <ul>
      <li>入力をEncoderのトークン埋め込みと関係クエリ（学習）として，関係埋め込みを得る
        <ul>
          <li>関係埋め込みを線形層に通すことで関係分類埋め込みを，FFNに通すことで主語と目的後のインデックスを得る</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>分類用テキスト埋め込み
    <ul>
      <li>物体／関係の分類では整数値ではなく，テキスト埋め込みを使用
        <ul>
          <li>物体の分類：「person」を「a photo of person」にしてテキストEncoderへ</li>
          <li>関係の分類：「person(主語), ride(述語), horse(目的)」を「a person riding a orse」にしてテキストEncoderへ</li>
        </ul>
      </li>
      <li>検出器はテキスト埋め込みに対してクラス確率を予測する</li>
    </ul>
  </li>
  <li>データ拡張
    <ul>
      <li>モザイク
        <ul>
          <li>画像をグリッドにすることで、モデルが見るスケールの幅を広げる</li>
        </ul>
      </li>
      <li>テキストプロンプティング
        <ul>
          <li>CLIPのように、プロンプトテンプレートを使用
            <ul>
              <li>物体：「a photo of &lt;object&gt;」</li>
              <li>関係：「a &lt;subject&gt; &lt;predicate&gt;ing a &lt;object&gt;」</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>学習
    <ul>
      <li>物体検出とDecoderを順番に学習したほうが安定的かつ効果的</li>
      <li>Decoderの学習時に学習データが限られている場合は物体検出器を凍結したほうがよい</li>
      <li>Loss関数
        <ul>
          <li><img src="/assets/images/posts/UniVRD/UniVRD-2.png" alt="" /></li>
          <li><img src="/assets/images/posts/UniVRD/UniVRD-3.png" alt="" /></li>
          <li><img src="/assets/images/posts/UniVRD/UniVRD-4.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>HOI(Himan-Object Interaction)
    <ul>
      <li>HICO-DETでSOTA
        <ul>
          <li><img src="/assets/images/posts/UniVRD/UniVRD-5.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>SGG(Scene Graph Generation)
    <ul>
      <li>Visual Genome
        <ul>
          <li><img src="/assets/images/posts/UniVRD/UniVRD-6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="HOI" /><category term="Transformer" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 Visual Relationship Detection(VRD)では、１つのデータセットから学習するため、画像ドメインと語彙に制約があり、汎用性と拡張性に限界がある Vision&amp;Languageモデルを活用し、複数のデータセットを統一するフレームワークUniVRDを提案 HICO-DETにおいて60%アップの38.07mAP]]></summary></entry><entry><title type="html">GO-Finder: A Registration-Free Wearable System for Assisting Users in Finding Lost Objects via Hand-Held Object Discovery</title><link href="http://localhost:4000/object%20detection/2023/03/14/GO-Finder.html" rel="alternate" type="text/html" title="GO-Finder: A Registration-Free Wearable System for Assisting Users in Finding Lost Objects via Hand-Held Object Discovery" /><published>2023-03-14T23:30:00+09:00</published><updated>2023-03-14T23:30:00+09:00</updated><id>http://localhost:4000/object%20detection/2023/03/14/GO-Finder</id><content type="html" xml:base="http://localhost:4000/object%20detection/2023/03/14/GO-Finder.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>登録不要のウェアラブルカメラを用いた物体の発見支援システムGO-Finderを提案</li>
  <li>手持ちの物体を自動的に検出しグループ化しておくことで、アプリから対象物の最後の出現を取得できる
    <ul>
      <li>手で扱う物体に限定することで、対象となる物体を大幅に削減</li>
    </ul>
  </li>
  <li>物体画像をクエリとして利用し、候補の中から物体を選択することができる
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>従来のシステムではユーザーが事前に対象物を登録する必要があった</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>概要
    <ul>
      <li>必要なもの
        <ul>
          <li>ウェアラブルカメラ、処理サーバー、参照用スマートフォン</li>
          <li><img src="/assets/images/posts/GO-Finder/GoFinder-2.png" alt="" /></li>
        </ul>
      </li>
      <li>観察フェーズ
        <ul>
          <li>ウェアラブルカメラで連続的に画像を撮って、サーバーに送信</li>
          <li>サーバーは手持ちの物体を検出・追跡</li>
          <li>物体の外観によってクラスタリング</li>
        </ul>
      </li>
      <li>検索フェーズ
        <ul>
          <li><img src="/assets/images/posts/GO-Finder/GoFinder-3.png" alt="" /></li>
          <li>スマートフォンのインターフェースを使って、処理結果を受信</li>
          <li>検索したい物体を選択</li>
          <li>物体が最後に出現したシーンがポップアップ画面で出てくる</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>アルゴリズム
    <ul>
      <li><img src="/assets/images/posts/GO-Finder/GoFinder-4.png" alt="" /></li>
      <li>手持ち物体の検出
        <ul>
          <li>既存手法「Understanding Human Hands in Contact at Internet Scale」を用いて、バウンディングボックスと携帯型物体判定と接触状態を取得</li>
          <li>携帯型物体判定かつ接触状態の物体のバウンディングボックスを取得</li>
          <li>フレームの半分の辺長はノイズとして除外</li>
        </ul>
      </li>
      <li>検出されたバウンディングボックスを、外観特徴に基づいてクラスタリング
        <ul>
          <li>ステージ１：トラッキングしたものを同じクラスタへ(c)</li>
          <li>ステージ２：トラッキングが失敗した場合、ImageNet学習済みResNetに通した特徴に対してコサイン類似度を計算し、閾値を超えたらクラスタへ(d上)
            <ul>
              <li>条件を満たすクラスタが1つもない場合は新しいクラスタを作成</li>
            </ul>
          </li>
          <li>ステージ３：クラスタ単位でコサイン類似度行列を作成、最大値と中央値が閾値を超えたらクラスタへ(d下)</li>
        </ul>
      </li>
      <li>手の外観や類似の質感による誤った関連付けを防ぐために、ヒューリスティックを導入
        <ul>
          <li>バウンディングボックスのアスペクト比が1.5より大きい場合</li>
          <li>肌色領域(カラーヒストグラム)の比率が0.3より大きい場合</li>
          <li>物体と手の面積比が1.5より大きい場合</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>12名のボランティアに指定した場所に物体を隠してもらい、15分のインターバルで位置を忘れさせ、システムを利用しながら隠した物体の一部を持ち帰るように指示
    <ul>
      <li><img src="/assets/images/posts/GO-Finder/GoFinder-5.png" alt="" /></li>
      <li>パフォーマンス
        <ul>
          <li><img src="/assets/images/posts/GO-Finder/GoFinder-6.png" alt="" /></li>
        </ul>
      </li>
      <li>タスク完了時間
        <ul>
          <li><img src="/assets/images/posts/GO-Finder/GoFinder-7.png" alt="" /></li>
        </ul>
      </li>
      <li>クラスタの様子
        <ul>
          <li><img src="/assets/images/posts/GO-Finder/GoFinder-8.png" alt="" /></li>
          <li>いくつかのクラスタは分割され過ぎている</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Object Detection" /><category term="Object Detection" /><summary type="html"><![CDATA[概要 登録不要のウェアラブルカメラを用いた物体の発見支援システムGO-Finderを提案 手持ちの物体を自動的に検出しグループ化しておくことで、アプリから対象物の最後の出現を取得できる 手で扱う物体に限定することで、対象となる物体を大幅に削減 物体画像をクエリとして利用し、候補の中から物体を選択することができる]]></summary></entry></feed>