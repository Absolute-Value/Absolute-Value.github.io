<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-28T22:05:26+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">軸屋敬介 | Keisuke Jikuya</title><entry><title type="html">Music Transformer: Generating music with long-term structure</title><link href="http://localhost:4000/transformer/2023/10/23/MusicTransformer.html" rel="alternate" type="text/html" title="Music Transformer: Generating music with long-term structure" /><published>2023-10-23T20:14:00+09:00</published><updated>2023-10-23T20:14:00+09:00</updated><id>http://localhost:4000/transformer/2023/10/23/MusicTransformer</id><content type="html" xml:base="http://localhost:4000/transformer/2023/10/23/MusicTransformer.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>音楽の長期構造を生成するためのMusic Transformerを提案</li>
  <li>Music Transformerは既存のTransformerモデルの相対位置情報の表現を改善し、音楽の相対的なタイミングとピッチを捉えることができる</li>
  <li>データセット「JSB Chorales」と「Piano-e-Competition」で評価され、後者で最先端の結果を達成
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>Transformerを用いて長期構造を持つ音楽を生成する初の成功例で、LSTMを超えた</li>
  <li>提案アルゴリズムは、相対的自己注意メカニズムの空間複雑さを大幅に削減し、より長い音楽構造の生成を可能にしている</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>Relative positional self-attention
    <ul>
      <li>Shawらによって提案されたもの
        <ul>
          <li><img src="/assets/images/posts/MusicTransformer/1.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Skewing
    <ul>
      <li>LxDの相対埋め込みE（学習パラメータ）がある</li>
      <li>ShawらはEからLxLxDの埋め込みに拡張しQをかけることでSを作成 O($L^2D$)</li>
      <li>SkewingではEに直接Qをかけて、図下のような変形をすることで効率化 O($LD$)
        <ul>
          <li><img src="/assets/images/posts/MusicTransformer/2.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Relative local attention
    <ul>
      <li>非常に長い文の場合にすべてのAttentionをとるのは非現実的</li>
      <li>Mブロックにわけた長さNに対してAttentionをとる</li>
      <li>その場合のSkewingは、図のような変形
        <ul>
          <li><img src="/assets/images/posts/MusicTransformer/3.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>ShawらのRelative Attentionとのメモリ使用量の比較
    <ul>
      <li><img src="/assets/images/posts/MusicTransformer/4.png" alt="" /></li>
    </ul>
  </li>
  <li>J.S.Bach Chorales
    <ul>
      <li><img src="/assets/images/posts/MusicTransformer/5.png" alt="" /></li>
    </ul>
  </li>
  <li>Piano-e-Competition dataset
    <ul>
      <li><img src="/assets/images/posts/MusicTransformer/6.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Transformer" /><category term="Attention" /><category term="Transformer" /><summary type="html"><![CDATA[概要 音楽の長期構造を生成するためのMusic Transformerを提案 Music Transformerは既存のTransformerモデルの相対位置情報の表現を改善し、音楽の相対的なタイミングとピッチを捉えることができる データセット「JSB Chorales」と「Piano-e-Competition」で評価され、後者で最先端の結果を達成]]></summary></entry><entry><title type="html">Places: An Image Database for Deep Scene Understanding</title><link href="http://localhost:4000/dataset/2023/10/23/Places.html" rel="alternate" type="text/html" title="Places: An Image Database for Deep Scene Understanding" /><published>2023-10-23T18:45:00+09:00</published><updated>2023-10-23T18:45:00+09:00</updated><id>http://localhost:4000/dataset/2023/10/23/Places</id><content type="html" xml:base="http://localhost:4000/dataset/2023/10/23/Places.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>シーンの意味論的なカテゴリと属性がラベル付けされた 1,000 万枚のシーン写真のリポジトリである Places Databaseについて説明</li>
  <li>最先端のCNNを使用して分類時に優れたベースラインパフォーマンス
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>環境のカテゴリの広範なカバレッジを提供することにより、シーン理解の能力を向上させることを目指している</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>オンライン画像検索エンジン(Google画像、Bing画像、およびFlickr) から、SUN データセットのクラスのリストからのクエリワードを使用して候補画像をダウンロード
    <ul>
      <li>外観の多様性のために696の一般的な形容詞 (例: 乱雑、晴れ、荒涼など)と組み合わせた</li>
      <li>SUNデータセットと同じ画像が含まれないようにしている</li>
    </ul>
  </li>
  <li>Amazon Mechanical Turk (AMT) にクラウドソーシングしてラベルが正しいかを検証
    <ul>
      <li><img src="/assets/images/posts/Places/1.png" alt="" /></li>
      <li>最初の反復では50％以上、2回目の反復では25.4％がNo</li>
      <li>最終的に少なくとも1,000個のサンプルが含まれるカテゴリが413、20,000個を超えるサンプルが含まれるカテゴリが98</li>
    </ul>
  </li>
  <li>AlexNetで残りの 5,300 万枚の画像を分類
    <ul>
      <li>データセットは1,000個のサンプルが含まれる413カテゴリ</li>
      <li>サンプルが少ないカテゴリは信頼度0.8以上のものをAMTへ</li>
      <li>5,000個のサンプルが含まれるカテゴリが401、20,000個を超えるサンプルが含まれるカテゴリが240</li>
    </ul>
  </li>
  <li>「スキー ロッジ」と「スキー リゾート」のような類似カテゴリを統合</li>
  <li>分類が曖昧な画像をどちらに属すかをAMTで収集することで分類
    <ul>
      <li><img src="/assets/images/posts/Places/2.png" alt="" /></li>
      <li><img src="/assets/images/posts/Places/3.png" alt="" /></li>
      <li>434の場所カテゴリ,10,624,928 枚の画像のデータベースが完成</li>
    </ul>
  </li>
  <li>4,000枚を超える画像を含む365のカテゴリからPlaces365-StandardとPlaces365-Challengeを作成
    <ul>
      <li>Places365-Standardは1,803,460個の学習画像</li>
      <li>Places365-Challengeは800万個の学習画像</li>
      <li>検証はクラスごとに50個、テストはクラスごとに900個</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>CNN分類精度(Places205 &amp; SUN205)
    <ul>
      <li><img src="/assets/images/posts/Places/4.png" alt="" /></li>
    </ul>
  </li>
  <li>CNN分類精度(Places365)
    <ul>
      <li><img src="/assets/images/posts/Places/5.png" alt="" /></li>
    </ul>
  </li>
  <li>VGG(Places365)
    <ul>
      <li><img src="/assets/images/posts/Places/6.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Dataset" /><category term="Dataset" /><summary type="html"><![CDATA[概要 シーンの意味論的なカテゴリと属性がラベル付けされた 1,000 万枚のシーン写真のリポジトリである Places Databaseについて説明 最先端のCNNを使用して分類時に優れたベースラインパフォーマンス]]></summary></entry><entry><title type="html">Sun database: Large-scale scene recognition from abbey to zoo</title><link href="http://localhost:4000/dataset/2023/10/22/sun397.html" rel="alternate" type="text/html" title="Sun database: Large-scale scene recognition from abbey to zoo" /><published>2023-10-22T18:45:00+09:00</published><updated>2023-10-22T18:45:00+09:00</updated><id>http://localhost:4000/dataset/2023/10/22/sun397</id><content type="html" xml:base="http://localhost:4000/dataset/2023/10/22/sun397.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>899のカテゴリと130,519の画像を含む広範なScene UNderstanding (SUN) データベースを提案</li>
  <li>さまざまな最先端のアルゴリズムを使用してシーン認識の新しいパフォーマンスの境界を設定
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>SUNデータベースは現存する最大のシーンカテゴリデータセットで、これまでの研究よりも多様な環境をカバー</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>小さな画像データセットで利用可能なWordNetの70,000の用語から、シーン、場所、および環境を記述する2500の空間とシーンの初期用語を選択
    <ul>
      <li>具体的な地名（ニューヨークなど）や特定のアイデンティティ（職場など）を想起させる用語は含めない</li>
      <li>ナビゲート不可能なシーン（デスクトップなど）や乗り物を含めない</li>
      <li>WordNetに欠けているいくつかのカテゴリを追加</li>
    </ul>
  </li>
  <li>同義語を重ねて、屋内と屋内などを分離して、899のカテゴリーと130,519の画像でSUNデータベースを構築</li>
  <li>各シーン・カテゴリーについて、ウェブ上の様々な検索エンジンでは200×200ピクセル以上のカラー画像を取得
    <ul>
      <li>類似のシーン・カテゴリー（例：修道院と教会）については、定義の重複を避けるために明示的なルールを設定</li>
      <li>退化した画像や異常な画像（白黒、歪んだ色、非常にぼやけている、ノイズが多い、回転が正しくない、航空写真、境界線が目立つ）は削除</li>
    </ul>
  </li>
  <li>899カテゴリのうち、100枚のユニークな写真がある397カテゴリで実験
    <ul>
      <li>人間のシーン分類
        <ul>
          <li>Amazon’s Mechanical Turk (AMT)を使って、各SUNカテゴリについて20の異なるテストシーンで合計397×20=7940の実験を行った</li>
          <li>語彙の混乱を避けるために米国の参加者限定</li>
        </ul>
      </li>
      <li>複数の特徴量（GIST、Dense SIFT、HOG2x2、自己類似性記述子、Tiny Images、色ヒストグラム、直線ヒストグラムなど）でSVMを学習</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>人間のシーン分類の精度
    <ul>
      <li>397のカテゴリで58.6%</li>
      <li>一部の作業者は0%の精度</li>
      <li>最初の階層で95%以上の精度を持つ「良い作業者」に焦点を当てると、精度は68.5%に上昇</li>
      <li>精度が高いカテゴリ
        <ul>
          <li><img src="/assets/images/posts/sun397/1.png" alt="" /></li>
        </ul>
      </li>
      <li>精度が低いカテゴリ
        <ul>
          <li><img src="/assets/images/posts/sun397/2.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>SVMの精度
    <ul>
      <li><img src="/assets/images/posts/sun397/3.png" alt="" /></li>
      <li>全特徴を用いて38%で人間を大きく下回る</li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Dataset" /><category term="Dataset" /><category term="SVM" /><summary type="html"><![CDATA[概要 899のカテゴリと130,519の画像を含む広範なScene UNderstanding (SUN) データベースを提案 さまざまな最先端のアルゴリズムを使用してシーン認識の新しいパフォーマンスの境界を設定]]></summary></entry><entry><title type="html">BEIT: BERT Pre-Training of Image Transformers</title><link href="http://localhost:4000/transformer/2023/10/16/BEIT.html" rel="alternate" type="text/html" title="BEIT: BERT Pre-Training of Image Transformers" /><published>2023-10-16T20:06:00+09:00</published><updated>2023-10-16T20:06:00+09:00</updated><id>http://localhost:4000/transformer/2023/10/16/BEIT</id><content type="html" xml:base="http://localhost:4000/transformer/2023/10/16/BEIT.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>Vision Transformer事前学習する自己教師ありタスクを提案</li>
  <li>BERTのようなマスク画像モデリングを行う</li>
  <li>画像分類とセマンティックセグメンテーションで競争力のある結果を達成し、事前トレーニング方法を改善
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>BERTスタイルの事前トレーニングを画像データに直接適用するのが難しいという課題を解決</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/BEIT/0.png" alt="" /></li>
  <li>事前学習
    <ul>
      <li>入力画像をdiscrete VAEでvisual tokensにする</li>
      <li>同時に入力画像をパッチ分割し、ランダムにマスクしてtransformerへ</li>
      <li>transformerはマスクされたパッチに対応するvisual tokensを予測するように学習</li>
    </ul>
  </li>
  <li>この事前学習をしたモデルをダウンストリームタスクに応用</li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>画像分類（ILSVRC-2012 ImageNet）
    <ul>
      <li><img src="/assets/images/posts/BEIT/1.png" alt="" /></li>
    </ul>
  </li>
  <li>セマンティックセグメンテーション
    <ul>
      <li><img src="/assets/images/posts/BEIT/2.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Transformer" /><category term="Vision Pretraining" /><category term="Transformer" /><summary type="html"><![CDATA[概要 Vision Transformer事前学習する自己教師ありタスクを提案 BERTのようなマスク画像モデリングを行う 画像分類とセマンティックセグメンテーションで競争力のある結果を達成し、事前トレーニング方法を改善]]></summary></entry><entry><title type="html">RedCaps: Web-curated image-text data created by the people, for the people</title><link href="http://localhost:4000/caption/2023/07/03/RedCaps.html" rel="alternate" type="text/html" title="RedCaps: Web-curated image-text data created by the people, for the people" /><published>2023-07-03T10:50:00+09:00</published><updated>2023-07-03T10:50:00+09:00</updated><id>http://localhost:4000/caption/2023/07/03/RedCaps</id><content type="html" xml:base="http://localhost:4000/caption/2023/07/03/RedCaps.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>ビジョンと言語のタスクのための大規模データセットは、検索エンジンをクエリにしたりHTMLのaltテキストを収集することで構築されているが、ウェブデータはノイズが多いため、品質を維持するために複雑なフィルタリングパイプラインが必要</li>
  <li>最小限のフィルタリングで高品質なデータを収集するための代替データソースを探索</li>
  <li>Redditから収集された1200万の画像とキャプションのペアのRedCapsという大規模なデータセットを紹介
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>Redditから収集された画像とキャプションのペアで構成</li>
  <li>独自のデータ収集パイプラインやフィルタリング手法を使用して高品質なデータを提供</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>パイプライン
    <ul>
      <li>ステップ1：Subredditの選択
        <ul>
          <li>手動で選ばれた一連のSubredditからデータを収集</li>
          <li>Subredditの選択により、個々のインスタンスに注釈を付けることなく、データセットの構成を調整することができる</li>
          <li>人々の画像を共有したり、コメントしたりすることを目的とするサブレディットは除外</li>
        </ul>
      </li>
      <li>ステップ2：画像投稿のフィルタリング
        <ul>
          <li>PushshiftとRedditのAPIを使用して、選択したSubredditに投稿されたすべての画像投稿をダウンロード</li>
          <li>画像はReddit、Imgur、Flickrの3つのドメインにホストされているもののみ収集</li>
          <li>人気のないコンテンツや不適切なコンテンツを避けるために、2つ未満の評価やNSFWマークのある投稿は除外</li>
        </ul>
      </li>
      <li>ステップ3：キャプションのクリーニング
        <ul>
          <li>Redditの投稿タイトルは他の大規模なソース（例：altテキスト）に比べてノイズが少ないため、テキストのクリーニングは最小限</li>
          <li>キャプションを小文字に変換し、文字のアクセント、絵文字、非ラテン文字を削除</li>
          <li>括弧で囲まれた部分を簡単なパターンマッチングで削除</li>
          <li>ソーシャルメディアのハンドル（’@’で始まる単語）を[USR]トークンに置き換え、ユーザーのプライバシーを保護し重複を減らす</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>プライバシー保護
    <ul>
      <li>RetinaFaceで信頼度0.9以上の顔が検出されたものを削除</li>
    </ul>
  </li>
  <li>NSFW画像
    <ul>
      <li>InceptionV3でポルノまたはヘンタイとして検出されたものを削除</li>
    </ul>
  </li>
  <li>Potentially derogatory language
    <ul>
      <li>軽蔑的な言葉を含むものを削除</li>
    </ul>
  </li>
  <li>Consent
    <ul>
      <li>ユーザーはデータセットになることを同意していないため、Redditの投稿が消えたらデータセットから削除され、削除申請できるフォームも用意</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>image-textデータセットとの比較
    <ul>
      <li><img src="/assets/images/posts/RedCaps/2.png" alt="" /></li>
    </ul>
  </li>
  <li>TOP20 Subreddit
    <ul>
      <li><img src="/assets/images/posts/RedCaps/3.png" alt="" /></li>
    </ul>
  </li>
  <li>Captionの長さ
    <ul>
      <li><img src="/assets/images/posts/RedCaps/4.png" alt="" /></li>
    </ul>
  </li>
  <li>言語的多様性の比較
    <ul>
      <li><img src="/assets/images/posts/RedCaps/5.png" alt="" /></li>
    </ul>
  </li>
  <li>言語統計
    <ul>
      <li>
        <p><img src="/assets/images/posts/RedCaps/6.png" alt="" /></p>
      </li>
      <li>VirTex-v2を大規模データセットでPretrainしたときの、7つのデータセットのゼロショット画像分類の比較
        <ul>
          <li>6つのデータセットで他の大規模データセットを上回る</li>
        </ul>
      </li>
      <li><img src="/assets/images/posts/RedCaps/7.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Caption" /><category term="Dataset" /><category term="Caption" /><summary type="html"><![CDATA[概要 ビジョンと言語のタスクのための大規模データセットは、検索エンジンをクエリにしたりHTMLのaltテキストを収集することで構築されているが、ウェブデータはノイズが多いため、品質を維持するために複雑なフィルタリングパイプラインが必要 最小限のフィルタリングで高品質なデータを収集するための代替データソースを探索 Redditから収集された1200万の画像とキャプションのペアのRedCapsという大規模なデータセットを紹介]]></summary></entry><entry><title type="html">OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models</title><link href="http://localhost:4000/vision%20and%20language/2023/05/15/OFASys.html" rel="alternate" type="text/html" title="OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models" /><published>2023-05-15T12:00:00+09:00</published><updated>2023-05-15T12:00:00+09:00</updated><id>http://localhost:4000/vision%20and%20language/2023/05/15/OFASys</id><content type="html" xml:base="http://localhost:4000/vision%20and%20language/2023/05/15/OFASys.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>マルチモーダルの汎用モデル学習システムOFASysを提案
    <ul>
      <li>7つ(TEXT、IMAGE、AUDIO、VIDEO、STRUCT、MOTION)のモダリティの23のタスク</li>
    </ul>
  </li>
  <li>複数モダリティのタスクを1行のコードで宣言することで、学習・推論用のタスクプランを自動生成する</li>
  <li>テキスト、画像、音声、動画、モーションデータを扱うことができる世界初の単一モデルOFA+も開発し、15個のタスクに調整されたモデルのわずか16％のパラメータで平均95％の性能を達成
<!--more-->
<img src="/assets/images/posts/OFASys/1.png" alt="" /></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>マルチモーダル用の汎用モデル学習システムOFASysを提案</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>マルチモーダル命令
    <ul>
      <li>タスクが何をするのか、データのモダリティの種類を指定する記述行
        <ul>
          <li><img src="/assets/images/posts/OFASys/2.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ユーザーインターフェイス
    <ul>
      <li>多様なデータやタスクに対応する命令形式（正規表現）
        <ul>
          <li><img src="/assets/images/posts/OFASys/3.png" alt="" /></li>
          <li><img src="/assets/images/posts/OFASys/4.png" alt="" /></li>
        </ul>
      </li>
      <li>例：入力-&gt;出力（Image Caption）
        <ul>
          <li><img src="/assets/images/posts/OFASys/5.png" alt="" /></li>
        </ul>
      </li>
      <li>例：可変長スロット(Object Detection)
        <ul>
          <li><img src="/assets/images/posts/OFASys/6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>システムデザイン
    <ul>
      <li>fairseqやtransformersなどのフレームワークは開発コストを削減したが、マルチモーダルやマルチタスクではデータ処理の実装や特徴抽出器などを手動で設定しなければならない
        <ul>
          <li>マルチモーダルやマルチタスクを単一のフレームワークで行なうOFASysとマルチタスク実行を管理するタスクスケジューラを開発</li>
        </ul>
      </li>
      <li>マルチデータプロセッシング
        <ul>
          <li>データの種類ごとに機械学習データに変換
            <ul>
              <li>テキストならトークン、オーディオならfbank特徴</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>計算機
        <ul>
          <li>T5やDiffusionに向けたU-Net、GPTなど様々なモデルに対応</li>
          <li>現在はtransformer Enc-DecとMixture-of-Experts（MoE）</li>
        </ul>
      </li>
      <li><img src="/assets/images/posts/OFASys/7.png" alt="" /></li>
      <li><img src="/assets/images/posts/OFASys/8.png" alt="" /></li>
    </ul>
  </li>
  <li>応用例：OFA+
    <ul>
      <li>OFA-Sysを用いて、テキスト、画像、音声、動画、モーションデータをオールインワンで扱えるGeneralistモデルを学習した</li>
      <li>OFA+ (Generalist)
        <ul>
          <li>OFA-baseの事前学習済み重みから学習</li>
          <li>90/270Mがモダリティ固有のパラメータ</li>
        </ul>
      </li>
      <li>OFA+ (Generalist MoE)
        <ul>
          <li>OFA-baseに基づくがVLMOの実装に近い</li>
          <li>275/455Mがモダリティ固有のパラメータ</li>
        </ul>
      </li>
      <li>どちらも7つのモダリティの17のタスクで学習し、タスク固有のFinetuningは行わない</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li><img src="/assets/images/posts/OFASys/9.png" alt="" /></li>
  <li><img src="/assets/images/posts/OFASys/10.png" alt="" /></li>
  <li>
    <p><img src="/assets/images/posts/OFASys/11.png" alt="" /></p>
  </li>
  <li>OFA+ (Specialist), OFA+ (Generalist), and OFA+ (Generalist MoE)
    <ul>
      <li>Generalist MoE &gt; Generalist</li>
      <li><img src="/assets/images/posts/OFASys/12.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Vision and Language" /><category term="Multimodal Pretraining" /><category term="Unified Frameworks" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 マルチモーダルの汎用モデル学習システムOFASysを提案 7つ(TEXT、IMAGE、AUDIO、VIDEO、STRUCT、MOTION)のモダリティの23のタスク 複数モダリティのタスクを1行のコードで宣言することで、学習・推論用のタスクプランを自動生成する テキスト、画像、音声、動画、モーションデータを扱うことができる世界初の単一モデルOFA+も開発し、15個のタスクに調整されたモデルのわずか16％のパラメータで平均95％の性能を達成]]></summary></entry><entry><title type="html">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision</title><link href="http://localhost:4000/vision%20and%20language/2023/05/10/SimVLM.html" rel="alternate" type="text/html" title="SimVLM: Simple Visual Language Model Pretraining with Weak Supervision" /><published>2023-05-10T12:00:00+09:00</published><updated>2023-05-10T12:00:00+09:00</updated><id>http://localhost:4000/vision%20and%20language/2023/05/10/SimVLM</id><content type="html" xml:base="http://localhost:4000/vision%20and%20language/2023/05/10/SimVLM.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>最小限のVision-Language PretrainingフレームワークであるSimple Visual Language Model (SimVLM)を提案</li>
  <li>Prefix Language Modelingによって余分なデータやタスク固有のカスタマイズが必要ない</li>
  <li>従来の事前学習方法を大幅に上回り、VQA、NLVR2、SNLI-VEなどの幅広いVLタスクでSOTA
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>効率的なVLの事前学習方法を提案</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>背景
    <ul>
      <li>Masked Language Modeling (MLM)
        <ul>
          <li>BERTのように文章の一部にマスクをし、そのトークンを復元する訓練をする事前学習
            <ul>
              <li><img src="/assets/images/posts/SimVLM/MLM.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Language Modeling (LM)
        <ul>
          <li>テキストの確率を最大化するように事前学習
            <ul>
              <li><img src="/assets/images/posts/SimVLM/LM.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Prefix Language Modeling (PrefixLM)
    <ul>
      <li>LM Lossに倣って、下図のように、画像と文章前半を受け取り続きを予測することで、ゼロショットで画像と言語の関係を学習する
        <ul>
          <li><img src="/assets/images/posts/SimVLM/PrefixLM.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>アーキテクチャ
    <ul>
      <li><img src="/assets/images/posts/SimVLM/SimVLM.png" alt="" /></li>
      <li>言語と画像で成功しているtransformerをバックボーンとする
        <ul>
          <li>PrefixLMはDecoderのみでもEnc-Decにも適用できるが、Enc-Decの機能バイアスが下流タスクに寄与することを確認</li>
        </ul>
      </li>
      <li>画像：ResNetの最初の3つのブロックに通して、パッチ化してFlattenしてtransformerへ
        <ul>
          <li>ViTで使用される単純な線形射影より有利</li>
        </ul>
      </li>
      <li>言語：サブワードトークンにトークン化</li>
      <li>画像と言語に学習可能な1D位置埋め込み</li>
      <li>transformerレイヤー内の画像パッチに2D相対位置埋め込み</li>
    </ul>
  </li>
  <li>学習
    <ul>
      <li>ALIGINとC4データセットで1Mステップの事前学習
        <ul>
          <li>512個のTPU v3
            <ul>
              <li>4096 Img-Text(ALIGIN), 512 Text(C4)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Finetuning後、6つのVLベンチマークで評価</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>popular VL banchmarks
    <ul>
      <li><img src="/assets/images/posts/SimVLM/VL.png" alt="" /></li>
    </ul>
  </li>
  <li>SNLI-VE and Multi30k (Zero-shot)
    <ul>
      <li>+1.37% accuracy</li>
      <li><img src="/assets/images/posts/SimVLM/6.png" alt="" /></li>
    </ul>
  </li>
  <li>VQA
    <ul>
      <li>+3.74%</li>
      <li><img src="/assets/images/posts/SimVLM/VQA.png" alt="" /></li>
    </ul>
  </li>
  <li>ImageNet
    <ul>
      <li><img src="/assets/images/posts/SimVLM/ImageNet.png" alt="" /></li>
    </ul>
  </li>
  <li>VQA(Ablation study)
    <ul>
      <li><img src="/assets/images/posts/SimVLM/9.png" alt="" /></li>
    </ul>
  </li>
  <li>生成例
    <ul>
      <li>(a) zero-shot image captioning (b) zero-shot cross-modality transfer on German image captioning (c) generative VQA (d) zero-shot visual text completion (e) zero-shot open-ended VQA.
        <ul>
          <li><img src="/assets/images/posts/SimVLM/10.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Vision and Language" /><category term="Multimodal Pretraining" /><category term="Vision-Language Pretraining" /><category term="Zero-shot Learning" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 最小限のVision-Language PretrainingフレームワークであるSimple Visual Language Model (SimVLM)を提案 Prefix Language Modelingによって余分なデータやタスク固有のカスタマイズが必要ない 従来の事前学習方法を大幅に上回り、VQA、NLVR2、SNLI-VEなどの幅広いVLタスクでSOTA]]></summary></entry><entry><title type="html">ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection</title><link href="http://localhost:4000/hoi/2023/05/09/ViPLO.html" rel="alternate" type="text/html" title="ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection" /><published>2023-05-09T13:00:00+09:00</published><updated>2023-05-09T13:00:00+09:00</updated><id>http://localhost:4000/hoi/2023/05/09/ViPLO</id><content type="html" xml:base="http://localhost:4000/hoi/2023/05/09/ViPLO.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>MOAモジュールと姿勢条件付きグラフの2段階のHOI検出器ViPROを提案</li>
  <li>MOAモジュールにより量子化問題に対処し、ViTを特徴抽出器として利用</li>
  <li>人間のプロセスに触発された姿勢条件付きグラフにより、人間の姿勢から豊富な情報を利用</li>
  <li>1段階法と比べて、低複雑性と実世界シナリオへの適用性の利点がある
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>HICO-DETとV-COCOでSOTA</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>ViT Backboneを使用した特徴抽出
    <ul>
      <li><img src="/assets/images/posts/ViPLO/backbone.png" alt="" /></li>
      <li>Faster R-CNNを使用して、画像から人間とオブジェクトを検出</li>
      <li>特徴抽出にResNetではなくViTを使用</li>
      <li>ViTに対応するためにMOAモジュールを導入
        <ul>
          <li>ViT Backboneの出力特徴から、人間とオブジェクトの特徴を抽出する</li>
          <li>パッチと領域の重なりを利用して、Attention MAPを作成</li>
          <li>MOAモジュールによりHOI検出で大きなパフォーマンスの向上
            <ul>
              <li><img src="/assets/images/posts/ViPLO/moa.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>姿勢条件付きグラフニューラルネットワーク
    <ul>
      <li><img src="/assets/images/posts/ViPLO/gnn.png" alt="" /></li>
      <li>人間とオブジェクト間のインタラクションを検出するために使用</li>
      <li>空間条件付きグラフ（SCG）ベース
        <ul>
          <li>ResNetとROIAlignを使用して抽出した特徴でノードを初期化</li>
          <li>エッジエンコーディングを人間とオブジェクトの2つの境界ボックスの空間情報に基づく特徴で初期化</li>
          <li>エッジエンコーディングに条件付けられたノード間で双方向メッセージパッシングを実行</li>
          <li>更新されたノードエンコーディングとエッジエンコーディングはHOIの分類に使用</li>
        </ul>
      </li>
      <li>姿勢認識エッジエンコーディング
        <ul>
          <li>空間情報だけでなく人間の姿勢情報に基づいて初期化する</li>
          <li>CGと同様にペアワイズ空間特徴（つまり、クエリ）を計算</li>
          <li>各関節の座標と関節から、ペアワイズ関節特徴（つまり、キー）を計算</li>
          <li>クエリとキーの内積によって各人間の関節の注意スコアを計算</li>
        </ul>
      </li>
      <li>メッセージパッシングと姿勢
        <ul>
          <li>各人間の関節のローカル領域ボックスのViT出力をUnFlattenしてROIAlignを適用してローカル特徴を抽出</li>
          <li>抽出特徴を人間のノードエンコーディングを更新するために使用</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>HICO-DET &amp; V-COCO
    <ul>
      <li><img src="/assets/images/posts/ViPLO/HICO.png" alt="" /></li>
      <li>HICOで2.07 mAP向上</li>
    </ul>
  </li>
  <li>SCGとの比較
    <ul>
      <li><img src="/assets/images/posts/ViPLO/SCG.png" alt="" /></li>
      <li>left: ViPLO, right: SCG</li>
    </ul>
  </li>
  <li>CLIP重みの有効性
    <ul>
      <li><img src="/assets/images/posts/ViPLO/CLIP.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="Human-Object Interaction" /><category term="CLIP" /><category term="ViT" /><summary type="html"><![CDATA[概要 MOAモジュールと姿勢条件付きグラフの2段階のHOI検出器ViPROを提案 MOAモジュールにより量子化問題に対処し、ViTを特徴抽出器として利用 人間のプロセスに触発された姿勢条件付きグラフにより、人間の姿勢から豊富な情報を利用 1段階法と比べて、低複雑性と実世界シナリオへの適用性の利点がある]]></summary></entry><entry><title type="html">Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks</title><link href="http://localhost:4000/vision%20and%20language/2023/05/08/Unified-IO.html" rel="alternate" type="text/html" title="Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks" /><published>2023-05-08T12:00:00+09:00</published><updated>2023-05-08T12:00:00+09:00</updated><id>http://localhost:4000/vision%20and%20language/2023/05/08/Unified-IO</id><content type="html" xml:base="http://localhost:4000/vision%20and%20language/2023/05/08/Unified-IO.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>統一された入力と出力を使用して、姿勢推定、物体検出、深度推定、画像生成などのCVタスク、領域キャプションや参照表現などのVLタスク、質問応答やテキスト要約などのNLタスクを実行する統合モデルUNIFIED-IOを提案</li>
  <li>UNIFIED-IOは、単一のtransformerベースのアーキテクチャを使用して、CVとNLの90を超える多様なデータセットを共同でトレーニングできる</li>
  <li>GRITベンチマークで7つのタスクすべてを実行できる最初のモデルであり、NYUv2-Depth、ImageNet、VQA2.0、OK-VQA、Swig、VizWizGround、BoolQ、およびSciTailなどの16の多様なベンチマークでタスク固有のFinetuningなしで優れた結果
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>モダリティ固有のブランチを必要とせずに、GRITベンチマークで7つのタスクをサポートする最初のモデル</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/Unified-IO/Unified-IO-2.png" alt="" /></p>
<ul>
  <li>アーキテクチャ
    <ul>
      <li>Text-to-Text Transfer Transformer (T5)に従って、基本は純粋なTransformer Encoder-Decoder構造</li>
      <li>画像はViTに倣ってパッチトークンにし、2次元絶対位置埋め込みを追加</li>
      <li>入力は言語256画像576トークン(384x384画像から24x24パッチ)で、出力は言語128画像256トークン(256x256画像から16x16パッチ)</li>
      <li>パラメータ
        <ul>
          <li><img src="/assets/images/posts/Unified-IO/Unified-IO-3.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>学習
    <ul>
      <li>2つの事前学習
        <ul>
          <li>言語のノイズ除去(BERTと同様)
            <ul>
              <li>半分がテキストデータ(C4とWikipedia)</li>
              <li>残りがImagenet21kなどの画像とクラスデータや、YFCC15Mなどの画像とキャプションデータ</li>
            </ul>
          </li>
          <li>画像のマスクノイズ除去(MAEと同様)
            <ul>
              <li>CVデータの一部の画像を使用</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>学習とデータセット
        <ul>
          <li>62箇所から95の公開データセットを使用
            <ul>
              <li><img src="/assets/images/posts/Unified-IO/Unified-IO-4.png" alt="" /></li>
              <li><img src="/assets/images/posts/Unified-IO/Unified-IO-5.png" alt="" /></li>
            </ul>
          </li>
          <li>バッチにデータセットを混ぜて訓練
            <ul>
              <li>データの多い画像合成は3/16、少ない密度ラベリングは1/16、それ以外はグループで均等</li>
              <li>グループ内ではデータセットサイズの平方根に比例してサンプリング</li>
              <li>一部のタスクはめったにサンプリングされない（深度推定は0.43％）</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>語彙
    <ul>
      <li>言語が32152，場所が1000，画像が16384の計49536</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>GRIT ベンチマーク
    <ul>
      <li>画像分類、物体検出、VQA、参照表現、セグメンテーション、キーポイント、および表面法線推定など7つのタスクで構成</li>
      <li>既存で最も多いGPV-2が4タスクしかできないのに対し、UNIFIED-IOは7つのタスクすべてをサポート</li>
      <li>UNIFIED-IO XLは画像分類、VQA、参照表現、セグメンテーションにおいてSOTA</li>
      <li>キーポイントはMask R-CNNより劣っている（推論が二段階のため）
        <ul>
          <li><img src="/assets/images/posts/Unified-IO/Unified-IO-6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Detection
    <ul>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-7.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-8.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-9.png" alt="" /></li>
    </ul>
  </li>
  <li>Reration
    <ul>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-10.png" alt="" /></li>
    </ul>
  </li>
  <li>Other
    <ul>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-11.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-12.png" alt="" /></li>
      <li><img src="/assets/images/posts/Unified-IO/Unified-IO-13.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Vision and Language" /><category term="Multimodal Pretraining" /><category term="Multitask Learning" /><category term="Unified Frameworks" /><category term="Zero-shot Learning" /><category term="Vision and Language" /><summary type="html"><![CDATA[概要 統一された入力と出力を使用して、姿勢推定、物体検出、深度推定、画像生成などのCVタスク、領域キャプションや参照表現などのVLタスク、質問応答やテキスト要約などのNLタスクを実行する統合モデルUNIFIED-IOを提案 UNIFIED-IOは、単一のtransformerベースのアーキテクチャを使用して、CVとNLの90を超える多様なデータセットを共同でトレーニングできる GRITベンチマークで7つのタスクすべてを実行できる最初のモデルであり、NYUv2-Depth、ImageNet、VQA2.0、OK-VQA、Swig、VizWizGround、BoolQ、およびSciTailなどの16の多様なベンチマークでタスク固有のFinetuningなしで優れた結果]]></summary></entry><entry><title type="html">Neural Machine Translation of Rare Words with Subword Units</title><link href="http://localhost:4000/language/2023/05/04/BPE.html" rel="alternate" type="text/html" title="Neural Machine Translation of Rare Words with Subword Units" /><published>2023-05-04T12:00:00+09:00</published><updated>2023-05-04T12:00:00+09:00</updated><id>http://localhost:4000/language/2023/05/04/BPE</id><content type="html" xml:base="http://localhost:4000/language/2023/05/04/BPE.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>実際の翻訳はopen-vocabularyであるのに対し、ニューラル機械翻訳(NMT)は固定の語彙で動作し、語彙にない単語は辞書で対処してきた（翻訳は1対1とは限らないので不適切）</li>
  <li>そこでBPEを単語分割のタスクに対応させ、希少や未知の単語をサブワード単位で符号化することで、open-vocabularyに対応した</li>
  <li>これにより、WMT15の翻訳課題において英→独で最大1.1BLEU、英→露で1.3BLEU向上
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>open-vocabularyでニューラル機械翻訳(NMT)</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>アルゴリズム
    <ul>
      <li><img src="/assets/images/posts/BPE/1.png" alt="" /></li>
      <li>BPE
        <ul>
          <li>シーケンス内で最も頻繁に使用されるバイトのペアを、単一の未使用バイトに繰り返し置き換えていく</li>
          <li>繰り返す回数はハイパラ</li>
        </ul>
      </li>
      <li>例
        <ul>
          <li>「l o w &lt;/w&gt;(5個), l o w e s t &lt;/w&gt;(2個), n e w e r &lt;/w&gt;(6個), w i d e r &lt;/w&gt;(3個)」という辞書がある場合（4回繰り返す）
            <ul>
              <li>最も頻繁に出てくるeとrの組み合わせを結合(er)</li>
              <li>最も頻繁に出てくるerと&lt;/w&gt;の組み合わせを結合(er&lt;/w&gt;)</li>
              <li>最も頻繁に出てくるlとoの組み合わせを結合(lo)</li>
              <li>最も頻繁に出てくるloとwの組み合わせを結合(low)</li>
            </ul>
          </li>
          <li>「low &lt;/w&gt;, low e s t &lt;/w&gt;, n e w er&lt;/w&gt;, w i d er&lt;/w&gt;」という辞書になる</li>
          <li>12(l o w e s t n r w i d &lt;/w&gt;)から10(low e s t n w er&lt;/w&gt; i d &lt;/w&gt;)に減った</li>
        </ul>
      </li>
      <li>ソースとターゲットで独立してEncodingを学習する方法と
  ソースとターゲットの結合でEncodingを学習する方法(joint BPE)を用意
        <ul>
          <li>独立する場合はニューラルモデルがサブワード単位間の翻訳を学習するのが難しくなる</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>newest2015(英→独)
    <ul>
      <li><img src="/assets/images/posts/BPE/2.png" alt="" /></li>
      <li>rareは上位5万語に含まれない単語</li>
    </ul>
  </li>
  <li>newest2015(英→露)
    <ul>
      <li><img src="/assets/images/posts/BPE/3.png" alt="" /></li>
    </ul>
  </li>
  <li>単語の分割例(英→独)
    <ul>
      <li><img src="/assets/images/posts/BPE/4.png" alt="" /></li>
    </ul>
  </li>
  <li>単語の分割例(英→露)
    <ul>
      <li><img src="/assets/images/posts/BPE/5.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Language" /><category term="Language" /><summary type="html"><![CDATA[概要 実際の翻訳はopen-vocabularyであるのに対し、ニューラル機械翻訳(NMT)は固定の語彙で動作し、語彙にない単語は辞書で対処してきた（翻訳は1対1とは限らないので不適切） そこでBPEを単語分割のタスクに対応させ、希少や未知の単語をサブワード単位で符号化することで、open-vocabularyに対応した これにより、WMT15の翻訳課題において英→独で最大1.1BLEU、英→露で1.3BLEU向上]]></summary></entry></feed>