<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-02-27T20:27:04+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">軸屋敬介 | Keisuke Jikuya</title><entry><title type="html">FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection</title><link href="http://localhost:4000/hoi/2023/02/21/FGAHOI.html" rel="alternate" type="text/html" title="FGAHOI: Fine-Grained Anchors for Human-Object Interaction Detection" /><published>2023-02-21T13:00:00+09:00</published><updated>2023-02-21T13:00:00+09:00</updated><id>http://localhost:4000/hoi/2023/02/21/FGAHOI</id><content type="html" xml:base="http://localhost:4000/hoi/2023/02/21/FGAHOI.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>MSS, HSAM, TAMという3つから成るEnd-to-endのtransformerベースの手法(FGAHOI)を提案
    <ul>
      <li>MSSは人間、物体、インタラクション領域の特徴を抽出</li>
      <li>HSAMとTAMは抽出された特徴量とクエリ埋め込みを
  階層的な空間視点とタスク視点で順番に意味的に整列・結合</li>
      <li>複雑な学習を軽減するために、新しい学習戦略Stage-wise Training Strategyを設計</li>
    </ul>
  </li>
  <li>新規のデータセットHOI-SDCを提案</li>
  <li>既存手法から大幅に精度向上
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>以下の問題を低減
    <ul>
      <li>複雑な背景情報を持つ画像からいかにして重要な特徴を抽出するか</li>
      <li>抽出した特徴量とクエリ埋め込みをどのように意味的に整合させるか</li>
    </ul>
  </li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>全体構造
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-1.png" alt="" /></li>
    </ul>
  </li>
  <li>Multi-Scale Features Extractor
    <ul>
      <li>
\[M = F_{encoder}(F_{flatten}(\phi(x)),p,s,r,l) \in \mathbb{R}^{N_s \times C_d}\]
      </li>
      <li>事前学習済みBackbone(Swin Transformer)でマルチスケール特徴を抽出</li>
      <li>transformer encoderで符号化して意味特徴量を得る</li>
    </ul>
  </li>
  <li>Decoder
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-2.png" alt="" /></li>
      <li>Multi-Scale Sampling (MSS)
        <ul>
          <li>
\[x^i_s = F_{sample} (reshape(M)^i, A, size^i, bilinear)\]
          </li>
          <li>transformer encoderの特徴を元の形状に整形</li>
          <li>小さなインスタンスを検出する浅い特徴量では小さくサンプリング</li>
          <li>大きなインスタンスを検出する深い特徴量では大きくサンプリング
            <ul>
              <li>サイズに関係なくインスタンスを検出するため</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Hierarchical Spatial-Aware Merging (HSAM)
        <ul>
          <li>サンプリングした各層の特徴をKeyとValue，位置PとコンテキストCをQueryとしてMHA
            <ul>
              <li>
\[C_u = C + F_{MHA}\big((C+P)W^q, (C+P)W^k, CW^v \big)\]
              </li>
              <li>
\[x^u_m = F_{concat} (\mathrm{head}_1, ... , \mathrm{head}_h) W^O\]
                <ul>
                  <li>
\[\mathrm{where} \space \mathrm{head}_n = \mathrm{Softmax\big(\frac{(C_u W^q_n)\cdot(x^i_s W^k_n)^T}{\sqrt{d_k}}\big)} (x^i_s W^v_n)\]
                  </li>
                </ul>
              </li>
              <li>特徴内Attention</li>
            </ul>
          </li>
          <li>MHAに通した各層の特徴をConcat
            <ul>
              <li>
\[X_m = F_{concat} ({x^i_m}_{i=0,1,2}) \in \mathbb{R}^{B \times N_q \times N_L \times N_{hd}}\]
              </li>
            </ul>
          </li>
          <li>Concatした特徴をKeyとValue，位置PとコンテキストCをQueryとしてMHA
            <ul>
              <li>
\[X_u = F_{concat} (\mathrm{head}_1, ... , \mathrm{head}_h) W^O\]
                <ul>
                  <li>
\[\mathrm{where} \space \mathrm{head}_n = \mathrm{Softmax\big(\frac{(C_u W^q_n)\cdot(X_m W^k_n)^T}{\sqrt{d_k}}\big)} (X W^v_n)\]
                  </li>
                </ul>
              </li>
              <li>特徴間Attention</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Task-Aware Mergin (TAM)
        <ul>
          <li>HSAMの出力特徴とコンテンツ埋め込みを融合しCross Attention
            <ul>
              <li>
\[X = F_{stack} (C_u, X_u) \in \mathbb{R}^{B \times N_q \times (2 \times N_{hd})}\]
              </li>
              <li>
\[X_{switch} = F_{stack} (\mathrm{head}_1, ... , \mathrm{head}_h) W^O\]
                <ul>
                  <li>
\[\mathrm{where} \space \mathrm{head}_n = \mathrm{Softmax\big(\frac{(C_u W^q_n)\cdot(X W^k_n)^T}{\sqrt{d_k}}\big)} (X W^v_n)\]
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>適切なチャンネルを選択する動的なスイッチを生成
            <ul>
              <li>
\[Switch^{\gamma} = F_{normalize}(D_{mlp}(X_{switch}))^{\gamma} \in \mathbb{R}^{B \times N_q \times 2 \times 2}\]
              </li>
            </ul>
          </li>
          <li>HSAMの出力特徴に対し，スイッチで一部を選択
            <ul>
              <li>
\[U^{\gamma} = F_{Max} \{Switch^{\gamma}_{i,0} \odot X^{\gamma}_u + Switch^{\gamma}_{i,1}\}_{i=0,1} + C^{\gamma}_u\]
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Decoding with Fine-Grained Anchor
        <ul>
          <li>内容埋め込みを線形層、リシェイプ、SoftMaxに通すことで
  Cross AttentionのQueryとなるAnchorとKeyとなるAttention weightを生成</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>HOI Detection Head
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-13.png" alt="" /></li>
      <li>HOI埋め込みと初期アンカーを利用して，人間と物体のBB・物体のクラス・インタラクションを予測
        <ul>
          <li><img src="/assets/images/posts/FGAHOI/FGAHOI-14.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>HICO-DET
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-15.png" alt="" /></li>
    </ul>
  </li>
  <li>V-COCO
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-16.png" alt="" /></li>
    </ul>
  </li>
  <li>HOI-SDC
    <ul>
      <li><img src="/assets/images/posts/FGAHOI/FGAHOI-17.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="Human-Object Interaction" /><category term="FGAHOI" /><category term="Fine-Grained Anchors" /><category term="Noisy Background" /><category term="Semantically Aligning" /><summary type="html"><![CDATA[概要 MSS, HSAM, TAMという3つから成るEnd-to-endのtransformerベースの手法(FGAHOI)を提案 MSSは人間、物体、インタラクション領域の特徴を抽出 HSAMとTAMは抽出された特徴量とクエリ埋め込みを 階層的な空間視点とタスク視点で順番に意味的に整列・結合 複雑な学習を軽減するために、新しい学習戦略Stage-wise Training Strategyを設計 新規のデータセットHOI-SDCを提案 既存手法から大幅に精度向上]]></summary></entry><entry><title type="html">Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection</title><link href="http://localhost:4000/hoi/2023/02/21/STIP.html" rel="alternate" type="text/html" title="Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection" /><published>2023-02-21T11:00:00+09:00</published><updated>2023-02-21T11:00:00+09:00</updated><id>http://localhost:4000/hoi/2023/02/21/STIP</id><content type="html" xml:base="http://localhost:4000/hoi/2023/02/21/STIP.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>新しいtransformerベースのHOI手法のStructure-aware Transformer over Interaction Proposals (STIP)を提案</li>
  <li>「インタラクションのある人間と物体のペア提案」と「構造考慮型transformerで提案をHOIに変換」の2つのフェーズでHOIを予測</li>
  <li>構造考慮型transformerはバニラtransformerに対し、全体的意味構造および各相互作用提案内のヒト／モノの局所的空間構造を追加的に符号化することでHOI予測を強化している
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li><img src="/assets/images/posts/STIP/1.png" alt="" /></li>
  <li>他のHOIに依存していることを考慮させた（例：「人間が（野球の）グローブをつけている」ゆえに、「（別の）人間がバットを持っている」）</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/STIP/2.png" alt="" /></li>
  <li>DETR
    <ul>
      <li>人間と物体のインスタンスを検出</li>
    </ul>
  </li>
  <li>Interaction Proposal Network (IPN)
    <ul>
      <li>すべての人間と物体のペアを構築</li>
      <li>すべてのペアに対してインタラクションの確率を外観特徴と空間特徴と言語特徴をMLPに入れて予測
        <ul>
          <li>外観特徴：DETRで得られる特徴</li>
          <li>空間特徴：$[dx, dy, dis, A_h, A_o, I, U]$
            <ul>
              <li>$dx$ : 人間と物体のx距離</li>
              <li>$dy$ : 人間と物体のy距離</li>
              <li>$dis$ : 人間と物体のユークリッド</li>
              <li>$A_h$ : 人間の面積</li>
              <li>$A_o$ : 物体の面積</li>
              <li>$I$ : 人間と物体の面積の積集合</li>
              <li>$U$ : 人間と物体の面積の和集合</li>
            </ul>
          </li>
          <li>言語特徴：物体のラベル(OneHot)を200次元のベクトルに</li>
        </ul>
      </li>
      <li>インタラクションの確率が高い上位K組を提案として出力
        <ul>
          <li>
\[L_{proposal} = \frac{1}{\sum^N_{i=1}z_i} \sum^N_{i=1} FL(\hat{z}_i, z_i)\]
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>インタラクション中心グラフ構築
    <ul>
      <li>インタラクション意味構造
        <ul>
          <li><img src="/assets/images/posts/STIP/4.png" alt="" /></li>
          <li>インタラクション提案をグラフノードとしてグラフを構築</li>
          <li>6つのクラス
            <ul>
              <li>disjunctive
                <ul>
                  <li>人間も物体もインスタンスを共有していない</li>
                </ul>
              </li>
              <li>same-human</li>
              <li>same-object
                <ul>
                  <li>人間／物体のインスタンスのみ同じ</li>
                </ul>
              </li>
              <li>series-opposing</li>
              <li>series
                <ul>
                  <li>人間／物体のインスタンスが物体／人間のインスタンスと同じ</li>
                </ul>
              </li>
              <li>same-pair
                <ul>
                  <li>人間と物体の両方のインスタンスが同じ</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>インタラクション空間構造
        <ul>
          <li><img src="/assets/images/posts/STIP/5.png" alt="" /></li>
          <li>局所的な空間特徴を考慮させる</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Structure-aware transformer
    <ul>
      <li>Structure-aware Self-Attention
        <ul>
          <li>
\[e^{self}_{ij} = \frac{(W_q q_i)^T (W_k q_j + \psi (q_j, E_{dep}(d_{ij})))}{\sqrt{d_{key}}}\]
          </li>
          <li>IPNのK組の提案に対してSelf-Attention
            <ul>
              <li>Keyに対してインタラクション意味構造の6つのクラスで意味依存性を付与する</li>
            </ul>
          </li>
          <li>$E_{dep}$は意味依存を符号化する2層MLP</li>
        </ul>
      </li>
      <li>Structure-aware Cross-attention
        <ul>
          <li><img src="/assets/images/posts/STIP/7.png" alt="" /></li>
          <li>
\[e^{cross}_{ij} = \frac{(W_{\hat{q}} \hat{q}_i)^T (W_{\hat{k}} x_j + pos_j+ \phi (x_j, E_{lay}(l_{ij})))}{\sqrt{d_{key}}}\]
          </li>
          <li>K組の中間HOI特徴をQuery，画像特徴マップをKeyとValueとしCross-attention
            <ul>
              <li>Keyに対してインタラクション空間構造の5つのクラスを付与</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>最終出力
    <ul>
      <li>2層MLPでインタラクションクラスを予測
        <ul>
          <li>Focal Loss likeな損失関数
            <ul>
              <li>
\[L_{cls} = \frac{1}{\sum^N_{i=1} \sum^C_{c=1}} \sum^N_{i=1} \sum^C_{c=1} FL (\hat{y}_{ic}, y_{ic})\]
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>全体の損失関数
        <ul>
          <li>
\[L_{STIP} = L_{proposal} + L_{cls}\]
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>V-COCO
    <ul>
      <li><img src="/assets/images/posts/STIP/11.png" alt="" /></li>
    </ul>
  </li>
  <li>HICO-DET
    <ul>
      <li><img src="/assets/images/posts/STIP/12.png" alt="" /></li>
    </ul>
  </li>
  <li>提案のK組による精度比較
    <ul>
      <li><img src="/assets/images/posts/STIP/13.png" alt="" /></li>
    </ul>
  </li>
  <li>構造考慮型transformerのレイヤー数によおる精度比較
    <ul>
      <li><img src="/assets/images/posts/STIP/14.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="Human-Object Interaction Detection" /><category term="Transformer" /><summary type="html"><![CDATA[概要 新しいtransformerベースのHOI手法のStructure-aware Transformer over Interaction Proposals (STIP)を提案 「インタラクションのある人間と物体のペア提案」と「構造考慮型transformerで提案をHOIに変換」の2つのフェーズでHOIを予測 構造考慮型transformerはバニラtransformerに対し、全体的意味構造および各相互作用提案内のヒト／モノの局所的空間構造を追加的に符号化することでHOI予測を強化している]]></summary></entry><entry><title type="html">CLRNet: Cross Layer Refinement Network for Lane Detection</title><link href="http://localhost:4000/lane%20detection/2022/12/15/CLRNet.html" rel="alternate" type="text/html" title="CLRNet: Cross Layer Refinement Network for Lane Detection" /><published>2022-12-15T12:00:00+09:00</published><updated>2022-12-15T12:00:00+09:00</updated><id>http://localhost:4000/lane%20detection/2022/12/15/CLRNet</id><content type="html" xml:base="http://localhost:4000/lane%20detection/2022/12/15/CLRNet.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>特徴抽出したFPN構造の特徴マップを、上位から下位まで複合的に活用する車線検出手法であるCross Layer Refinement Network (CLRNet)を提案</li>
  <li>CULaneとTuSimpleとLLAMASのデータセットで従来手法を上回る
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>車線の部分領域と全体を組み合わせて大域的な特徴表現を獲得するROIGatherを使用</li>
  <li>車線の検出結果に対するIoUとしてLIoU(Line IoU)を定義し、LIoUの最大化を損失関数に含んだ</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/CLRNet/img1.png" alt="" /></li>
  <li>二次元の点列$𝑃$（レーン事前分布）をネットワークで出力
    <ul>
      <li>前景と背景の確率、線の長さ、角度、予測と正解の水平距離</li>
    </ul>
  </li>
  <li>ResNetをBackboneとし、FPN構造から特徴マップ$𝐿_0, 𝐿_1, 𝐿_2$を生成</li>
  <li>点列$P$と特徴マップ$L$を組み合わせる
    <ul>
      <li>
\[P_t = P_{t-1} \circ R_t (L_{t-1},P_{t-1})\]
      </li>
      <li>上位から下位の特徴を活用するため</li>
    </ul>
  </li>
  <li>点列Pをもとに特徴マップ$L$をROIAlignで抽出</li>
  <li>Attentionをとる
    <ul>
      <li>
\[\mathcal{G} = \mathcal{W}\mathcal{X}^T_f\]
      </li>
      <li>
\[\mathcal{W} = f(\frac{\mathcal{X}^T_p \mathcal{X}_f}{\sqrt{C}})\]
      </li>
    </ul>
  </li>
  <li>Line IoU Loss
    <ul>
      <li>点を別々の変数として扱うため、既存の距離損失を用いると精度が低くなってしまう</li>
      <li>そこで、車線専用のLossを提案
        <ul>
          <li>
\[IoU = \frac{d^\omicron_i}{d^u_i} = \frac{\min(x^p_i+e, x^q_i+e) - \max(x^p_i+e, x^q_i+e)}{\max(x^p_i-e, x^q_i-e) - \min(x^p_i-e, x^q_i-e)}\]
          </li>
          <li>
\[LIoU = \frac{\sum^N_{i=1} d^\omicron_i}{\sum^N_{i=1} d^u_i}\]
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>CULane
    <ul>
      <li><img src="/assets/images/posts/CLRNet/img7.png" alt="" /></li>
    </ul>
  </li>
  <li>LLAMAS
    <ul>
      <li><img src="/assets/images/posts/CLRNet/img8.png" alt="" /></li>
    </ul>
  </li>
  <li>TuSimple
    <ul>
      <li><img src="/assets/images/posts/CLRNet/img9.png" alt="" /></li>
      <li><img src="/assets/images/posts/CLRNet/img10.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Lane Detection" /><category term="Lane Detection" /><summary type="html"><![CDATA[概要 特徴抽出したFPN構造の特徴マップを、上位から下位まで複合的に活用する車線検出手法であるCross Layer Refinement Network (CLRNet)を提案 CULaneとTuSimpleとLLAMASのデータセットで従来手法を上回る]]></summary></entry><entry><title type="html">CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution</title><link href="http://localhost:4000/lane%20detection/2022/12/14/CondLaneNet.html" rel="alternate" type="text/html" title="CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution" /><published>2022-12-14T12:00:00+09:00</published><updated>2022-12-14T12:00:00+09:00</updated><id>http://localhost:4000/lane%20detection/2022/12/14/CondLaneNet</id><content type="html" xml:base="http://localhost:4000/lane%20detection/2022/12/14/CondLaneNet.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>既存の車線検出は密集線や分岐線のような複雑な場合に苦労している(下図)</li>
  <li>車線を検出し、次に各車線の形状を予測する車線検出フレームワークであるCondLaneNetを提案</li>
  <li>3つのベンチマークデータセットで最先端手法を凌駕
<!--more--></li>
  <li><img src="/assets/images/posts/CondLaneNet/img1.png" alt="" /></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>密集線や分岐線などの複雑な車線を検出する問題を克服</li>
  <li>CULaneでは78.14 F1スコアと220 FPSを達成するなど，精度と効率の両立が可能</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/CondLaneNet/img2.png" alt="" /></li>
  <li>事前学習済みResNetをBackboneとしてFPNを用いて、マルチスケール特徴を得る
    <ul>
      <li>車線は細長いため、文脈特徴の抽出のためにBackboneの最終層にTransformer Encoderを追加
        <ul>
          <li><img src="/assets/images/posts/CondLaneNet/img3.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Proposal head
    <ul>
      <li>線の始点を予測する
        <ul>
          <li>CenterNetに従うが、細長い線は中心を見つけることが難しいため始点</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Recurrent Instance Module
    <ul>
      <li><img src="/assets/images/posts/CondLaneNet/img4.png" alt="" /></li>
      <li>予測した始点の特徴量から動的カーネルパラメータを再帰的に予測</li>
      <li>密な線や複数の線が同一の始点から始まる場合(ex.分岐)に対応</li>
    </ul>
  </li>
  <li>Conditional shape head
    <ul>
      <li>RIMの動的カーネルパラメータを使って畳み込み、マルチスケール特徴から各線の形状を予測
        <ul>
          <li><img src="/assets/images/posts/CondLaneNet/img5.png" alt="" /></li>
        </ul>
      </li>
      <li>Location maps：行ごと列ごとに予測</li>
      <li>Offset maps：行ごとの水平方向の正確な位置を予測</li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>可視化
    <ul>
      <li>上からCurveLanes、CULane、TuSimple
        <ul>
          <li><img src="/assets/images/posts/CondLaneNet/img6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>CurveLanes
    <ul>
      <li><img src="/assets/images/posts/CondLaneNet/img7.png" alt="" /></li>
    </ul>
  </li>
  <li>CULane
    <ul>
      <li><img src="/assets/images/posts/CondLaneNet/img8.png" alt="" /></li>
    </ul>
  </li>
  <li>TuSimple
    <ul>
      <li><img src="/assets/images/posts/CondLaneNet/img9.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Lane Detection" /><category term="Lane Detection" /><summary type="html"><![CDATA[概要 既存の車線検出は密集線や分岐線のような複雑な場合に苦労している(下図) 車線を検出し、次に各車線の形状を予測する車線検出フレームワークであるCondLaneNetを提案 3つのベンチマークデータセットで最先端手法を凌駕]]></summary></entry><entry><title type="html">Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection</title><link href="http://localhost:4000/lane%20detection/2022/12/13/LaneATT.html" rel="alternate" type="text/html" title="Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection" /><published>2022-12-13T17:00:00+09:00</published><updated>2022-12-13T17:00:00+09:00</updated><id>http://localhost:4000/lane%20detection/2022/12/13/LaneATT</id><content type="html" xml:base="http://localhost:4000/lane%20detection/2022/12/13/LaneATT.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>YOLOv3やSSDのようなアンカーベースのモデルであるLaneATTを提案</li>
  <li>大域的情報の取得のためにAttentionも使用</li>
  <li>CULaneとTuSimpleとLLAMASのデータセットで最先端手法を凌駕
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>高速な学習・推論が可能（250FPSを達成）</li>
  <li>アンカーベースのAttentionメカニズムは他ドメインでも有用な可能性</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li><img src="/assets/images/posts/LaneATT/img1.png" alt="" /></li>
  <li>画像の境界の1点と方向θで定義される仮想の直線をアンカーと定義する</li>
  <li>ResNetなどの学習済みCNNで特徴マップを生成</li>
  <li>アンカーに該当する特徴マップを抽出する</li>
  <li>抽出した特徴マップ同士でAttentionを取る
    <ul>
      <li>局所情報のみだと車で隠れている場合に予測できないため</li>
    </ul>
  </li>
  <li>全結合に通して、クラスラベル（白線、黄線、背景など）とアンカーとの水平距離、長さを予測</li>
  <li>NMSアルゴリズムを適用</li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>TuSimple
    <ul>
      <li><img src="/assets/images/posts/LaneATT/img2.png" alt="" /></li>
      <li><img src="/assets/images/posts/LaneATT/img3.png" alt="" /></li>
    </ul>
  </li>
  <li>CULane
    <ul>
      <li><img src="/assets/images/posts/LaneATT/img4.png" alt="" /></li>
      <li><img src="/assets/images/posts/LaneATT/img5.png" alt="" /></li>
    </ul>
  </li>
  <li>LLAMAS
    <ul>
      <li><img src="/assets/images/posts/LaneATT/img6.png" alt="" /></li>
    </ul>
  </li>
  <li>可視化
    <ul>
      <li><img src="/assets/images/posts/LaneATT/img7.png" alt="" /></li>
      <li>青は正解、緑と赤が予測</li>
      <li>上段がTuSimple、中段がCULane、下段がLLAMAS</li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Lane Detection" /><category term="Lane Detection" /><summary type="html"><![CDATA[概要 YOLOv3やSSDのようなアンカーベースのモデルであるLaneATTを提案 大域的情報の取得のためにAttentionも使用 CULaneとTuSimpleとLLAMASのデータセットで最先端手法を凌駕]]></summary></entry><entry><title type="html">LineNet: a Zoomable CNN for Crowdsourced High Definition Maps Modeling in Urban Environments</title><link href="http://localhost:4000/lane%20detection/2022/12/13/LineNet.html" rel="alternate" type="text/html" title="LineNet: a Zoomable CNN for Crowdsourced High Definition Maps Modeling in Urban Environments" /><published>2022-12-13T12:00:00+09:00</published><updated>2022-12-13T12:00:00+09:00</updated><id>http://localhost:4000/lane%20detection/2022/12/13/LineNet</id><content type="html" xml:base="http://localhost:4000/lane%20detection/2022/12/13/LineNet.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>現在のCNNを用いた車線検出の研究はセグメンテーションに限定されており、直感的でなく不正確である</li>
  <li>HDマップのモデリングのために、LP層とZoomモジュールを持つCNN手法のLineNetを提案</li>
  <li>車線検出用のデータセットTTLaneを紹介
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>LineNetとTTLaneを組み合わせることで、HDマップのモデリングを行うパイプラインを初めて提案</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>Line Prediction(LP)層
    <ul>
      <li>車線の位置決めと分類のために設計された追加層</li>
      <li>Mask、Position、Direction、Confidence、Distance、Typeの6つのブランチがある
        <ul>
          <li><img src="/assets/images/posts/LaneDetectionSurvey/img30.png" alt="" /></li>
          <li>Mask
            <ul>
              <li>一定の幅（32pixel）で描かれたストローク</li>
            </ul>
          </li>
          <li>Position
            <ul>
              <li>アンカーポイントから線への最小距離のベクトル</li>
            </ul>
          </li>
          <li>Direction
            <ul>
              <li>車線の向き</li>
            </ul>
          </li>
          <li>Confidence
            <ul>
              <li>ネットワークが車線を見れているかの信頼度</li>
            </ul>
          </li>
          <li>Distance
            <ul>
              <li>アンカーポイントから線への最小距離の長さ</li>
            </ul>
          </li>
          <li>Type
            <ul>
              <li>6種類のマーキング（白実線、白破線、黄実線、黄破線、二重線、その他）</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Zoomモジュール
    <ul>
      <li><img src="/assets/images/posts/LaneDetectionSurvey/img31.png" alt="" /></li>
      <li>ネットワーク構造を変えずに、視野を任意のサイズに変更できる</li>
      <li>サムネイルCNNと高解像度トリミングCNNに分割
        <ul>
          <li>2つのCNNは重みを共有する</li>
          <li>サムネイルCNN
            <ul>
              <li>グローバル特徴を獲得</li>
            </ul>
          </li>
          <li>高解像度トリミングCNN
            <ul>
              <li>詳細に「見る」</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>インジェクション層で2つのCNNの特徴を融合</li>
      <li>LP層で確信度が低い領域についてZoomモジュールを複数回適用する
        <ul>
          <li><img src="/assets/images/posts/LineNet/img1.png" alt="" /></li>
          <li>0.5~16倍まで</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>まだ不連続な点のため、DBSCANでクラスタリング</li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>定量評価
    <ul>
      <li><img src="/assets/images/posts/LineNet/img2.png" alt="" /></li>
    </ul>
  </li>
  <li>定性評価
    <ul>
      <li><img src="/assets/images/posts/LineNet/img3.png" alt="" /></li>
      <li>(a)原画像 (b)(c)正解 (d)SCNN (e)Mask E-CNN (f)MLD-CRF (g)LineNet</li>
      <li>LineNetは二重線検出や複雑なシーンにも強い</li>
    </ul>
  </li>
  <li>HD地図モデリング
    <ul>
      <li><img src="/assets/images/posts/LineNet/img4.png" alt="" /></li>
      <li><img src="/assets/images/posts/LineNet/img5.png" alt="" /></li>
      <li>車線の平均誤差がGPSの5mから31.3cmへと大幅に改善</li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Lane Detection" /><category term="Lane Detection" /><summary type="html"><![CDATA[概要 現在のCNNを用いた車線検出の研究はセグメンテーションに限定されており、直感的でなく不正確である HDマップのモデリングのために、LP層とZoomモジュールを持つCNN手法のLineNetを提案 車線検出用のデータセットTTLaneを紹介]]></summary></entry><entry><title type="html">Lane Detection: A Survey with New Results</title><link href="http://localhost:4000/lane%20detection/2022/12/12/LaneDetectionSurvey.html" rel="alternate" type="text/html" title="Lane Detection: A Survey with New Results" /><published>2022-12-12T12:00:00+09:00</published><updated>2022-12-12T12:00:00+09:00</updated><id>http://localhost:4000/lane%20detection/2022/12/12/LaneDetectionSurvey</id><content type="html" xml:base="http://localhost:4000/lane%20detection/2022/12/12/LaneDetectionSurvey.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>視覚に基づく車線検出のデータセット、深層学習を用いた手法の比較</li>
  <li>HD地図のモデリングに向けた新しいデータセット（TTLane）と複雑な道路状況での自立走行に向けた方向性とLineNetを紹介する
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>全車線の検出とHD地図のモデリングに向けた新しいデータセット（TTLane）</li>
  <li>複雑な道路状況下での正確な車線検出のための新しい深層CNN手法、LineNetを提案</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>データセット
    <ul>
      <li><img src="/assets/images/posts/LaneDetectionSurvey/img1.png" alt="" /></li>
      <li>KITTI  road
        <ul>
          <li>全車線と現在走行している車線「（エゴレーン）の二種類のアノテーション</li>
        </ul>
      </li>
      <li>ELAS
        <ul>
          <li>エゴレーン＋レーンマーキングタイプ（LMT）</li>
          <li>20以上の異なるシーン（15,000フレーム以上）</li>
        </ul>
      </li>
      <li>Caltech Lanes
        <ul>
          <li>都市環境における4つのビデオ</li>
          <li>1225枚の画像</li>
        </ul>
      </li>
      <li>BDD100K
        <ul>
          <li>車線が走行方向と平行か否かを示すアノテーション付き</li>
        </ul>
      </li>
      <li>VPGNet
        <ul>
          <li>4つのシナリオ、8種類の車線マーキング、9種類の道路マーキング</li>
          <li>約20,000枚</li>
          <li>すべての車線にアノテーション</li>
        </ul>
      </li>
      <li>tuSimple lane challenge
        <ul>
          <li>高速道路で撮影された3626枚の学習画像と2782枚のテスト画像</li>
          <li>車線の種類を区別しない</li>
          <li>破線の車線を実線と表記</li>
        </ul>
      </li>
      <li>CULane
        <ul>
          <li>複数車線検出の最大のデータセット</li>
          <li>現在走っている道路のアノテーションしかない＋道路境界のアノテーションがない</li>
          <li>HD地図のモデリングに向いてない</li>
        </ul>
      </li>
      <li>TTLane Dataset
        <ul>
          <li><img src="/assets/images/posts/LaneDetectionSurvey/img2.png" alt="" /></li>
          <li>全車線の検出とHD地図のモデリングに向けたデータセット</li>
          <li>晴天から雨天まで、異なる光の条件と天候</li>
          <li>全ての車線にアノテーション</li>
          <li>LMT（白実線、白破線、黄実線、黄破線、二重線）
            <ul>
              <li>破線の間にもアノテーション</li>
              <li>実践と破線の組み合わせもできる</li>
            </ul>
          </li>
          <li>中心点は手動、連続はベジェ曲線でフィッティング</li>
          <li>13200枚のうち3000枚にオクルージョン情報
            <ul>
              <li>クルマで重なって見えない部分</li>
            </ul>
          </li>
          <li>分離帯などは「その他」
            <ul>
              <li><img src="/assets/images/posts/LaneDetectionSurvey/img3.png" alt="" /></li>
              <li><img src="/assets/images/posts/LaneDetectionSurvey/img4.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>手法
    <ul>
      <li>共通の課題：光線状態、天候状態、オクルージョン</li>
      <li>エゴレーン（現在走行している車線）検出
        <ul>
          <li>車線逸脱警報（LDWS）やレーンセンタリングで用いられる</li>
          <li>リアルタイム性能が要求される</li>
          <li>シングルタスク（車線検出のみの手法）
            <ul>
              <li>Efficient Deep Models for Monocular Road Segmentation
                <ul>
                  <li><img src="/assets/images/posts/LaneDetectionSurvey/img5.png" alt="" /></li>
                  <li>VGGベースのエンコーダ・デコーダネットワーク</li>
                </ul>
              </li>
              <li>
                <p>Drivable Road Detection Based on Dilated FPN with Feature Aggregation
  <img src="/assets/images/posts/LaneDetectionSurvey/img6.png" alt="" /></p>

                <ul>
                  <li>特徴ピラミッドネットワーク（FPN）</li>
                  <li>KITTIで最高のF1</li>
                </ul>
              </li>
              <li>Road Segmentation Using CNN and Distributed LSTM
                <ul>
                  <li>CNN+LSTMによるセグメンテーション</li>
                  <li><img src="/assets/images/posts/LaneDetectionSurvey/img7.png" alt="" /></li>
                  <li><img src="/assets/images/posts/LaneDetectionSurvey/img8.png" alt="" /></li>
                </ul>
              </li>
            </ul>
          </li>
          <li>マルチタスク（車線検出以外にも道路分類、車線検出、パラメータ回帰）
            <ul>
              <li>RBNet: A Deep Neural Network for Unified Road and Road Boundary Detection
                <ul>
                  <li><img src="/assets/images/posts/LaneDetectionSurvey/img9.png" alt="" /></li>
                  <li>道路と道路境界検出</li>
                  <li>ResNet50で特徴抽出し、3つのタスクをサブネットで検出</li>
                </ul>
              </li>
              <li>Estimating High Definition Map Parameters with Convolutional Neural Networks
                <ul>
                  <li><img src="/assets/images/posts/LaneDetectionSurvey/img10.png" alt="" /></li>
                  <li>マルチタスクCNN</li>
                  <li>道路の種類，車線数，路側，角度などのパラメータを推定
                    <ul>
                      <li>HD地図に必須</li>
                    </ul>
                  </li>
                  <li>ナビから生成された強度マップ、意味マップ、占有グリッドマップが必要</li>
                </ul>
              </li>
              <li>MultiNet: Real-time joint semantic reasoning for autonomous driving
                <ul>
                  <li><img src="/assets/images/posts/LaneDetectionSurvey/img11.png" alt="" /></li>
                  <li>道路分類、車両検出、道路セグメンテーションを同時に行うエンドツーエンドのマルチタスクアーキテクチャ</li>
                  <li>共有の3層CNN Encoderの特徴を入力とし、3つのDecoderで予測</li>
                  <li>リアルタイム性能を達成</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>エゴ・ロードレーン（走行方向の道路にある全てのレーン）検出
            <ul>
              <li>課題：道路幅の変化により、レーン数が変化することがある
                <ul>
                  <li>インスタンスセグメンテーションとみなされる</li>
                </ul>
              </li>
              <li>End-to-end方式
                <ul>
                  <li>個々の車線を直接出力</li>
                  <li>VPGNet: Vanishing point guided network for lane and road marking detection and recognition
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img12.png" alt="" /></li>
                      <li>車線と道路標識の同時検出を行うマルチタスクCNN</li>
                      <li>車線の消失点を利用</li>
                      <li>Caltech Lanesで最高のF1</li>
                    </ul>
                  </li>
                  <li>Spatial as deep: Spatial CNN for traffic scene understanding
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img13.png" alt="" /></li>
                      <li>空間CNN（SCNN）というレイヤーを提案</li>
                      <li>行／列に沿ったメッセージの受け渡し、受容野を画像全体に拡大</li>
                      <li>車線が画像上で交差している可能性があるため、車線認識に有効</li>
                      <li>あらかじめ定義された数の車線しか検出することができない</li>
                    </ul>
                  </li>
                  <li>SpinNet: Spinning convolutional network for lane boundary detection
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img14.png" alt="" /></li>
                      <li>多くの情報を収集するために回転畳み込み層を導入</li>
                      <li>特徴マップから車線曲線を回帰する車線境界パラメータ化技でEnd-to-end</li>
                    </ul>
                  </li>
                  <li>Learning lightweight lane detection CNNs by self attention distillation
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img15.png" alt="" /></li>
                      <li>隣接する2 つの ENet Encoder間の自己注意を学習し、固定数のレーンをセグメント化する新しいモジュールSelf Attention Distillation (SAD)を提案</li>
                    </ul>
                  </li>
                  <li>Lane Detection and Classification using Cascaded CNNs
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img16.png" alt="" /></li>
                      <li>インスタンスセグメンテーションネットワークと分類ネットワークをカスケード接続</li>
                    </ul>
                  </li>
                  <li>FastDraw: Addressing the long tail of lane detection by adapting a sequential prediction network
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img17.png" alt="" /></li>
                      <li>セグメンテーションの代わりにResNet-50を適応し、
  複数の車線の表現を自動回帰させ、
  道路上の任意の数の車線を検出できるように</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>車線標識を見つけるセグメントネットワーク＋
  車線インスタンスを得るためのクラスタリングや車線曲線フィット
                <ul>
                  <li>Semantic instance segmentation with a discriminative loss function
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img18.png" alt="" /></li>
                      <li>ResNet38の特徴を高速なポスト処理によって特徴をクラスタリング</li>
                    </ul>
                  </li>
                  <li>Towards end-to-end lane detection: An instance segmentation approach
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img19.png" alt="" /></li>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img20.png" alt="" /></li>
                      <li>車線分割サブネットワーク、上記手法と同様の画素埋め込みサブネットワーク、および透視変換ネットワークからなる複雑なネットワークを提案</li>
                      <li>最後に透視変換の各車線のインスタンスに対し、3次多項式</li>
                    </ul>
                  </li>
                  <li>Learning to Cluster for Proposal-Free Instance Segmentation
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img21.png" alt="" /></li>
                      <li>セグメンテーションネットワークを利用し、車線と車線の特徴を同時に見つける</li>
                      <li>後処理でクラスタリング</li>
                    </ul>
                  </li>
                  <li>Multi-lane detection using instance segmentation and attentive voting
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img22.png" alt="" /></li>
                      <li>複雑な気象条件で収集された独自の市道データセットに対して車線分割ネットワークを学習</li>
                      <li>その後、教師なしクラスタリング</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>全車線検出
            <ul>
              <li>自立走行において曲がる可能性がある十字路、HD地図のモデリングなどで必要</li>
              <li>エゴレーン検出用の手法
                <ul>
                  <li>Efficient Deep Models for Monocular Road Segmentation</li>
                  <li>RBNet: A Deep Neural Network for Unified Road and Road Boundary Detection</li>
                  <li>MultiNet: Real-time joint semantic reasoning for autonomous driving</li>
                  <li>オクルージョンの影響を受けやすく、車線境界の種類も無視</li>
                  <li>入力画像が前方視であるため，消失点に向かってレーンマークが細く小さくなり，識別しづらい</li>
                </ul>
              </li>
              <li>より多くのモダリティとの組み合わせ
                <ul>
                  <li>Accurate and robust lane detection based on dual-view convolutional neutral network
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img23.png" alt="" /></li>
                      <li>前方視と鳥瞰図を組み合わせた</li>
                    </ul>
                  </li>
                  <li>3D-laneNet: End-to-end 3D multiple lane detection
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img24.png" alt="" /></li>
                      <li>前方視と鳥瞰図を利用して、道路平面 と3DレーンをEnd-to-endで予測</li>
                    </ul>
                  </li>
                  <li>Deep multi-sensor lane detection
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img25.png" alt="" /></li>
                      <li>LiDARとRGBカメラの組み合わせによる予測</li>
                      <li>3D LiDARで接地面の高さと角度を予測</li>
                      <li>予測されたパラメータを使用して、画像を鳥瞰図に再投影</li>
                    </ul>
                  </li>
                  <li>HD maps: Finegrained road segmentation by parsing ground and aerial images
  <img src="/assets/images/posts/LaneDetectionSurvey/img26.png" alt="" />
                    <ul>
                      <li>航空写真を使って、すべての道路を認識しモデル化</li>
                    </ul>
                  </li>
                  <li>Aerial LaneNet: Lane Marking Semantic Segmentation in Aerial Imagery using Wavelet-Enhanced Cost-sensitive Symmetric Fully Convolutional Neural Networks
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img27.png" alt="" /></li>
                      <li>Encoder Decoderでリモートセンシング画像内のすべての車線をセグメンテーション</li>
                    </ul>
                  </li>
                  <li>Deep learning segmentation and 3D reconstruction of road markings using multiview aerial imagery
                    <ul>
                      <li><img src="/assets/images/posts/LaneDetectionSurvey/img28.png" alt="" /></li>
                      <li>上の研究に基づいて、マルチビュー航空写真から高精細な写真を再構成</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>学習型車線検出の動向
    <ul>
      <li>ロバスト性（晴れでも曇りでも、昼でも夜でも、夏でも冬でも、都会でも田舎でも、渋滞でも晴れでも、1年を通して車線検出する）のために
        <ul>
          <li>多くのモダリティ
            <ul>
              <li>LiDAR、赤外線画像、航空写真、パノラマ画像の利用</li>
            </ul>
          </li>
          <li>汎用性の向上
            <ul>
              <li>データセットに制約されていて汎化されてない</li>
              <li>転送学習や実世界の道路情報を示す、より一般的なデータセットの検討</li>
            </ul>
          </li>
          <li>3D車線検出
            <ul>
              <li>2次元では曲がる際や合流する際に不可欠な距離情報が欠落</li>
            </ul>
          </li>
          <li>マルチタスク
            <ul>
              <li>自律走行システムのためのネットワーク負荷を軽減するために、複数タスクを同時の行うネットワークが必要</li>
            </ul>
          </li>
          <li>半教師ありや教師なし学習の利用</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>LineNet
    <ul>
      <li><img src="/assets/images/posts/LaneDetectionSurvey/img29.png" alt="" /></li>
      <li>既存のCNNベースの手法は画像分類タスク用ネットワークを使用していて、白泉検出には適さない</li>
      <li>事前学習済みDeepLabを基幹とし、Line Prediction(LP)層とZoomモジュールを含む</li>
      <li>Line Prediction(LP)層
        <ul>
          <li>マスク層、位置層、方向層、信頼度層、距離層、およびタイプ層の6層から成る
            <ul>
              <li><img src="/assets/images/posts/LaneDetectionSurvey/img30.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Zoomモジュール
        <ul>
          <li><img src="/assets/images/posts/LaneDetectionSurvey/img31.png" alt="" /></li>
          <li>低解像度画像における結果が信頼できない領域を、サムネイルCNNと高解像度トリミングCNNの2つに分割
            <ul>
              <li>重みと特徴を共有</li>
              <li>サムネイルCNNはグローバル特徴、高解像度トリミングCNNは詳細に「見る」</li>
            </ul>
          </li>
          <li>生成した離散的な点をDBSCANを用いてクラスタリングし線に</li>
          <li>滑らかで信頼性の高い線を車線検出結果として得る</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>Caltech Lanes
    <ul>
      <li><img src="/assets/images/posts/LaneDetectionSurvey/img32.png" alt="" /></li>
    </ul>
  </li>
  <li>CULane
    <ul>
      <li><img src="/assets/images/posts/LaneDetectionSurvey/img33.png" alt="" /></li>
    </ul>
  </li>
  <li>TTLane
    <ul>
      <li><img src="/assets/images/posts/LaneDetectionSurvey/img34.png" alt="" /></li>
      <li><img src="/assets/images/posts/LaneDetectionSurvey/img35.png" alt="" /></li>
      <li>(a)原画像 (b)(c)正解 (d)SCNN (e)Mask E-CNN (f)MLD-CRF (g)LineNet
        <ul>
          <li>LineNetは二重線検出や複雑なシーンにも強い
            <ul>
              <li><img src="/assets/images/posts/LaneDetectionSurvey/img36.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>HD地図モデリング
    <ul>
      <li>クラウドソーシングで収集した画像と GPS 情報から地図を生成</li>
      <li>車線の平均誤差がGPSの5mから31.3cmへと大幅に改善
  <img src="/assets/images/posts/LaneDetectionSurvey/img37.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Lane Detection" /><category term="Lane Detection" /><summary type="html"><![CDATA[概要 視覚に基づく車線検出のデータセット、深層学習を用いた手法の比較 HD地図のモデリングに向けた新しいデータセット（TTLane）と複雑な道路状況での自立走行に向けた方向性とLineNetを紹介する]]></summary></entry><entry><title type="html">OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework</title><link href="http://localhost:4000/hoi/2022/11/29/OFA.html" rel="alternate" type="text/html" title="OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework" /><published>2022-11-29T12:00:00+09:00</published><updated>2022-11-29T12:00:00+09:00</updated><id>http://localhost:4000/hoi/2022/11/29/OFA</id><content type="html" xml:base="http://localhost:4000/hoi/2022/11/29/OFA.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>包括的なタスクを行うことができる、タスクとモダリティを無視できるフレームワークであるOFA(One For All)を提案
    <ul>
      <li>Text2Image, Visual Grounding, VQA, Image Caption, Image Classification, language modeling</li>
    </ul>
  </li>
  <li>一般に公開されている2000万件の画像テキストペアのデータセットで事前学習</li>
  <li>自然言語理解（RoBERTa、ELECTRA、DeBERTa）
自然言語生成（UniLM、Pegasus、 ProphetNet）
画像分類（MoCo-v3、BEiT、MAE）と同等のパフォーマンスを達成</li>
  <li>未学習のタスクにも移行できる
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>下流タスクであるVQAや画像キャプションでの性能劣化が起きない</li>
  <li>画像生成機能を持っている</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<ul>
  <li>アーキテクチャ
    <ul>
      <li><img src="/assets/images/posts/OFA/OFA.png" alt="" /></li>
      <li>TransformerベースのEncoder Decoderフレームワーク</li>
      <li>テキストと画像に対して絶対位置埋め込み</li>
      <li>ハイパラ
        <ul>
          <li><img src="/assets/images/posts/OFA/HyperPram.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>事前学習
    <ul>
      <li><img src="/assets/images/posts/OFA/PreTrain.png" alt="" /></li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>VQA and visual entailment
    <ul>
      <li><img src="/assets/images/posts/OFA/VQA.png" alt="" /></li>
    </ul>
  </li>
  <li>MSCOCO Image Captioning
    <ul>
      <li><img src="/assets/images/posts/OFA/ImageCaptioning.png" alt="" /></li>
    </ul>
  </li>
  <li>RefCOCO, RefCOCO+, RefCOCOg
    <ul>
      <li><img src="/assets/images/posts/OFA/RefCOCO.png" alt="" /></li>
    </ul>
  </li>
  <li>text-to-image generation task
    <ul>
      <li><img src="/assets/images/posts/OFA/Text2Image1.png" alt="" /></li>
      <li><img src="/assets/images/posts/OFA/Text2Image2.png" alt="" /></li>
    </ul>
  </li>
  <li>GLUE benchmark datasets
    <ul>
      <li><img src="/assets/images/posts/OFA/GLUE.png" alt="" /></li>
    </ul>
  </li>
  <li>Gigaword abstractive summarization
    <ul>
      <li><img src="/assets/images/posts/OFA/Gigaword.png" alt="" /></li>
    </ul>
  </li>
  <li>ImageNet-1K
    <ul>
      <li><img src="/assets/images/posts/OFA/ImageNet.png" alt="" /></li>
    </ul>
  </li>
  <li>unseen task grounded QA
    <ul>
      <li><img src="/assets/images/posts/OFA/UnseenTask.png" alt="" /></li>
    </ul>
  </li>
  <li>unseen domain VQA
    <ul>
      <li><img src="/assets/images/posts/OFA/UnseenDomain.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="Multimodal Pretraining" /><category term="Multitask Learning" /><category term="Unified Frameworks" /><category term="Zero-shot Learning" /><category term="Transformer" /><summary type="html"><![CDATA[概要 包括的なタスクを行うことができる、タスクとモダリティを無視できるフレームワークであるOFA(One For All)を提案 Text2Image, Visual Grounding, VQA, Image Caption, Image Classification, language modeling 一般に公開されている2000万件の画像テキストペアのデータセットで事前学習 自然言語理解（RoBERTa、ELECTRA、DeBERTa） 自然言語生成（UniLM、Pegasus、 ProphetNet） 画像分類（MoCo-v3、BEiT、MAE）と同等のパフォーマンスを達成 未学習のタスクにも移行できる]]></summary></entry><entry><title type="html">QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information</title><link href="http://localhost:4000/hoi/2022/10/20/QPIC.html" rel="alternate" type="text/html" title="QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information" /><published>2022-10-20T12:00:00+09:00</published><updated>2022-10-20T12:00:00+09:00</updated><id>http://localhost:4000/hoi/2022/10/20/QPIC</id><content type="html" xml:base="http://localhost:4000/hoi/2022/10/20/QPIC.html"><![CDATA[<h1 id="概要">概要</h1>

<ul>
  <li>CNNベースのHOI手法ではCNNの局所性により全体の特徴を使用できず、手動で設定した関心領域に依存し、複数のHOIを混在する欠点がある</li>
  <li>transformerベースの特徴抽出器を利用することで、画像全体を集約し複数のHOIの混在を避けることができる</li>
  <li>効果的なtransformerベースの特徴抽出器によって検出ヘッドがシンプルで直感的になり、文脈上重要な特徴をうまく抽出し、既存手法を大きく上回った
    <ul>
      <li>HICO-DETで5.37mAP↑,V-COCOで5.7mAP↑
<!--more--></li>
    </ul>
  </li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>transformerを使用し、文脈特徴をペアワイズで集約</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/QPIC/1.png" alt="" /></p>

<ul>
  <li>アーキテクチャ
    <ul>
      <li>事前学習済みCNNで特徴マップを取得し、畳み込みでチャンネル数を調整</li>
      <li>Positional Encodingを追加し、Transformer encoder, decoderに通す
        <ul>
          <li>decoderの入力(Query vectors)は学習</li>
          <li>Query vectorsの数はインタラクション数</li>
        </ul>
      </li>
      <li>Interaction detection heads
        <ul>
          <li>Human box FFN
            <ul>
              <li>人間のバウンディングボックスを予測</li>
            </ul>
          </li>
          <li>Object box FFN
            <ul>
              <li>物体のバウンディングボックスを予測</li>
            </ul>
          </li>
          <li>Object class FFN
            <ul>
              <li>物体のクラスを予測</li>
            </ul>
          </li>
          <li>Action class FFN
            <ul>
              <li>行動のクラスを予測</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Loss
    <ul>
      <li>予測と正解のに分割マッチング
        <ul>
          <li>DETRのHungarianアルゴリズム</li>
        </ul>
      </li>
      <li>マッチングされたペアに対するlossの計算
        <ul>
          <li>$L = \lambda_b L_b + \lambda_u L_u + \lambda_c L_c + \lambda_a L_a$
            <ul>
              <li>$L_b$：人と物体のバウンディングボックスの座標のLoss
                <ul>
                  <li><img src="/assets/images/posts/QPIC/2.png" alt="" /></li>
                </ul>
              </li>
              <li>$L_u$：人と物体のバウンディングボックスのIoUのLoss
                <ul>
                  <li><img src="/assets/images/posts/QPIC/3.png" alt="" /></li>
                </ul>
              </li>
              <li>$L_c$：物体のクラス分類Loss
                <ul>
                  <li><img src="/assets/images/posts/QPIC/4.png" alt="" /></li>
                </ul>
              </li>
              <li>$L_a$：行動のクラス分類Loss
                <ul>
                  <li><img src="/assets/images/posts/QPIC/5.png" alt="" /></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>HICO-DET
    <ul>
      <li>5.37mAP↑
        <ul>
          <li><img src="/assets/images/posts/QPIC/6.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>V-COCO
    <ul>
      <li>5.7mAP↑
        <ul>
          <li><img src="/assets/images/posts/QPIC/7.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>既存手法との比較
    <ul>
      <li><img src="/assets/images/posts/QPIC/8.png" alt="" /></li>
      <li>既存手法
        <ul>
          <li>(a)(b)はDRF, (c)(d)はPPDM→行動の検出に失敗</li>
        </ul>
      </li>
      <li>QPIC
        <ul>
          <li>下段のAttentionマップからわかるように物体を見て検出できた</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="HOI" /><category term="Human-Object Interaction Detection" /><category term="Transformer" /><summary type="html"><![CDATA[概要 CNNベースのHOI手法ではCNNの局所性により全体の特徴を使用できず、手動で設定した関心領域に依存し、複数のHOIを混在する欠点がある transformerベースの特徴抽出器を利用することで、画像全体を集約し複数のHOIの混在を避けることができる 効果的なtransformerベースの特徴抽出器によって検出ヘッドがシンプルで直感的になり、文脈上重要な特徴をうまく抽出し、既存手法を大きく上回った HICO-DETで5.37mAP↑,V-COCOで5.7mAP↑]]></summary></entry><entry><title type="html">Correlating Belongings with Passengers in a Simulated Airport Security Checkpoint</title><link href="http://localhost:4000/object%20detection/2022/10/17/Passengers.html" rel="alternate" type="text/html" title="Correlating Belongings with Passengers in a Simulated Airport Security Checkpoint" /><published>2022-10-17T00:00:00+09:00</published><updated>2022-10-17T00:00:00+09:00</updated><id>http://localhost:4000/object%20detection/2022/10/17/Passengers</id><content type="html" xml:base="http://localhost:4000/object%20detection/2022/10/17/Passengers.html"><![CDATA[<h1 id="概要">概要</h1>
<ul>
  <li>空港の保安検査場における乗客と持ち物のトラッキング・関連付けのアルゴリズムを提示し、その有効性を実証</li>
  <li>手作業とディープラーニングベースのアプローチの両方を活用</li>
  <li>現実のデータセットで、乗客と持ち物を検出、トラッキング、関連付けることができた
<!--more--></li>
</ul>

<h1 id="新規性差分">新規性・差分</h1>

<ul>
  <li>自然光があり実世界に近く、物があり複雑な空港監視システムにおける初めての追跡と関連付け</li>
</ul>

<h1 id="アイデア">アイデア</h1>

<p><img src="/assets/images/posts/Passengers/1.png" alt="" /></p>
<ul>
  <li>乗客の検出とトラッキング
    <ul>
      <li>オプティカルフローでフローが閾値以上の画素を得ることで、乗客の位置を荒く推定</li>
      <li>Faster R-CNNで精密な位置をバウンディングボックスで得る</li>
      <li>オプティカルフローの大きさと方向で移動方向を予測し、トラッキング</li>
      <li><img src="/assets/images/posts/Passengers/2.png" alt="" /></li>
    </ul>
  </li>
  <li>持ち物の検出とトラッキング
    <ul>
      <li>はじめビン（持ち物を入れる容器）は灰色で背景は暗いので、強度の変化で検出できる</li>
      <li>一意のIDとバウンディングボックスを割り当てる</li>
      <li>相関フィルタでモデル化し畳み込みで追跡
        <ul>
          <li>Background Aware Correlation Filterを採用
            <ul>
              <li>ターゲットの周囲の背景をネガティブサンプルとして使用</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Best-Buddies Similarity (BBS) テンプレートマッチングでビン内に離脱物があるか判定
        <ul>
          <li>ビンの色の急激な変化（乗客の手や日陰に入ることが原因）に対応するため</li>
          <li>類似度が閾値以下で空と判定</li>
          <li><img src="/assets/images/posts/Passengers/3.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>乗客と持ち物（ビン）の関連付け
    <ul>
      <li>単純な距離だと、複数の乗客が近接していると失敗する</li>
      <li>カメラ2（置く）
        <ul>
          <li>VGG19の特徴量で上半身をポーズ推定</li>
          <li>手のひらの座標がビンに最も近い乗客を追跡</li>
          <li>ビンから離れたときに、最も手が近い乗客を所有者に割り当て
            <ul>
              <li>所有者以外がビンを一時的に移動させた際に対応</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>カメラ4（拾う）
        <ul>
          <li>手のひらの座標がビンに最も近い乗客を受取人に
            <ul>
              <li>その際に受取人のラベルが所有者と同じかを判定</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><img src="/assets/images/posts/Passengers/4.png" alt="" /></li>
    </ul>
  </li>
</ul>

<h1 id="結果">結果</h1>

<ul>
  <li>結果
    <ul>
      <li>評価方法
        <ul>
          <li>PD：全イベントのうち、正しく検出できたもの↑</li>
          <li>PFA：全イベントのうち、誤って検出したもの↓</li>
          <li>Switch：乗客か持ち物のラベルに変更があったか↓</li>
          <li>Mismatch：乗客と持ち物の関連付けが不一致↓</li>
        </ul>
      </li>
      <li>対象
        <ul>
          <li>PAX：乗客</li>
          <li>DVI：持ち物</li>
          <li>XFR：記載なし</li>
        </ul>
      </li>
      <li>
        <ul>
          <li><img src="/assets/images/posts/Passengers/5.png" alt="" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>評価結果の例
    <ul>
      <li>見方
        <ul>
          <li>緑：GT</li>
          <li>赤：アルゴリズムの予測</li>
          <li>黄色：アルゴリズムの間違い</li>
        </ul>
      </li>
      <li>フレーム
        <ul>
          <li>(a)~(c)：正しく検出</li>
          <li>(d)~(e)：乗客の誤検出（見切れているもの）</li>
          <li>(f)：ビンの見落とし（出てきたばかりのため）</li>
        </ul>
      </li>
      <li><img src="/assets/images/posts/Passengers/6.png" alt="" /></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="Object Detection" /><category term="Object Detection" /><category term="Tracking" /><summary type="html"><![CDATA[概要 空港の保安検査場における乗客と持ち物のトラッキング・関連付けのアルゴリズムを提示し、その有効性を実証 手作業とディープラーニングベースのアプローチの両方を活用 現実のデータセットで、乗客と持ち物を検出、トラッキング、関連付けることができた]]></summary></entry></feed>